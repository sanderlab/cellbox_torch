{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cellbox\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow.compat.v1 as tf\n",
    "import shutil\n",
    "import argparse\n",
    "import json\n",
    "import glob\n",
    "import time\n",
    "from tensorflow.compat.v1.errors import OutOfRangeError\n",
    "from cellbox.utils import TimeLogger\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'experiment_id': 'Example_RP', 'model_prefix': 'seed', 'ckpt_name': 'model11.ckpt', 'export_verbose': 3, 'experiment_type': 'random partition', 'sparse_data': False, 'batchsize': 4, 'trainset_ratio': 0.7, 'validset_ratio': 0.8, 'n_batches_eval': None, 'add_noise_level': 0, 'dT': 0.1, 'ode_solver': 'heun', 'envelope_form': 'tanh', 'envelope': 0, 'pert_form': 'by u', 'ode_degree': 1, 'ode_last_steps': 2, 'n_iter_buffer': 50, 'n_iter_patience': 100, 'weight_loss': 'None', 'l1lambda': 0.0001, 'l2lambda': 0.0001, 'model': 'LinReg', 'pert_file': '/users/ngun7t/Documents/cellbox-jun-6/data/pert.csv', 'expr_file': '/users/ngun7t/Documents/cellbox-jun-6/data/expr.csv', 'node_index_file': '/users/ngun7t/Documents/cellbox-jun-6/data/node_Index.csv', 'n_protein_nodes': 82, 'n_activity_nodes': 87, 'n_x': 99, 'envelop_form': 'tanh', 'envelop': 0, 'n_epoch': 100, 'n_iter': 100, 'stages': [{'nT': 200, 'sub_stages': [{'lr_val': 0.001, 'l1lambda': 0.0001}]}], 'ckpt_path_full': './model11.ckpt', 'drug_index': 5, 'seed': 1000}\n",
      "Working directory is ready at results/Example_RP_b01af15c946ff044d15b7abc831b4538.\n",
      "Hello!\n"
     ]
    }
   ],
   "source": [
    "def set_seed(in_seed):\n",
    "    int_seed = int(in_seed)\n",
    "    tf.compat.v1.set_random_seed(int_seed)\n",
    "    np.random.seed(int_seed)\n",
    "\n",
    "\n",
    "def prepare_workdir(in_cfg):\n",
    "    # Read Data\n",
    "    in_cfg.root_dir = os.getcwd()\n",
    "    in_cfg.node_index = pd.read_csv(in_cfg.node_index_file, header=None, names=None) \\\n",
    "        if hasattr(in_cfg, 'node_index_file') else pd.DataFrame(np.arange(in_cfg.n_x))\n",
    "\n",
    "    # Create Output Folder\n",
    "    experiment_path = 'results/{}_{}'.format(in_cfg.experiment_id, md5)\n",
    "    try:\n",
    "        os.makedirs(experiment_path)\n",
    "    except Exception:\n",
    "        pass\n",
    "    out_cfg = vars(in_cfg)\n",
    "    out_cfg = {key: out_cfg[key] for key in out_cfg if type(out_cfg[key]) is not pd.DataFrame}\n",
    "    os.chdir(experiment_path)\n",
    "    json.dump(out_cfg, open('config.json', 'w'), indent=4)\n",
    "\n",
    "    if \"leave one out\" in in_cfg.experiment_type:\n",
    "        try:\n",
    "            in_cfg.model_prefix = '{}_{}'.format(in_cfg.model_prefix, in_cfg.drug_index)\n",
    "        except Exception('Drug index not specified') as e:\n",
    "            raise e\n",
    "\n",
    "    in_cfg.working_index = in_cfg.model_prefix + \"_\" + str(working_index).zfill(3)\n",
    "\n",
    "    try:\n",
    "        shutil.rmtree(in_cfg.working_index)\n",
    "    except Exception:\n",
    "        pass\n",
    "    os.makedirs(in_cfg.working_index)\n",
    "    os.chdir(in_cfg.working_index)\n",
    "\n",
    "    with open(\"record_eval.csv\", 'w') as f:\n",
    "        f.write(\"epoch,iter,train_loss,valid_loss,train_mse,valid_mse,test_mse,time_elapsed\\n\")\n",
    "\n",
    "    print('Working directory is ready at {}.'.format(experiment_path))\n",
    "    return 0\n",
    "\n",
    "experiment_config_path = \"/users/ngun7t/Documents/cellbox-jun-6/configs_dev/Example.random_partition.json\"\n",
    "working_index = 0\n",
    "stage = {\n",
    "    \"nT\": 100,\n",
    "    \"sub_stages\":[\n",
    "        {\"lr_val\": 0.1,\"l1lambda\": 0.01, \"n_iter_patience\":1000},\n",
    "        {\"lr_val\": 0.01,\"l1lambda\": 0.01},\n",
    "        {\"lr_val\": 0.01,\"l1lambda\": 0.0001},\n",
    "        {\"lr_val\": 0.001,\"l1lambda\": 0.00001}\n",
    "    ]}\n",
    "\n",
    "cfg = cellbox.config.Config(experiment_config_path)\n",
    "cfg.ckpt_path_full = os.path.join('./', cfg.ckpt_name)\n",
    "md5 = cellbox.utils.md5(cfg)\n",
    "cfg.drug_index = 5         # Change this for testing purposes\n",
    "cfg.seed = working_index + cfg.seed if hasattr(cfg, \"seed\") else working_index + 1000\n",
    "set_seed(cfg.seed)\n",
    "print(vars(cfg))\n",
    "\n",
    "prepare_workdir(cfg)\n",
    "logger = cellbox.utils.TimeLogger(time_logger_step=1, hierachy=3)\n",
    "args = cfg\n",
    "for i, stage in enumerate(cfg.stages):\n",
    "    set_seed(cfg.seed)\n",
    "    cfg = cellbox.dataset.factory(cfg)\n",
    "    args.sub_stages = stage['sub_stages']\n",
    "    args.n_T = stage['nT']\n",
    "    model = cellbox.model.factory(args)\n",
    "    if i == 0: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Screenshot(dict):\n",
    "    \"\"\"summarize the model\"\"\"\n",
    "    def __init__(self, args, n_iter_buffer):\n",
    "        # initialize loss_min\n",
    "        super().__init__()\n",
    "        self.loss_min = 1000\n",
    "        # initialize tuning_metric\n",
    "        self.saved_losses = [self.loss_min]\n",
    "        self.n_iter_buffer = n_iter_buffer\n",
    "        # initialize verbose\n",
    "        self.summary = {}\n",
    "        self.summary = {}\n",
    "        self.substage_i = []\n",
    "        self.export_verbose = args.export_verbose\n",
    "\n",
    "    def avg_n_iters_loss(self, new_loss):\n",
    "        \"\"\"average the last few losses\"\"\"\n",
    "        self.saved_losses = self.saved_losses + [new_loss]\n",
    "        self.saved_losses = self.saved_losses[-self.n_iter_buffer:]\n",
    "        return sum(self.saved_losses) / len(self.saved_losses)\n",
    "\n",
    "    def screenshot(self, sess, model, substage_i, node_index, loss_min, args):\n",
    "        \"\"\"evaluate models\"\"\"\n",
    "        self.substage_i = substage_i\n",
    "        self.loss_min = loss_min\n",
    "        # Save the variables to disk.\n",
    "        if self.export_verbose > 0:\n",
    "            params = sess.run(model.params)\n",
    "            for item in params:\n",
    "                try:\n",
    "                    params[item] = pd.DataFrame(params[item], index=node_index[0])\n",
    "                except Exception:\n",
    "                    params[item] = pd.DataFrame(params[item])\n",
    "            self.update(params)\n",
    "\n",
    "        if self.export_verbose > 1 or self.export_verbose == -1:  # no params but y_hat\n",
    "            sess.run(model.iter_eval.initializer, feed_dict=model.args.feed_dicts['test_set'])\n",
    "            y_hat = eval_model(sess, model.iter_eval, model.eval_yhat, args.feed_dicts['test_set'], return_avg=False)\n",
    "            y_hat = pd.DataFrame(y_hat, columns=node_index[0])\n",
    "            self.update({'y_hat': y_hat})\n",
    "\n",
    "        if self.export_verbose > 2:\n",
    "            try:\n",
    "                # TODO: not yet support data iterators\n",
    "                summary_train = sess.run(model.convergence_metric,\n",
    "                                         feed_dict={model.in_pert: args.dataset['pert_train']})\n",
    "                summary_test = sess.run(model.convergence_metric, feed_dict={model.in_pert: args.dataset['pert_test']})\n",
    "                summary_valid = sess.run(model.convergence_metric,\n",
    "                                         feed_dict={model.in_pert: args.dataset['pert_valid']})\n",
    "                summary_train = pd.DataFrame(summary_train, columns=[node_index.values + '_mean', node_index.values +\n",
    "                                                                     '_sd', node_index.values + '_dxdt'])\n",
    "                summary_test = pd.DataFrame(summary_test, columns=[node_index.values + '_mean', node_index.values +\n",
    "                                                                   '_sd', node_index.values + '_dxdt'])\n",
    "                summary_valid = pd.DataFrame(summary_valid, columns=[node_index.values + '_mean', node_index.values +\n",
    "                                                                     '_sd', node_index.values + '_dxdt'])\n",
    "                self.update(\n",
    "                    {'summary_train': summary_train, 'summary_test': summary_test, 'summary_valid': summary_valid}\n",
    "                )\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    def save(self):\n",
    "        \"\"\"save model parameters\"\"\"\n",
    "        for file in glob.glob(str(self.substage_i) + \"_best.*.csv\"):\n",
    "            os.remove(file)\n",
    "        for key in self:\n",
    "            self[key].to_csv(\"{}_best.{}.loss.{}.csv\".format(self.substage_i, key, self.loss_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(saver, sess, path):\n",
    "    \"\"\"save model\"\"\"\n",
    "    # Save the variables to disk.\n",
    "    tmp = saver.save(sess, path)\n",
    "    print(\"Model saved in path: %s\" % tmp)\n",
    "\n",
    "def append_record(filename, contents):\n",
    "    \"\"\"define function for appending training record\"\"\"\n",
    "    with open(filename, 'a') as f:\n",
    "        for content in contents:\n",
    "            f.write('{},'.format(content))\n",
    "        f.write('\\n')\n",
    "\n",
    "\n",
    "def eval_model(sess, eval_iter, obj_fn, eval_dict, return_avg=True, n_batches_eval=None):\n",
    "    \"\"\"simulate the model for prediction\"\"\"\n",
    "    sess.run(eval_iter.initializer, feed_dict=eval_dict)\n",
    "    counter = 0\n",
    "    eval_results = []\n",
    "    while True:\n",
    "        try:\n",
    "            eval_results.append(sess.run(obj_fn, feed_dict=eval_dict))\n",
    "        except OutOfRangeError:\n",
    "            break\n",
    "        counter += 1\n",
    "        if n_batches_eval is not None and counter > n_batches_eval:\n",
    "            break\n",
    "\n",
    "    print(f\"eval_model eval_results: {eval_results[0].shape} with len {len(eval_results)}\")\n",
    "    if return_avg:\n",
    "        return np.mean(np.array(eval_results), axis=0)\n",
    "    return np.vstack(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_substage(model, sess, lr_val, l1_lambda, l2_lambda, n_epoch, n_iter, n_iter_buffer, n_iter_patience, args):\n",
    "    \"\"\"\n",
    "    Training function that does one stage of training. The stage training can be repeated and modified to give better\n",
    "    training result.\n",
    "\n",
    "    Args:\n",
    "        model (CellBox): an CellBox instance\n",
    "        sess (tf.Session): current session, need reinitialization for every nT\n",
    "        lr_val (float): learning rate (read in from config file)\n",
    "        l1_lambda (float): l1 regularization weight\n",
    "        l2_lambda (float): l2 regularization weight\n",
    "        n_epoch (int): maximum number of epochs\n",
    "        n_iter (int): maximum number of iterations\n",
    "        n_iter_buffer (int): training loss moving average window\n",
    "        n_iter_patience (int): training loss tolerance\n",
    "        args: Args or configs\n",
    "    \"\"\"\n",
    "\n",
    "    stages = glob.glob(\"*best*.csv\")\n",
    "    try:\n",
    "        substage_i = 1 + max([int(stage[0]) for stage in stages])\n",
    "    except Exception:\n",
    "        substage_i = 1\n",
    "\n",
    "    best_params = Screenshot(args, n_iter_buffer)\n",
    "\n",
    "    n_unchanged = 0\n",
    "    idx_iter = 0\n",
    "    for key in args.feed_dicts:\n",
    "        args.feed_dicts[key].update({\n",
    "            model.lr: lr_val,\n",
    "            model.l1_lambda: l1_lambda,\n",
    "            model.l2_lambda: l2_lambda\n",
    "        })\n",
    "    args.logger.log(\"--------- lr: {}\\tl1: {}\\tl2: {}\\t\".format(lr_val, l1_lambda, l2_lambda))\n",
    "    sess.run(model.iter_monitor.initializer, feed_dict=args.feed_dicts['valid_set'])\n",
    "    loss_train_across_epochs, loss_train_mse_across_epochs = [], []\n",
    "    loss_val_across_epochs, loss_val_mse_across_epochs = [], []\n",
    "    for idx_epoch in range(n_epoch):\n",
    "\n",
    "        loss_train_l, loss_train_mse_l = [], []\n",
    "        loss_val_l, loss_val_mse_l = [], []\n",
    "        if idx_iter > n_iter or n_unchanged > n_iter_patience:\n",
    "            break\n",
    "\n",
    "        sess.run(model.iter_train.initializer, feed_dict=args.feed_dicts['train_set'])\n",
    "        while True:\n",
    "            if idx_iter > n_iter or n_unchanged > n_iter_patience:\n",
    "                break\n",
    "            t0 = time.perf_counter()\n",
    "            try:\n",
    "                _, loss_train_i, loss_train_mse_i = sess.run(\n",
    "                    (model.op_optimize, model.train_loss, model.train_mse_loss), feed_dict=args.feed_dicts['train_set'])\n",
    "                print(f\"Loss train i: {loss_train_i}\")\n",
    "                loss_train_l.append(loss_train_i)\n",
    "                loss_train_mse_l.append(loss_train_mse_i)\n",
    "\n",
    "            except OutOfRangeError:  # for iter_train\n",
    "                break\n",
    "\n",
    "            # record training\n",
    "            loss_valid_i, loss_valid_mse_i = sess.run(\n",
    "                (model.monitor_loss, model.monitor_mse_loss), feed_dict=args.feed_dicts['valid_set'])\n",
    "            loss_val_l.append(loss_valid_i)\n",
    "            loss_val_mse_l.append(loss_valid_mse_i)\n",
    "            #new_loss = best_params.avg_n_iters_loss(loss_valid_i)\n",
    "            #if args.export_verbose > 0:\n",
    "            #    print((\"Substage:{}\\tEpoch:{}/{}\\tIteration: {}/{}\" + \"\\tloss (train):{:1.6f}\" + \"\\tbest:{:1.6f}\\tTolerance: {}/{}\").format(\n",
    "            #        substage_i, idx_epoch, n_epoch, idx_iter,\n",
    "            #        n_iter, loss_train_i,\n",
    "            #        best_params.loss_min, n_unchanged,\n",
    "            #        n_iter_patience\n",
    "            #        ))\n",
    "            new_loss = best_params.avg_n_iters_loss(loss_valid_i)\n",
    "            if args.export_verbose > 0:\n",
    "                print((\"Substage:{}\\tEpoch:{}/{}\\tIteration: {}/{}\" + \"\\tloss (train):{:1.6f}\\tloss (buffer on valid):\"\n",
    "                       \"{:1.6f}\" + \"\\tbest:{:1.6f}\\tTolerance: {}/{}\").format(substage_i, idx_epoch, n_epoch, idx_iter,\n",
    "                                                                              n_iter, loss_train_i, new_loss,\n",
    "                                                                              best_params.loss_min, n_unchanged,\n",
    "                                                                              n_iter_patience))\n",
    "            append_record(\"record_eval.csv\",\n",
    "                          [idx_epoch, idx_iter, loss_train_i, loss_valid_i, loss_train_mse_i,\n",
    "                           loss_valid_mse_i, None, time.perf_counter() - t0])\n",
    "            # early stopping\n",
    "            idx_iter += 1\n",
    "            if new_loss < best_params.loss_min:\n",
    "                n_unchanged = 0\n",
    "                best_params.screenshot(sess, model, substage_i, args=args,\n",
    "                                       node_index=args.dataset['node_index'], loss_min=new_loss)\n",
    "            else:\n",
    "                n_unchanged += 1\n",
    "\n",
    "        loss_train_across_epochs.append(loss_train_l)\n",
    "        loss_train_mse_across_epochs.append(loss_train_mse_l)\n",
    "        loss_val_across_epochs.append(loss_val_l)\n",
    "        loss_val_mse_across_epochs.append(loss_val_mse_l)\n",
    "\n",
    "\n",
    "    return best_params, {\n",
    "        \"train\": loss_train_across_epochs,\n",
    "        \"train_mse\": loss_train_mse_across_epochs,\n",
    "        \"val\": loss_val_across_epochs,\n",
    "        \"val_mse\": loss_val_mse_across_epochs\n",
    "    }\n",
    "\n",
    "\n",
    "    # Evaluation on valid set\n",
    "    #t0 = time.perf_counter()\n",
    "    #sess.run(model.iter_eval.initializer, feed_dict=args.feed_dicts['valid_set'])\n",
    "    #loss_valid_i, loss_valid_mse_i = eval_model(sess, model.iter_eval, (model.eval_loss, model.eval_mse_loss),\n",
    "    #                                            args.feed_dicts['valid_set'], n_batches_eval=args.n_batches_eval)\n",
    "    #append_record(\"record_eval.csv\", [-1, None, None, loss_valid_i, None, loss_valid_mse_i, None, time.perf_counter() - t0])\n",
    "#\n",
    "    ## Evaluation on test set\n",
    "    #t0 = time.perf_counter()\n",
    "    #sess.run(model.iter_eval.initializer, feed_dict=args.feed_dicts['test_set'])\n",
    "    #loss_test_mse = eval_model(sess, model.iter_eval, model.eval_mse_loss,\n",
    "    #                           args.feed_dicts['test_set'], n_batches_eval=args.n_batches_eval)\n",
    "    #append_record(\"record_eval.csv\", [-1, None, None, None, None, None, loss_test_mse, time.perf_counter() - t0])\n",
    "#\n",
    "    #best_params.save()\n",
    "    #args.logger.log(\"------------------ Substage {} finished!-------------------\".format(substage_i))\n",
    "    #save_model(args.saver, sess, './' + args.ckpt_name)\n",
    "#\n",
    "#\n",
    "def append_record(filename, contents):\n",
    "    \"\"\"define function for appending training record\"\"\"\n",
    "    with open(filename, 'a') as f:\n",
    "        for content in contents:\n",
    "            f.write('{},'.format(content))\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, args):\n",
    "    \"\"\"Train the model\"\"\"\n",
    "    args.logger = TimeLogger(time_logger_step=1, hierachy=2)\n",
    "\n",
    "    # Check if all variables in scope\n",
    "    # TODO: put variables under appropriate scopes\n",
    "    for i in tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.GLOBAL_VARIABLES, scope='initialization'):\n",
    "        print(i)\n",
    "\n",
    "    # Initialization\n",
    "    args.saver = tf.compat.v1.train.Saver()\n",
    "    from tensorflow.core.protobuf import rewriter_config_pb2\n",
    "    config = tf.compat.v1.ConfigProto()\n",
    "    off = rewriter_config_pb2.RewriterConfig.OFF\n",
    "    config.graph_options.rewrite_options.memory_optimization = off\n",
    "\n",
    "    # Launching session\n",
    "    sess = tf.compat.v1.Session(config=config)\n",
    "    sess.run(tf.compat.v1.global_variables_initializer())\n",
    "    try:\n",
    "        args.saver.restore(sess, './' + args.ckpt_name)\n",
    "        print('Load existing model at {}...'.format(args.ckpt_name))\n",
    "    except Exception:\n",
    "        print('Create new model at {}...'.format(args.ckpt_name))\n",
    "\n",
    "    # Training\n",
    "    for substage in args.sub_stages:\n",
    "        n_iter_buffer = substage['n_iter_buffer'] if 'n_iter_buffer' in substage else args.n_iter_buffer\n",
    "        n_iter = substage['n_iter'] if 'n_iter' in substage else args.n_iter\n",
    "        n_iter_patience = substage['n_iter_patience'] if 'n_iter_patience' in substage else args.n_iter_patience\n",
    "        n_epoch = substage['n_epoch'] if 'n_epoch' in substage else args.n_epoch\n",
    "        l1 = substage['l1lambda'] if 'l1lambda' in substage else args.l1lambda if hasattr(args, 'l1lambda') else 0\n",
    "        l2 = substage['l2lambda'] if 'l2lambda' in substage else args.l2lambda if hasattr(args, 'l2lambda') else 0\n",
    "        screenshot, d = train_substage(model, sess, substage['lr_val'], l1_lambda=l1, l2_lambda=l2, n_epoch=n_epoch,\n",
    "                       n_iter=n_iter, n_iter_buffer=n_iter_buffer, n_iter_patience=n_iter_patience, args=args)\n",
    "\n",
    "    # Terminate session\n",
    "    sess.close()\n",
    "    tf.compat.v1.reset_default_graph()\n",
    "\n",
    "    return screenshot, d"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'initialization/W:0' shape=(99, 99) dtype=float32_ref>\n",
      "<tf.Variable 'initialization/b:0' shape=(99, 1) dtype=float32_ref>\n",
      "Create new model at model11.ckpt...\n",
      "########   --------- lr: 0.001\tl1: 0.0001\tl2: 0.0001\t   --time elapsed: 0.05\n",
      "Loss train i: 3.614074230194092\n",
      "Substage:1\tEpoch:0/100\tIteration: 0/100\tloss (train):3.614074\tloss (buffer on valid):504.059919\tbest:1000.000000\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 12\n",
      "Loss train i: 3.607429265975952\n",
      "Substage:1\tEpoch:1/100\tIteration: 1/100\tloss (train):3.607429\tloss (buffer on valid):338.206011\tbest:504.059919\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 12\n",
      "Loss train i: 3.6007916927337646\n",
      "Substage:1\tEpoch:2/100\tIteration: 2/100\tloss (train):3.600792\tloss (buffer on valid):255.587957\tbest:338.206011\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 12\n",
      "Loss train i: 3.5941641330718994\n",
      "Substage:1\tEpoch:3/100\tIteration: 3/100\tloss (train):3.594164\tloss (buffer on valid):205.465756\tbest:255.587957\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 12\n",
      "Loss train i: 3.5875465869903564\n",
      "Substage:1\tEpoch:4/100\tIteration: 4/100\tloss (train):3.587547\tloss (buffer on valid):172.332705\tbest:205.465756\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 12\n",
      "Loss train i: 3.580939292907715\n",
      "Substage:1\tEpoch:5/100\tIteration: 5/100\tloss (train):3.580939\tloss (buffer on valid):148.209183\tbest:172.332705\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 12\n",
      "Loss train i: 3.5743422508239746\n",
      "Substage:1\tEpoch:6/100\tIteration: 6/100\tloss (train):3.574342\tloss (buffer on valid):130.462908\tbest:148.209183\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 12\n",
      "Loss train i: 3.5677552223205566\n",
      "Substage:1\tEpoch:7/100\tIteration: 7/100\tloss (train):3.567755\tloss (buffer on valid):116.990114\tbest:130.462908\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 12\n",
      "Loss train i: 3.561178684234619\n",
      "Substage:1\tEpoch:8/100\tIteration: 8/100\tloss (train):3.561179\tloss (buffer on valid):105.937968\tbest:116.990114\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 12\n",
      "Loss train i: 3.554612159729004\n",
      "Substage:1\tEpoch:9/100\tIteration: 9/100\tloss (train):3.554612\tloss (buffer on valid):97.141822\tbest:105.937968\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 12\n",
      "Loss train i: 3.5480575561523438\n",
      "Substage:1\tEpoch:10/100\tIteration: 10/100\tloss (train):3.548058\tloss (buffer on valid):89.728798\tbest:97.141822\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 12\n",
      "Loss train i: 3.5415143966674805\n",
      "Substage:1\tEpoch:11/100\tIteration: 11/100\tloss (train):3.541514\tloss (buffer on valid):83.220269\tbest:89.728798\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 12\n",
      "Loss train i: 3.5349817276000977\n",
      "Substage:1\tEpoch:12/100\tIteration: 12/100\tloss (train):3.534982\tloss (buffer on valid):77.911526\tbest:83.220269\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 12\n",
      "Loss train i: 3.5284605026245117\n",
      "Substage:1\tEpoch:13/100\tIteration: 13/100\tloss (train):3.528461\tloss (buffer on valid):73.131354\tbest:77.911526\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 12\n",
      "Loss train i: 3.5219497680664062\n",
      "Substage:1\tEpoch:14/100\tIteration: 14/100\tloss (train):3.521950\tloss (buffer on valid):68.981867\tbest:73.131354\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 12\n",
      "Loss train i: 3.5154521465301514\n",
      "Substage:1\tEpoch:15/100\tIteration: 15/100\tloss (train):3.515452\tloss (buffer on valid):65.176550\tbest:68.981867\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 12\n",
      "Loss train i: 3.5089662075042725\n",
      "Substage:1\tEpoch:16/100\tIteration: 16/100\tloss (train):3.508966\tloss (buffer on valid):61.816677\tbest:65.176550\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 12\n",
      "Loss train i: 3.502490282058716\n",
      "Substage:1\tEpoch:17/100\tIteration: 17/100\tloss (train):3.502490\tloss (buffer on valid):58.979957\tbest:61.816677\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 12\n",
      "Loss train i: 3.496026039123535\n",
      "Substage:1\tEpoch:18/100\tIteration: 18/100\tloss (train):3.496026\tloss (buffer on valid):56.471708\tbest:58.979957\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 12\n",
      "Loss train i: 3.489572763442993\n",
      "Substage:1\tEpoch:19/100\tIteration: 19/100\tloss (train):3.489573\tloss (buffer on valid):54.161535\tbest:56.471708\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 12\n",
      "Loss train i: 3.483130931854248\n",
      "Substage:1\tEpoch:20/100\tIteration: 20/100\tloss (train):3.483131\tloss (buffer on valid):52.012025\tbest:54.161535\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 12\n",
      "Loss train i: 3.4767005443573\n",
      "Substage:1\tEpoch:21/100\tIteration: 21/100\tloss (train):3.476701\tloss (buffer on valid):49.950995\tbest:52.012025\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 12\n",
      "Loss train i: 3.4702811241149902\n",
      "Substage:1\tEpoch:22/100\tIteration: 22/100\tloss (train):3.470281\tloss (buffer on valid):48.188913\tbest:49.950995\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 12\n",
      "Loss train i: 3.4638748168945312\n",
      "Substage:1\tEpoch:23/100\tIteration: 23/100\tloss (train):3.463875\tloss (buffer on valid):46.614062\tbest:48.188913\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 12\n",
      "Loss train i: 3.4574804306030273\n",
      "Substage:1\tEpoch:24/100\tIteration: 24/100\tloss (train):3.457480\tloss (buffer on valid):45.110757\tbest:46.614062\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 12\n",
      "Loss train i: 3.4510974884033203\n",
      "Substage:1\tEpoch:25/100\tIteration: 25/100\tloss (train):3.451097\tloss (buffer on valid):43.764911\tbest:45.110757\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 12\n",
      "Loss train i: 3.444727897644043\n",
      "Substage:1\tEpoch:26/100\tIteration: 26/100\tloss (train):3.444728\tloss (buffer on valid):42.533115\tbest:43.764911\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 12\n",
      "Loss train i: 3.4383718967437744\n",
      "Substage:1\tEpoch:27/100\tIteration: 27/100\tloss (train):3.438372\tloss (buffer on valid):41.333832\tbest:42.533115\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 12\n",
      "Loss train i: 3.4320285320281982\n",
      "Substage:1\tEpoch:28/100\tIteration: 28/100\tloss (train):3.432029\tloss (buffer on valid):40.098000\tbest:41.333832\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 12\n",
      "Loss train i: 3.4256978034973145\n",
      "Substage:1\tEpoch:29/100\tIteration: 29/100\tloss (train):3.425698\tloss (buffer on valid):39.029576\tbest:40.098000\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 12\n",
      "Loss train i: 3.419379234313965\n",
      "Substage:1\tEpoch:30/100\tIteration: 30/100\tloss (train):3.419379\tloss (buffer on valid):37.999643\tbest:39.029576\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 12\n",
      "Loss train i: 3.413071632385254\n",
      "Substage:1\tEpoch:31/100\tIteration: 31/100\tloss (train):3.413072\tloss (buffer on valid):37.011577\tbest:37.999643\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 12\n",
      "Loss train i: 3.40677547454834\n",
      "Substage:1\tEpoch:32/100\tIteration: 32/100\tloss (train):3.406775\tloss (buffer on valid):36.187498\tbest:37.011577\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 12\n",
      "Loss train i: 3.400491237640381\n",
      "Substage:1\tEpoch:33/100\tIteration: 33/100\tloss (train):3.400491\tloss (buffer on valid):35.359189\tbest:36.187498\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 12\n",
      "Loss train i: 3.394218921661377\n",
      "Substage:1\tEpoch:34/100\tIteration: 34/100\tloss (train):3.394219\tloss (buffer on valid):34.614562\tbest:35.359189\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 12\n",
      "Loss train i: 3.38795804977417\n",
      "Substage:1\tEpoch:35/100\tIteration: 35/100\tloss (train):3.387958\tloss (buffer on valid):33.842333\tbest:34.614562\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 12\n",
      "Loss train i: 3.381709337234497\n",
      "Substage:1\tEpoch:36/100\tIteration: 36/100\tloss (train):3.381709\tloss (buffer on valid):33.039853\tbest:33.842333\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 12\n",
      "Loss train i: 3.3754734992980957\n",
      "Substage:1\tEpoch:37/100\tIteration: 37/100\tloss (train):3.375473\tloss (buffer on valid):32.278194\tbest:33.039853\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 12\n",
      "Loss train i: 3.369251012802124\n",
      "Substage:1\tEpoch:38/100\tIteration: 38/100\tloss (train):3.369251\tloss (buffer on valid):31.642961\tbest:32.278194\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 12\n",
      "Loss train i: 3.3630411624908447\n",
      "Substage:1\tEpoch:39/100\tIteration: 39/100\tloss (train):3.363041\tloss (buffer on valid):30.951306\tbest:31.642961\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 12\n",
      "Loss train i: 3.3568434715270996\n",
      "Substage:1\tEpoch:40/100\tIteration: 40/100\tloss (train):3.356843\tloss (buffer on valid):30.361378\tbest:30.951306\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 12\n",
      "Loss train i: 3.3506579399108887\n",
      "Substage:1\tEpoch:41/100\tIteration: 41/100\tloss (train):3.350658\tloss (buffer on valid):29.735208\tbest:30.361378\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 12\n",
      "Loss train i: 3.344486713409424\n",
      "Substage:1\tEpoch:42/100\tIteration: 42/100\tloss (train):3.344487\tloss (buffer on valid):29.253457\tbest:29.735208\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 12\n",
      "Loss train i: 3.3383278846740723\n",
      "Substage:1\tEpoch:43/100\tIteration: 43/100\tloss (train):3.338328\tloss (buffer on valid):28.768789\tbest:29.253457\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 12\n",
      "Loss train i: 3.3321824073791504\n",
      "Substage:1\tEpoch:44/100\tIteration: 44/100\tloss (train):3.332182\tloss (buffer on valid):28.215725\tbest:28.768789\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 12\n",
      "Loss train i: 3.326047897338867\n",
      "Substage:1\tEpoch:45/100\tIteration: 45/100\tloss (train):3.326048\tloss (buffer on valid):27.730603\tbest:28.215725\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 12\n",
      "Loss train i: 3.3199265003204346\n",
      "Substage:1\tEpoch:46/100\tIteration: 46/100\tloss (train):3.319927\tloss (buffer on valid):27.262576\tbest:27.730603\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 12\n",
      "Loss train i: 3.313817024230957\n",
      "Substage:1\tEpoch:47/100\tIteration: 47/100\tloss (train):3.313817\tloss (buffer on valid):26.854622\tbest:27.262576\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 12\n",
      "Loss train i: 3.307718515396118\n",
      "Substage:1\tEpoch:48/100\tIteration: 48/100\tloss (train):3.307719\tloss (buffer on valid):26.437125\tbest:26.854622\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 12\n",
      "Loss train i: 3.3016321659088135\n",
      "Substage:1\tEpoch:49/100\tIteration: 49/100\tloss (train):3.301632\tloss (buffer on valid):6.598738\tbest:26.437125\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 12\n",
      "Loss train i: 3.295558214187622\n",
      "Substage:1\tEpoch:50/100\tIteration: 50/100\tloss (train):3.295558\tloss (buffer on valid):6.614590\tbest:6.598738\tTolerance: 0/100\n",
      "Loss train i: 6.69840145111084\n",
      "Substage:1\tEpoch:50/100\tIteration: 51/100\tloss (train):6.698401\tloss (buffer on valid):6.598096\tbest:6.598738\tTolerance: 1/100\n",
      "eval_model eval_results: (4, 99) with len 11\n",
      "Loss train i: 3.2836718559265137\n",
      "Substage:1\tEpoch:51/100\tIteration: 52/100\tloss (train):3.283672\tloss (buffer on valid):6.561645\tbest:6.598096\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 12\n",
      "Loss train i: 3.2778444290161133\n",
      "Substage:1\tEpoch:52/100\tIteration: 53/100\tloss (train):3.277844\tloss (buffer on valid):6.583462\tbest:6.561645\tTolerance: 0/100\n",
      "Loss train i: 6.667107105255127\n",
      "Substage:1\tEpoch:52/100\tIteration: 54/100\tloss (train):6.667107\tloss (buffer on valid):6.572794\tbest:6.561645\tTolerance: 1/100\n",
      "Loss train i: 6.157244682312012\n",
      "Substage:1\tEpoch:52/100\tIteration: 55/100\tloss (train):6.157245\tloss (buffer on valid):6.652059\tbest:6.561645\tTolerance: 2/100\n",
      "Loss train i: 3.3241634368896484\n",
      "Substage:1\tEpoch:52/100\tIteration: 56/100\tloss (train):3.324163\tloss (buffer on valid):6.606250\tbest:6.561645\tTolerance: 3/100\n",
      "Loss train i: 6.093626499176025\n",
      "Substage:1\tEpoch:52/100\tIteration: 57/100\tloss (train):6.093626\tloss (buffer on valid):6.541885\tbest:6.561645\tTolerance: 4/100\n",
      "eval_model eval_results: (4, 99) with len 8\n",
      "Loss train i: 3.249840497970581\n",
      "Substage:1\tEpoch:53/100\tIteration: 58/100\tloss (train):3.249840\tloss (buffer on valid):6.500818\tbest:6.541885\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 12\n",
      "Loss train i: 3.244413137435913\n",
      "Substage:1\tEpoch:54/100\tIteration: 59/100\tloss (train):3.244413\tloss (buffer on valid):6.426352\tbest:6.500818\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 12\n",
      "Loss train i: 3.238961696624756\n",
      "Substage:1\tEpoch:55/100\tIteration: 60/100\tloss (train):3.238962\tloss (buffer on valid):6.396752\tbest:6.426352\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 12\n",
      "Loss train i: 3.2334907054901123\n",
      "Substage:1\tEpoch:56/100\tIteration: 61/100\tloss (train):3.233491\tloss (buffer on valid):6.499787\tbest:6.396752\tTolerance: 0/100\n",
      "Loss train i: 6.57785701751709\n",
      "Substage:1\tEpoch:56/100\tIteration: 62/100\tloss (train):6.577857\tloss (buffer on valid):6.486908\tbest:6.396752\tTolerance: 1/100\n",
      "Loss train i: 6.078774452209473\n",
      "Substage:1\tEpoch:56/100\tIteration: 63/100\tloss (train):6.078774\tloss (buffer on valid):6.514022\tbest:6.396752\tTolerance: 2/100\n",
      "Loss train i: 3.2785627841949463\n",
      "Substage:1\tEpoch:56/100\tIteration: 64/100\tloss (train):3.278563\tloss (buffer on valid):6.539011\tbest:6.396752\tTolerance: 3/100\n",
      "Loss train i: 6.013956546783447\n",
      "Substage:1\tEpoch:56/100\tIteration: 65/100\tloss (train):6.013957\tloss (buffer on valid):6.595839\tbest:6.396752\tTolerance: 4/100\n",
      "Loss train i: 3.221985101699829\n",
      "Substage:1\tEpoch:56/100\tIteration: 66/100\tloss (train):3.221985\tloss (buffer on valid):6.617287\tbest:6.396752\tTolerance: 5/100\n",
      "Loss train i: 5.023586750030518\n",
      "Substage:1\tEpoch:56/100\tIteration: 67/100\tloss (train):5.023587\tloss (buffer on valid):6.588122\tbest:6.396752\tTolerance: 6/100\n",
      "Loss train i: 4.087807655334473\n",
      "Substage:1\tEpoch:56/100\tIteration: 68/100\tloss (train):4.087808\tloss (buffer on valid):6.613182\tbest:6.396752\tTolerance: 7/100\n",
      "Loss train i: 3.050382137298584\n",
      "Substage:1\tEpoch:56/100\tIteration: 69/100\tloss (train):3.050382\tloss (buffer on valid):6.565040\tbest:6.396752\tTolerance: 8/100\n",
      "Loss train i: 8.911349296569824\n",
      "Substage:1\tEpoch:56/100\tIteration: 70/100\tloss (train):8.911349\tloss (buffer on valid):6.543840\tbest:6.396752\tTolerance: 9/100\n",
      "Loss train i: 5.632122993469238\n",
      "Substage:1\tEpoch:56/100\tIteration: 71/100\tloss (train):5.632123\tloss (buffer on valid):6.591551\tbest:6.396752\tTolerance: 10/100\n",
      "Loss train i: 6.728065490722656\n",
      "Substage:1\tEpoch:56/100\tIteration: 72/100\tloss (train):6.728065\tloss (buffer on valid):6.566021\tbest:6.396752\tTolerance: 11/100\n",
      "Loss train i: 2.8972067832946777\n",
      "Substage:1\tEpoch:56/100\tIteration: 73/100\tloss (train):2.897207\tloss (buffer on valid):6.467229\tbest:6.396752\tTolerance: 12/100\n",
      "Loss train i: 3.168938636779785\n",
      "Substage:1\tEpoch:57/100\tIteration: 74/100\tloss (train):3.168939\tloss (buffer on valid):6.403484\tbest:6.396752\tTolerance: 13/100\n",
      "Loss train i: 6.435719013214111\n",
      "Substage:1\tEpoch:57/100\tIteration: 75/100\tloss (train):6.435719\tloss (buffer on valid):6.319126\tbest:6.396752\tTolerance: 14/100\n",
      "eval_model eval_results: (4, 99) with len 11\n",
      "Loss train i: 3.159947395324707\n",
      "Substage:1\tEpoch:58/100\tIteration: 76/100\tloss (train):3.159947\tloss (buffer on valid):6.216546\tbest:6.319126\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 12\n",
      "Loss train i: 3.155383586883545\n",
      "Substage:1\tEpoch:59/100\tIteration: 77/100\tloss (train):3.155384\tloss (buffer on valid):6.143502\tbest:6.216546\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 12\n",
      "Loss train i: 3.1507303714752197\n",
      "Substage:1\tEpoch:60/100\tIteration: 78/100\tloss (train):3.150730\tloss (buffer on valid):6.143321\tbest:6.143502\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 12\n",
      "Loss train i: 3.145996570587158\n",
      "Substage:1\tEpoch:61/100\tIteration: 79/100\tloss (train):3.145997\tloss (buffer on valid):6.119412\tbest:6.143321\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 12\n",
      "Loss train i: 3.1411914825439453\n",
      "Substage:1\tEpoch:62/100\tIteration: 80/100\tloss (train):3.141191\tloss (buffer on valid):6.115620\tbest:6.119412\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 12\n",
      "Loss train i: 3.1363229751586914\n",
      "Substage:1\tEpoch:63/100\tIteration: 81/100\tloss (train):3.136323\tloss (buffer on valid):6.149529\tbest:6.115620\tTolerance: 0/100\n",
      "Loss train i: 6.366684913635254\n",
      "Substage:1\tEpoch:63/100\tIteration: 82/100\tloss (train):6.366685\tloss (buffer on valid):6.085138\tbest:6.115620\tTolerance: 1/100\n",
      "eval_model eval_results: (4, 99) with len 11\n",
      "Loss train i: 3.12658429145813\n",
      "Substage:1\tEpoch:64/100\tIteration: 83/100\tloss (train):3.126584\tloss (buffer on valid):6.041863\tbest:6.085138\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 12\n",
      "Loss train i: 3.1217093467712402\n",
      "Substage:1\tEpoch:65/100\tIteration: 84/100\tloss (train):3.121709\tloss (buffer on valid):5.987825\tbest:6.041863\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 12\n",
      "Loss train i: 3.1167829036712646\n",
      "Substage:1\tEpoch:66/100\tIteration: 85/100\tloss (train):3.116783\tloss (buffer on valid):5.955359\tbest:5.987825\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 12\n",
      "Loss train i: 3.111811637878418\n",
      "Substage:1\tEpoch:67/100\tIteration: 86/100\tloss (train):3.111812\tloss (buffer on valid):6.003324\tbest:5.955359\tTolerance: 0/100\n",
      "Loss train i: 6.321596622467041\n",
      "Substage:1\tEpoch:67/100\tIteration: 87/100\tloss (train):6.321597\tloss (buffer on valid):6.049742\tbest:5.955359\tTolerance: 1/100\n",
      "Loss train i: 5.861172199249268\n",
      "Substage:1\tEpoch:67/100\tIteration: 88/100\tloss (train):5.861172\tloss (buffer on valid):6.081344\tbest:5.955359\tTolerance: 2/100\n",
      "Loss train i: 3.152820587158203\n",
      "Substage:1\tEpoch:67/100\tIteration: 89/100\tloss (train):3.152821\tloss (buffer on valid):6.101309\tbest:5.955359\tTolerance: 3/100\n",
      "Loss train i: 5.803872585296631\n",
      "Substage:1\tEpoch:67/100\tIteration: 90/100\tloss (train):5.803873\tloss (buffer on valid):6.082856\tbest:5.955359\tTolerance: 4/100\n",
      "Loss train i: 3.1044209003448486\n",
      "Substage:1\tEpoch:67/100\tIteration: 91/100\tloss (train):3.104421\tloss (buffer on valid):6.143834\tbest:5.955359\tTolerance: 5/100\n",
      "Loss train i: 4.839813709259033\n",
      "Substage:1\tEpoch:67/100\tIteration: 92/100\tloss (train):4.839814\tloss (buffer on valid):6.102910\tbest:5.955359\tTolerance: 6/100\n",
      "Loss train i: 3.9358649253845215\n",
      "Substage:1\tEpoch:67/100\tIteration: 93/100\tloss (train):3.935865\tloss (buffer on valid):6.083179\tbest:5.955359\tTolerance: 7/100\n",
      "Loss train i: 2.9409093856811523\n",
      "Substage:1\tEpoch:67/100\tIteration: 94/100\tloss (train):2.940909\tloss (buffer on valid):6.104274\tbest:5.955359\tTolerance: 8/100\n",
      "Loss train i: 8.604701042175293\n",
      "Substage:1\tEpoch:67/100\tIteration: 95/100\tloss (train):8.604701\tloss (buffer on valid):6.173513\tbest:5.955359\tTolerance: 9/100\n",
      "Loss train i: 5.449737071990967\n",
      "Substage:1\tEpoch:67/100\tIteration: 96/100\tloss (train):5.449737\tloss (buffer on valid):6.127456\tbest:5.955359\tTolerance: 10/100\n",
      "Loss train i: 6.492332935333252\n",
      "Substage:1\tEpoch:67/100\tIteration: 97/100\tloss (train):6.492333\tloss (buffer on valid):6.137829\tbest:5.955359\tTolerance: 11/100\n",
      "Loss train i: 2.7877888679504395\n",
      "Substage:1\tEpoch:67/100\tIteration: 98/100\tloss (train):2.787789\tloss (buffer on valid):6.104113\tbest:5.955359\tTolerance: 12/100\n",
      "Loss train i: 3.052077531814575\n",
      "Substage:1\tEpoch:68/100\tIteration: 99/100\tloss (train):3.052078\tloss (buffer on valid):6.070437\tbest:5.955359\tTolerance: 13/100\n",
      "Loss train i: 6.200275897979736\n",
      "Substage:1\tEpoch:68/100\tIteration: 100/100\tloss (train):6.200276\tloss (buffer on valid):6.000135\tbest:5.955359\tTolerance: 14/100\n"
     ]
    }
   ],
   "source": [
    "screenshot, d = train_model(model, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': [[3.6140742, 7.227595, 6.70115, 3.6506257, 6.622177, 3.5594428, 5.5601854, 4.5101857, 3.3610966, 9.822557, 6.1879897, 7.4154177, 3.217002], [3.5472631, 7.108092, 6.5936656, 3.5849123, 6.519177, 3.498232, 5.4680862, 4.434257, 3.303298, 9.674823, 6.095251, 7.3001394, 3.158329], [3.4862378, 6.9968557, 6.4919667, 3.5228007, 6.420512, 3.4389234, 5.378127, 4.3604865, 3.2469144, 9.530659, 6.004282, 7.186862, 3.1008427], [3.4265602, 6.888028, 6.392318, 3.462064, 6.3238683, 3.38082, 5.289957, 4.2882223, 3.191661, 9.389341, 5.9149823, 7.0757785, 3.0445414], [3.3680334, 6.781307, 6.2945023, 3.402525, 6.2290397, 3.323862, 5.2035356, 4.2173944, 3.1375017, 9.250699, 5.8273, 6.96684, 2.9893854], [3.310612, 6.676588, 6.198433, 3.3441286, 6.135934, 3.2680173, 5.1187944, 4.1479607, 3.0844135, 9.114625, 5.7411876, 6.8599553, 2.9353518], [3.2542937, 6.57382, 6.104082, 3.286867, 6.0445, 3.2132654, 5.035672, 4.079875, 3.0323608, 8.981022, 5.6565876, 6.7550373, 2.882391], [3.199048, 6.4729385, 6.011403, 3.2307167, 5.954699, 3.1595926, 4.9541283, 4.013123, 2.981333, 8.84982, 5.5734644, 6.652027, 2.8304787], [3.1448717, 6.373915, 5.9203672, 3.1756618, 5.866491, 3.106972, 4.8741126, 3.9476697, 2.9313087, 8.720971, 5.4917893, 6.550873, 2.7795956], [3.0917516, 6.276715, 5.8309484, 3.1216917, 5.7798524, 3.055396, 4.7955976, 3.8834858, 2.8822534, 8.594389, 5.411502, 6.451497, 2.7296948], [3.0396466, 6.181275, 5.743085, 3.0687582, 5.6947193, 3.0048137, 4.718524, 3.820527, 2.8341398, 8.470017, 5.332571, 6.353852, 2.680747], [2.9885287, 6.087545, 5.6567435, 3.01684, 5.611058, 2.955201, 4.6428566, 3.7587588, 2.786937, 8.3477955, 5.254969, 6.257899, 2.6327434], [2.938389, 5.9954934, 5.571897, 2.9659274, 5.5288467, 2.906548, 4.5685687, 3.6981661, 2.7406378, 8.227682, 5.1786623, 6.1635904, 2.5856485], [2.8891943, 5.905073, 5.4885, 2.9159844, 5.4480395, 2.8588204, 4.495615, 3.63871, 2.695209, 8.109624, 5.1036253, 6.070892, 2.5394468], [2.840935, 5.816259, 5.4065347, 2.867003, 5.3686175, 2.8120086, 4.423981, 3.5803823, 2.6506484, 7.9935904, 5.029849, 5.9797835, 2.4941278], [2.7935982, 5.729021, 5.3259783, 2.8189735, 5.290561, 2.7661016, 4.3536563, 3.523166, 2.6069455, 7.8795466, 4.9573045, 5.890234, 2.449684], [2.7471712, 5.6433372, 5.2468085, 2.7718725, 5.2138357, 2.7210689, 4.284589, 3.4670184, 2.5640638, 7.7674284, 4.885952, 5.8022013, 2.4060824], [2.7016246, 5.559161, 5.1689944, 2.7256818, 5.1384206, 2.6769078, 4.216769, 3.4119346, 2.5220046, 7.657213, 4.815779, 5.715659, 2.3633113], [2.656938, 5.476451, 5.092495, 2.6803606, 5.064266, 2.6335788, 4.150148, 3.3578637, 2.4807289, 7.5488434, 4.746747, 5.63056, 2.3213415], [2.6130903, 5.395178, 5.017292, 2.6359031, 4.9913616, 2.591079, 4.084716, 3.3048081, 2.440242, 7.442298, 4.6788564, 5.546901, 2.2801824], [2.5700874, 5.3153296, 4.9433737, 2.5923038, 4.9196935, 2.5493965, 4.02045, 3.2527452, 2.4005208, 7.3375287, 4.612073, 5.4646363, 2.2398016], [2.5279028, 5.236869, 4.870706, 2.5495377, 4.849228, 2.5085087, 3.9573283, 3.2016501, 2.3615522, 7.234499, 4.5463786, 5.38374, 2.2001836], [2.48651, 5.1597543, 4.7992578, 2.5075836, 4.7799397, 2.46839, 3.8953207, 3.151494, 2.3233166, 7.133174, 4.4817486, 5.3041844, 2.1613135], [2.4458952, 5.083966, 4.7290077, 2.4664245, 4.7117987, 2.429023, 3.8344023, 3.1022527, 2.285792, 7.033512, 4.418161, 5.225939, 2.1231713], [2.4060395, 5.0094743, 4.659931, 2.426043, 4.644787, 2.3903954, 3.7745547, 3.053916, 2.248973, 6.935488, 4.355604, 5.148986, 2.0857496], [2.3669322, 4.9362507, 4.592006, 2.3864217, 4.578882, 2.3524907, 3.7157497, 3.006455, 2.212833, 6.839058, 4.2940454, 5.073291, 2.0490265], [2.328553, 4.8642726, 4.525211, 2.3475451, 4.514062, 2.3152986, 3.6579757, 2.9598675, 2.17737, 6.7441974, 4.233478, 4.9988317, 2.0129888], [2.290886, 4.7935047, 4.4595175, 2.309391, 4.4502954, 2.2787895, 3.6011932, 2.914112, 2.142554, 6.6508584, 4.173868, 4.925575, 1.9776149], [2.2539077, 4.72392, 4.3949003, 2.2719364, 4.387554, 2.242947, 3.5453825, 2.8691711, 2.1083703, 6.5590057, 4.1151896, 4.853491, 1.9428785], [2.217596, 4.655485, 4.331332, 2.2351687, 4.325819, 2.2077556, 3.4905267, 2.8250294, 2.074813, 6.4686203, 4.0574393, 4.782575, 1.9087926], [2.181961, 4.5881977, 4.268805, 2.1990852, 4.2650824, 2.1732163, 3.4366188, 2.7816818, 2.041882, 6.379687, 4.0006084, 4.712805, 1.8753368], [2.1469793, 4.522032, 4.2072945, 2.1636662, 4.205321, 2.139315, 3.3836405, 2.7391043, 2.0095577, 6.2921696, 3.944679, 4.644163, 1.8425072], [2.1126478, 4.456974, 4.1467886, 2.12891, 4.146525, 2.1060443, 3.3315825, 2.6972954, 1.9778366, 6.2060485, 3.889636, 4.5766273, 1.8102891], [2.0789533, 4.3929977, 4.0872707, 2.0948, 4.088675, 2.0733895, 3.2804205, 2.6562333, 1.9467036, 6.1213, 3.8354666, 4.5101857, 1.7786816], [2.0458875, 4.330088, 4.0287333, 2.0613332, 4.031761, 2.041346, 3.2301445, 2.615911, 1.9161537, 6.0378995, 3.7821584, 4.444823, 1.7476696], [2.0134346, 4.268219, 3.971155, 2.02849, 3.9757617, 2.0098982, 3.1807368, 2.5763123, 1.8861754, 5.955821, 3.7296913, 4.3805127, 1.7172357], [1.9815823, 4.207363, 3.9145117, 1.9962461, 3.920645, 1.9790163, 3.1321654, 2.5374053, 1.8567386, 5.875026, 3.6780357, 4.317226, 1.687363], [1.9503123, 4.147503, 3.8587892, 1.964597, 3.8664036, 1.9486988, 3.0844216, 2.4991868, 1.8278444, 5.795497, 3.6271842, 4.254947, 1.658042], [1.9196151, 4.0886173, 3.8039565, 1.9335209, 3.813013, 1.9189229, 3.0374782, 2.4616325, 1.7994657, 5.717201, 3.5771139, 4.1936555, 1.6292584], [1.8894761, 4.03069, 3.750006, 1.903014, 3.7604685, 1.8896914, 2.991339, 2.4247463, 1.7716137, 5.640132, 3.5278273, 4.1333537, 1.6010188], [1.8599, 3.973716, 3.6969292, 1.8730699, 3.7087536, 1.8609929, 2.9459848, 2.3885117, 1.7442768, 5.564265, 3.4793103, 4.074017, 1.5733013], [1.8308672, 3.917665, 3.6447074, 1.8436732, 3.6578498, 1.8328128, 2.9013958, 2.352908, 1.7174323, 5.489564, 3.4315395, 4.015617, 1.5460935], [1.8023568, 3.86251, 3.593313, 1.8148046, 3.6077275, 1.8051295, 2.8575475, 2.3179123, 1.6910671, 5.4159985, 3.3844893, 3.9581223, 1.5193775], [1.7743545, 3.808235, 3.5427291, 1.7864478, 3.5583758, 1.777936, 2.8144255, 2.2835135, 1.6651741, 5.343551, 3.3381562, 3.9015245, 1.4931425], [1.7468477, 3.754825, 3.4929402, 1.7585936, 3.509781, 1.7512224, 2.7720194, 2.2497015, 1.6397436, 5.272205, 3.2925339, 3.8458178, 1.4673955], [1.71984, 3.7022705, 3.443942, 1.7312452, 3.4619412, 1.7249904, 2.7303245, 2.2164714, 1.6147715, 5.20194, 3.2476006, 3.790973, 1.4421182], [1.69331, 3.6505454, 3.3957129, 1.70438, 3.4148345, 1.6992244, 2.6893249, 2.183815, 1.5902532, 5.1327443, 3.2033567, 3.7369924, 1.417314], [1.6672605, 3.5996442, 3.3482492, 1.6780031, 3.3684554, 1.6739187, 2.649012, 2.1517186, 1.566174, 5.064595, 3.1597836, 3.683852, 1.3929663], [1.6416775, 3.5495412, 3.3015308, 1.6520965, 3.3227825, 1.6490643, 2.6093736, 2.1201713, 1.5425293, 4.997475, 3.1168716, 3.6315355, 1.3690679], [1.6165494, 3.5002246, 3.2555444, 1.6266497, 3.2778022, 1.6246464, 2.5703886, 2.089157, 1.5193045, 4.9313574, 3.0746055, 3.5800247, 1.3456022], [1.5918607, 3.4516711, 3.2102618, 1.6016465, 3.2334945, 1.6006451, 2.5320358, 2.0586588, 1.4964846, 4.8662186, 3.0329633, 3.5292947, 1.3225536], [1.5676018, 3.403867, 3.165674, 1.577077, 3.1898494, 1.5770582, 2.4943085, 2.028667, 1.4740629, 4.802044, 2.9919405, 3.479341, 1.2999201], [1.5437633, 3.3568013, 3.1217692, 1.5529327, 3.146855, 1.5538775, 2.4571946, 1.999175, 1.4520347, 4.738816, 2.951524, 3.430151, 1.2776961], [1.5203435, 3.3104649, 3.078536, 1.529212, 3.104508, 1.5311017, 2.4206889, 1.9701754, 1.4303954, 4.676526, 2.9117136, 3.3817182, 1.2558763], [1.4973358, 3.2648458, 3.035967, 1.505903, 3.062788, 1.5087159, 2.3847759, 1.9416561, 1.4091346, 4.61515, 2.8724906, 3.334022, 1.2344532], [1.4747279, 3.2199264, 2.9940503, 1.483, 3.0216944, 1.4867201, 2.3494499, 1.913613, 1.3882499, 4.554677, 2.8338504, 3.2870512, 1.2134159], [1.4525115, 3.1756914, 2.9527721, 1.4604855, 2.9812088, 1.4650993, 2.314693, 1.8860335, 1.3677307, 4.495093, 2.7957866, 3.2407947, 1.1927652], [1.430686, 3.1321385, 2.912133, 1.4383683, 2.9413297, 1.4438593, 2.2805037, 1.8589114, 1.3475765, 4.4363847, 2.7582908, 3.1952386, 1.1724955], [1.4092481, 3.089255, 2.8721163, 1.4166377, 2.9020479, 1.422992, 2.246872, 1.8322427, 1.3277801, 4.378537, 2.721353, 3.1503708, 1.1525884], [1.3881814, 3.0470219, 2.8327038, 1.3952757, 2.8633409, 1.402477, 2.2137828, 1.8060129, 1.3083293, 4.3215327, 2.684961, 3.1061916, 1.1330539], [1.3674889, 3.005436, 2.7938979, 1.3742937, 2.8252134, 1.3823237, 2.181232, 1.7802225, 1.2892247, 4.2653556, 2.649101, 3.062681, 1.1138808], [1.3471591, 2.9644809, 2.7556787, 1.3536727, 2.7876449, 1.3625193, 2.1492028, 1.7548513, 1.2704506, 4.20998, 2.6137624, 3.0198236, 1.0950497], [1.3271718, 2.9241383, 2.7180293, 1.3334002, 2.7506201, 1.3430499, 2.1176872, 1.7298946, 1.2520063, 4.1554065, 2.578942, 2.9776149, 1.0765648], [1.3075346, 2.8844123, 2.6809525, 1.3134819, 2.7141447, 1.3239213, 2.0866854, 1.7053479, 1.2338878, 4.101627, 2.5446343, 2.9360447, 1.0584202], [1.2882417, 2.8452923, 2.6444335, 1.2939047, 2.6782038, 1.3051205, 2.0561852, 1.6812072, 1.2160871, 4.048622, 2.510826, 2.8950963, 1.0406023], [1.2692778, 2.8067572, 2.6084602, 1.2746575, 2.6427834, 1.2866331, 2.0261664, 1.6574502, 1.1985896, 3.9963734, 2.4775102, 2.8547616, 1.0231057], [1.2506373, 2.7688005, 2.5730252, 1.2557359, 2.60788, 1.2684608, 1.9966264, 1.6340721, 1.1813922, 3.9448671, 2.4446735, 2.8150249, 1.0059204], [1.232312, 2.731406, 2.5381193, 1.2371305, 2.5734773, 1.2505878, 1.9675512, 1.611066, 1.1644875, 3.8940897, 2.4123085, 2.7758784, 0.9890362], [1.2142946, 2.6945615, 2.5037224, 1.2188346, 2.5395713, 1.2330151, 1.9389392, 1.5884339, 1.1478714, 3.8440325, 2.3804116, 2.7373166, 0.97245806], [1.196583, 2.6582625, 2.4698331, 1.2008393, 2.5061421, 1.215725, 1.9107687, 1.5661511, 1.1315285, 3.7946708, 2.3489594, 2.699314, 0.9561667], [1.1791582, 2.6224856, 2.4364343, 1.1831365, 2.4731865, 1.1987169, 1.8830389, 1.5442196, 1.115461, 3.7460043, 2.3179593, 2.6618764, 0.9401665], [1.162029, 2.5872395, 2.403532, 1.1657318, 2.4407032, 1.1819915, 1.8557508, 1.5226389, 1.0996679, 3.6980267, 2.2874095, 2.624998, 0.92445934], [1.1451912, 2.5525117, 2.3711197, 1.148621, 2.4086843, 1.1655445, 1.8288916, 1.5014031, 1.0841455, 3.650725, 2.2572944, 2.5886655, 0.9090328], [1.1286293, 2.5182862, 2.3391807, 1.1317881, 2.3771172, 1.1493703, 1.8024491, 1.4805042, 1.0688883, 3.604092, 2.2276149, 2.552875, 0.89388895], [1.1123531, 2.4845712, 2.3077219, 1.1152446, 2.3460085, 1.1334728, 1.7764344, 1.4599545, 1.0539014, 3.5581203, 2.1983702, 2.51762, 0.87902117], [1.096356, 2.4513526, 2.2767253, 1.0989779, 2.315345, 1.117836, 1.7508259, 1.4397309, 1.0391692, 3.512794, 2.1695454, 2.4828835, 0.8644192], [1.0806311, 2.4186225, 2.2461784, 1.0829797, 2.2851152, 1.1024547, 1.7256207, 1.4198279, 1.024684, 3.4680943, 2.1411273, 2.448654, 0.85007286], [1.0651654, 2.3863661, 2.2160735, 1.0672457, 2.2553134, 1.0873222, 1.7008075, 1.4002372, 1.0104439, 3.4240158, 2.1131124, 2.4149258, 0.8359849], [1.0499632, 2.3545787, 2.1864116, 1.0517744, 2.2259355, 1.0724419, 1.676385, 1.38095, 0.99644566, 3.3805497, 2.0854995, 2.381694, 0.8221524], [1.0350187, 2.3232563, 2.1571877, 1.0365536, 2.196969, 1.0578053, 1.6523398, 1.3619553, 0.9826828, 3.3376808, 2.0582712, 2.3489413, 0.8085611], [1.0203158, 2.2923799, 2.1283858, 1.0215774, 2.1684074, 1.0434047, 1.6286659, 1.3432544, 0.9691523, 3.2954097, 2.031434, 2.316672, 0.7952249], [1.0058677, 2.2619572, 2.1000116, 1.0068567, 2.1402562, 1.0292444, 1.6053686, 1.3248512, 0.9558555, 3.253726, 2.00498, 2.2848754, 0.7821318], [0.9916601, 2.2319758, 2.0720506, 0.99238205, 2.1125019, 1.0153146, 1.582437, 1.3067384, 0.9427862, 3.2126203, 1.9789022, 2.2535474, 0.7692787], [0.97769547, 2.2024271, 2.0444944, 0.9781484, 2.0851398, 1.0016128, 1.5598657, 1.2889096, 0.9299407, 3.172085, 1.9531971, 2.222679, 0.756657], [0.9639652, 2.1733036, 2.0173402, 0.9641539, 2.0581603, 0.9881319, 1.5376422, 1.2713563, 0.9173104, 3.1321046, 1.9278523, 2.1922593, 0.74426013], [0.95045996, 2.1445942, 1.9905784, 0.9503907, 2.0315607, 0.97486943, 1.51576, 1.2540716, 0.904889, 3.0926647, 1.9028603, 2.1622763, 0.7320778], [0.9371729, 2.1162887, 1.9641942, 0.9368423, 2.0053282, 0.961815, 1.494212, 1.2370496, 0.8926704, 3.0537572, 1.8782178, 2.1327262, 0.7201091], [0.92410064, 2.0883825, 1.9381877, 0.9235113, 1.9794577, 0.948967, 1.472989, 1.2202843, 0.8806533, 3.0153728, 1.8539119, 2.1035976, 0.7083477], [0.9112412, 2.0608675, 1.9125457, 0.9103893, 1.9539398, 0.9363192, 1.4520833, 1.2037697, 0.8688312, 2.9775043, 1.8299389, 2.0748835, 0.6967896], [0.8985962, 2.0337393, 1.887272, 0.8974804, 1.9287776, 0.9238727, 1.4315019, 1.1875114, 0.8572045, 2.9401457, 1.8063011, 2.0465844, 0.6854376], [0.8861619, 2.0069935, 1.8623655, 0.8847837, 1.9039712, 0.9116256, 1.4112402, 1.1715037, 0.8457796, 2.903296, 1.7829958, 2.0186982, 0.6742922], [0.8739381, 1.9806291, 1.8378233, 0.87229466, 1.8795142, 0.8995791, 1.3912967, 1.1557437, 0.83454794, 2.8669477, 1.7600219, 1.9912196, 0.66334784], [0.86191815, 1.9546351, 1.8136339, 0.86000884, 1.8553995, 0.88772094, 1.3716573, 1.140226, 0.8235004, 2.8310902, 1.7373627, 1.9641312, 0.65259755], [0.8500942, 1.929004, 1.789789, 0.84791845, 1.8316197, 0.8760524, 1.3523202, 1.1249468, 0.81263644, 2.7957213, 1.7150202, 1.9374341, 0.6420455], [0.83846587, 1.9037397, 1.7662933, 0.8360252, 1.8081756, 0.86457574, 1.3332866, 1.1099117, 0.80195874, 2.7608352, 1.6929944, 1.9111254, 0.631686], [0.8270284, 1.8788285, 1.743131, 0.82432073, 1.7850568, 0.8532783, 1.3145406, 1.0951012, 0.79145384, 2.7264175, 1.6712743, 1.8851943, 0.62151706], [0.8157806, 1.8542699, 1.720305, 0.8128058, 1.7622619, 0.8421671, 1.2960851, 1.0805178, 0.78112864, 2.692467, 1.6498625, 1.8596361, 0.6115312], [0.8047117, 1.8300538, 1.6978022, 0.801467, 1.7397758, 0.8312267, 1.2779053, 1.0661519, 0.77096903, 2.658965, 1.6287411, 1.834435, 0.6017167], [0.79381543, 1.8061692, 1.6756124, 0.7903049, 1.7175943, 0.8204511, 1.2599939, 1.0519993, 0.7609708, 2.6259072, 1.607909, 1.8095875, 0.59207416], [0.78309447, 1.7826139, 1.6537335, 0.77931756, 1.6957121, 0.809847, 1.2423551, 1.038059, 0.7511362, 2.5932899, 1.587365, 1.7850958, 0.58261067]], 'train_mse': [[1.8348542, 5.450862, 4.9268403, 1.8787048, 4.852622, 1.7922447, 3.7953393, 2.74769, 1.6009489, 8.064751, 4.432519, 5.662278, 1.4661918], [1.7987717, 5.361916, 4.849804, 1.8433641, 4.77994, 1.761302, 3.7334592, 2.7019327, 1.5732731, 7.947093, 4.3698163, 5.5769954, 1.4374747], [1.7676644, 5.280559, 4.777945, 1.8110524, 4.7110343, 1.7317132, 3.6731799, 2.6578014, 1.5464883, 7.832488, 4.308366, 5.493197, 1.4094268], [1.7373857, 5.2010922, 4.707619, 1.7796, 4.643636, 1.7028146, 3.6141744, 2.6146612, 1.5203172, 7.720211, 4.248065, 5.4110713, 1.3820417], [1.7077341, 5.123205, 4.6385965, 1.7488147, 4.5775228, 1.6745337, 3.5563915, 2.5724325, 1.494719, 7.6100903, 4.1888633, 5.3305726, 1.3552877], [1.6786764, 5.04681, 4.5708117, 1.7186621, 4.51262, 1.6468515, 3.4997725, 2.531081, 1.4696735, 7.5020204, 4.130716, 5.2516146, 1.3291411], [1.6502049, 4.971851, 4.504232, 1.689135, 4.448883, 1.6197592, 3.4442725, 2.4905803, 1.4451662, 7.3959227, 4.073583, 5.1741257, 1.3035711], [1.6223133, 4.8982844, 4.4388285, 1.660221, 4.3862796, 1.5932437, 3.3898475, 2.4509082, 1.4211816, 7.291729, 4.0174303, 5.0980477, 1.2785525], [1.5949922, 4.826079, 4.3745737, 1.6319102, 4.324778, 1.5672927, 3.3364608, 2.4120443, 1.3977067, 7.1893883, 3.9622254, 5.0233264, 1.2540662], [1.5682325, 4.755202, 4.311441, 1.6041892, 4.2643514, 1.5418929, 3.2840884, 2.3739693, 1.3747264, 7.088846, 3.9079432, 4.9499183, 1.2300965], [1.5420227, 4.6856227, 4.249404, 1.5770463, 4.204974, 1.5170313, 3.2327006, 2.3366604, 1.3522271, 6.9900556, 3.85456, 4.877788, 1.2066299], [1.5163511, 4.617304, 4.1884384, 1.550471, 4.1466217, 1.4926927, 3.182272, 2.300098, 1.330197, 6.8929715, 3.802058, 4.8068995, 1.183656], [1.4912066, 4.5502143, 4.1285195, 1.5244516, 4.0892706, 1.4688663, 3.1327782, 2.2642655, 1.3086252, 6.7975526, 3.7504158, 4.737223, 1.16116], [1.4665792, 4.4843287, 4.0696244, 1.4989775, 4.032899, 1.4455417, 3.0841951, 2.229148, 1.2875011, 6.703767, 3.6996164, 4.6687293, 1.1391301], [1.4424586, 4.4196196, 4.011732, 1.4740368, 3.9774852, 1.4227057, 3.0365043, 2.1947289, 1.2668158, 6.6115746, 3.6496487, 4.601397, 1.1175554], [1.4188329, 4.356061, 3.9548216, 1.4496188, 3.9230072, 1.4003433, 2.98969, 2.1609898, 1.2465575, 6.5209427, 3.6004858, 4.5351977, 1.0964288], [1.3956916, 4.293631, 3.898875, 1.425712, 3.8694458, 1.3784447, 2.9437258, 2.1279154, 1.2267171, 6.4318337, 3.5521092, 4.4701076, 1.0757387], [1.3730233, 4.2323003, 3.843874, 1.4023023, 3.8167806, 1.357003, 2.8985953, 2.0954897, 1.2072872, 6.344219, 3.504507, 4.406107, 1.0554787], [1.3508192, 4.172044, 3.7898002, 1.3793766, 3.7649908, 1.3360081, 2.8542786, 2.063695, 1.1882573, 6.2580647, 3.4576616, 4.3431654, 1.035637], [1.329069, 4.112838, 3.736632, 1.3569232, 3.7140596, 1.3154498, 2.8107564, 2.032518, 1.1696177, 6.173335, 3.4115558, 4.28126, 1.0162019], [1.307762, 4.0546556, 3.68435, 1.3349301, 3.6639678, 1.2953155, 2.7680104, 2.0019445, 1.1513578, 6.0899987, 3.3661754, 4.220369, 0.9971637], [1.2868882, 3.9974754, 3.6329343, 1.313388, 3.614698, 1.2755933, 2.7260244, 1.9719584, 1.13347, 6.008023, 3.3215065, 4.1604705, 0.9785156], [1.266437, 3.9412744, 3.5823708, 1.2922894, 3.5662355, 1.2562733, 2.6847882, 1.9425447, 1.1159486, 5.9273844, 3.2775362, 4.1015463, 0.9602492], [1.2463986, 3.886035, 3.5326421, 1.2716244, 3.5185614, 1.2373458, 2.644282, 1.9136888, 1.098782, 5.848052, 3.2342498, 4.043575, 0.94235444], [1.2267631, 3.8317354, 3.48373, 1.2513802, 3.4716606, 1.2188014, 2.6044888, 1.8853781, 1.081961, 5.769999, 3.1916368, 3.986539, 0.92482334], [1.2075212, 3.7783527, 3.4356194, 1.2315462, 3.4255161, 1.200631, 2.5653934, 1.8576013, 1.0654784, 5.693199, 3.1496828, 3.9304202, 0.90764934], [1.188664, 3.7258682, 3.388293, 1.212113, 3.3801124, 1.1828285, 2.5269816, 1.8303497, 1.0493271, 5.617625, 3.1083767, 3.8751988, 0.8908245], [1.1701841, 3.6742642, 3.3417377, 1.1930715, 3.3354352, 1.1653849, 2.4892406, 1.803611, 1.0335023, 5.5432515, 3.067707, 3.8208575, 0.8743413], [1.1520733, 3.6235228, 3.2959406, 1.1744136, 3.291466, 1.1482904, 2.4521546, 1.7773715, 1.0179965, 5.470054, 3.0276606, 3.7673824, 0.8581911], [1.1343231, 3.5736225, 3.2508812, 1.1561298, 3.2481906, 1.1315346, 2.41571, 1.751617, 1.0028025, 5.3980083, 2.9882236, 3.7147546, 0.8423663], [1.116924, 3.524548, 3.206543, 1.1382097, 3.2055938, 1.1151106, 2.3798935, 1.7263356, 0.98791325, 5.327091, 2.9493873, 3.6629567, 0.8268602], [1.0998684, 3.4762847, 3.1629105, 1.1206459, 3.163662, 1.0990148, 2.3446975, 1.701517, 0.97332287, 5.257284, 2.911143, 3.611974, 0.8116658], [1.0831494, 3.428815, 3.1199691, 1.10343, 3.1223829, 1.0832367, 2.3101063, 1.6771504, 0.9590218, 5.1885605, 2.8734741, 3.56179, 0.7967763], [1.0667579, 3.3821182, 3.077708, 1.086553, 3.0817418, 1.0677671, 2.2761056, 1.6532265, 0.9450026, 5.120902, 2.836371, 3.5123904, 0.7821853], [1.0506858, 3.3361788, 3.0361161, 1.0700088, 3.0417287, 1.0526013, 2.242685, 1.6297371, 0.93126315, 5.0542884, 2.7998264, 3.4637668, 0.7678906], [1.0349281, 3.2909813, 2.9951868, 1.053791, 3.00233, 1.0377322, 2.2098343, 1.606672, 0.91779673, 4.9887013, 2.7638288, 3.4159055, 0.7538837], [1.0194801, 3.2465098, 2.9549077, 1.0378898, 2.9635355, 1.02315, 2.1775398, 1.5840209, 0.9045944, 4.924118, 2.7283633, 3.3687873, 0.74015707], [1.0043347, 3.2027516, 2.915264, 1.0222988, 2.9253302, 1.0088471, 2.1457899, 1.5617748, 0.89165, 4.860518, 2.6934204, 3.3223963, 0.72670406], [0.9894845, 3.1596923, 2.8762379, 1.0070084, 2.8877046, 0.99481684, 2.1145723, 1.5399253, 0.8789558, 4.7978845, 2.6589904, 3.2767239, 0.7135189], [0.9749221, 3.11732, 2.837819, 0.992011, 2.8506482, 0.9810515, 2.0838766, 1.5184625, 0.8665069, 4.7361984, 2.6250663, 3.2317622, 0.7005957], [0.9606423, 3.0756218, 2.7999983, 0.977302, 2.814148, 0.96754587, 2.0536947, 1.4973781, 0.8542983, 4.6754394, 2.591638, 3.1874955, 0.6879289], [0.94663864, 3.0345795, 2.762766, 0.9628749, 2.778192, 0.9542945, 2.024015, 1.4766637, 0.84232455, 4.615588, 2.5586958, 3.143905, 0.67551315], [0.932903, 2.994181, 2.7261076, 0.9487228, 2.7427692, 0.9412911, 1.9948267, 1.4563098, 0.83058226, 4.5566273, 2.5262322, 3.1009767, 0.66334295], [0.91942805, 2.954415, 2.690014, 0.9348373, 2.7078693, 0.9285308, 1.9661204, 1.4363072, 0.8190655, 4.4985385, 2.494239, 3.0587018, 0.6514132], [0.9062069, 2.9152696, 2.6544719, 0.92121196, 2.6734843, 0.91600907, 1.9378862, 1.4166476, 0.8077682, 4.441304, 2.462708, 3.0170665, 0.63971883], [0.89323235, 2.876731, 2.6194713, 0.90784085, 2.6396043, 0.9037186, 1.9101151, 1.3973248, 0.7966851, 4.384911, 2.4316292, 2.9760578, 0.6282576], [0.8804995, 2.8387842, 2.5850008, 0.8947173, 2.6062198, 0.8916542, 1.8827988, 1.3783321, 0.78581154, 4.3293424, 2.4009936, 2.9356658, 0.61702406], [0.868003, 2.801417, 2.5510523, 0.88183755, 2.5733202, 0.87981224, 1.8559314, 1.3596632, 0.7751434, 4.2745857, 2.3707945, 2.8958814, 0.6060143], [0.85573953, 2.7646165, 2.5176198, 0.86919755, 2.540896, 0.868188, 1.8295056, 1.3413116, 0.7646761, 4.220625, 2.3410254, 2.856691, 0.59522396], [0.84370255, 2.7283728, 2.4846883, 0.85679084, 2.5089378, 0.85677433, 1.8035084, 1.3232687, 0.75440514, 4.167445, 2.3116794, 2.8180835, 0.58464664], [0.8318869, 2.6926763, 2.452247, 0.844611, 2.4774377, 0.8455653, 1.777931, 1.3055278, 0.74432695, 4.1150312, 2.2827473, 2.7800481, 0.57427555], [0.82028717, 2.657516, 2.4202867, 0.8326522, 2.446387, 0.8345566, 1.752765, 1.288082, 0.7344357, 4.0633698, 2.2542212, 2.7425754, 0.5641077], [0.8088995, 2.622884, 2.3887987, 0.82090974, 2.4157794, 0.82374537, 1.7280041, 1.2709264, 0.72472805, 4.0124483, 2.2260954, 2.7056587, 0.5541402], [0.79772055, 2.5887728, 2.3577743, 0.8093808, 2.3856063, 0.8131285, 1.7036421, 1.2540549, 0.7152004, 3.9622536, 2.1983635, 2.669289, 0.54436827], [0.7867441, 2.5551693, 2.327207, 0.79805833, 2.355859, 0.80269974, 1.6796697, 1.2374606, 0.70584947, 3.9127712, 2.1710181, 2.633454, 0.5347902], [0.77596515, 2.5220635, 2.2970877, 0.78693634, 2.3265305, 0.79245347, 1.6560789, 1.2211381, 0.6966697, 3.8639894, 2.144055, 2.5981457, 0.5254003], [0.7653814, 2.4894454, 2.2674117, 0.7760099, 2.2976165, 0.7823887, 1.6328626, 1.2050827, 0.6876594, 3.815898, 2.1174676, 2.5633507, 0.51619685], [0.75498784, 2.4573083, 2.2381716, 0.7652762, 2.2691069, 0.7725036, 1.6100128, 1.1892854, 0.67881465, 3.7684844, 2.0912514, 2.5290582, 0.5071729], [0.7447808, 2.425641, 2.2093558, 0.75473076, 2.2409945, 0.7627905, 1.5875213, 1.1737425, 0.6701291, 3.721734, 2.0653975, 2.495261, 0.49832347], [0.7347561, 2.394435, 2.180957, 0.7443689, 2.2132719, 0.75324374, 1.5653838, 1.1584485, 0.6615992, 3.675635, 2.0398943, 2.4619544, 0.48964706], [0.7249088, 2.363681, 2.152967, 0.73418725, 2.1859317, 0.74386317, 1.5435907, 1.1434015, 0.6532235, 3.630171, 2.0147347, 2.4291306, 0.48114604], [0.71523684, 2.3333707, 2.1253788, 0.7241842, 2.1589665, 0.73464864, 1.5221395, 1.1285954, 0.64500076, 3.5853338, 1.9899198, 2.3967838, 0.47281328], [0.7057337, 2.3034964, 2.0981853, 0.71435386, 2.1323702, 0.72559434, 1.5010245, 1.114023, 0.63692695, 3.5411167, 1.9654422, 2.3649037, 0.46464175], [0.6963958, 2.274056, 2.071379, 0.70469105, 2.1061368, 0.716695, 1.4802378, 1.09968, 0.6289981, 3.4975123, 1.9412957, 2.3334801, 0.45662954], [0.6872212, 2.2450407, 2.0449524, 0.6951931, 2.0802617, 0.70794475, 1.4597747, 1.0855628, 0.62120867, 3.454507, 1.9174746, 2.302506, 0.44877306], [0.67820525, 2.2164407, 2.0189006, 0.68585515, 2.0547373, 0.69934165, 1.4396273, 1.0716635, 0.613555, 3.4120882, 1.8939741, 2.2719746, 0.44106698], [0.6693421, 2.188249, 1.9932188, 0.6766729, 2.0295599, 0.6908824, 1.4197887, 1.0579754, 0.60603553, 3.370247, 1.8707907, 2.2418776, 0.43350768], [0.6606317, 2.1604564, 1.9679008, 0.6676446, 2.0047228, 0.68256277, 1.4002548, 1.0444982, 0.59864616, 3.3289733, 1.8479179, 2.2122111, 0.4260926], [0.652071, 2.1330564, 1.9429362, 0.65876615, 1.98022, 0.6743809, 1.381021, 1.0312315, 0.5913848, 3.2882586, 1.8253509, 2.182968, 0.4188225], [0.643655, 2.1060412, 1.9183201, 0.65003395, 1.9560443, 0.66633254, 1.3620801, 1.0181673, 0.58424884, 3.2480931, 1.8030839, 2.1541386, 0.411692], [0.6353805, 2.079403, 1.8940467, 0.6414447, 1.9321901, 0.6584134, 1.3434284, 1.0053011, 0.57723385, 3.2084675, 1.781113, 2.1257186, 0.4046968], [0.6272436, 2.053137, 1.870114, 0.6329973, 1.9086516, 0.65062106, 1.3250602, 0.99262875, 0.57033825, 3.169375, 1.7594353, 2.0977008, 0.39783713], [0.6192415, 2.0272346, 1.8465148, 0.6246877, 1.8854233, 0.64295405, 1.3069689, 0.98014927, 0.56356144, 3.1308079, 1.7380444, 2.0700803, 0.39111218], [0.61137027, 2.001688, 1.8232433, 0.6165109, 1.8625001, 0.6354119, 1.2891486, 0.9678613, 0.55690163, 3.092759, 1.7169367, 2.042849, 0.38451573], [0.60362834, 1.9764937, 1.8002933, 0.60846496, 1.8398776, 0.6279884, 1.2715955, 0.9557616, 0.55035484, 3.055217, 1.6961095, 2.0160005, 0.37804267], [0.5960149, 1.9516487, 1.7776583, 0.60054743, 1.8175528, 0.62068033, 1.2543055, 0.9438453, 0.5439172, 3.0181735, 1.6755571, 1.989526, 0.37169084], [0.5885293, 1.9271473, 1.7553304, 0.5927592, 1.7955217, 0.61348534, 1.2372756, 0.93210703, 0.5375868, 2.9816186, 1.655272, 1.9634184, 0.36545795], [0.5811667, 1.902982, 1.7333058, 0.5850948, 1.7737784, 0.6064017, 1.2204995, 0.9205414, 0.5313609, 2.9455433, 1.6352507, 1.937673, 0.35934132], [0.5739255, 1.8791453, 1.711583, 0.57755077, 1.7523179, 0.59942794, 1.2039729, 0.9091405, 0.525239, 2.9099414, 1.6154908, 1.9122846, 0.35334143], [0.56680304, 1.8556356, 1.6901629, 0.5701241, 1.7311352, 0.59256506, 1.1876913, 0.8978993, 0.5192195, 2.8748085, 1.5959892, 1.8872474, 0.3474564], [0.5597965, 1.8324444, 1.6690354, 0.56281227, 1.7102268, 0.5858072, 1.1716497, 0.886819, 0.51329845, 2.8401353, 1.576739, 1.8625546, 0.34168464], [0.55290145, 1.8095657, 1.6481943, 0.55561364, 1.6895877, 0.57914907, 1.1558442, 0.8758986, 0.5074743, 2.8059137, 1.5577372, 1.8382009, 0.33602512], [0.5461179, 1.7869958, 1.6276336, 0.5485297, 1.6692137, 0.5725886, 1.1402727, 0.8651352, 0.50174516, 2.7721388, 1.5389805, 1.8141829, 0.33047166], [0.5394427, 1.7647277, 1.6073493, 0.54155654, 1.6491007, 0.56612575, 1.12493, 0.85452527, 0.49610886, 2.738803, 1.5204645, 1.7904946, 0.3250202], [0.5328724, 1.7427547, 1.5873364, 0.5346945, 1.6292455, 0.5597589, 1.10981, 0.84406614, 0.49056247, 2.7058961, 1.502184, 1.7671293, 0.31966832], [0.52640396, 1.7210727, 1.5675913, 0.5279381, 1.6096437, 0.55348605, 1.0949093, 0.8337542, 0.48510402, 2.6734114, 1.4841381, 1.7440842, 0.31441522], [0.52003556, 1.6996769, 1.5481089, 0.5212829, 1.5902936, 0.5473049, 1.080225, 0.823587, 0.47973242, 2.641341, 1.4663237, 1.7213539, 0.3092582], [0.51376665, 1.6785645, 1.5288863, 0.5147271, 1.5711911, 0.54121655, 1.0657527, 0.81356335, 0.47444898, 2.6096816, 1.4487354, 1.698933, 0.30419552], [0.50759935, 1.6577327, 1.5099192, 0.50827235, 1.5523317, 0.5352179, 1.051489, 0.8036821, 0.46924958, 2.5784273, 1.4313679, 1.6768153, 0.29922512], [0.50153166, 1.6371738, 1.4912071, 0.5019157, 1.5337129, 0.52930653, 1.0374328, 0.79394054, 0.46413267, 2.547569, 1.4142199, 1.6549977, 0.29434583], [0.4955608, 1.616883, 1.4727464, 0.49565506, 1.5153341, 0.5234784, 1.0235814, 0.7843335, 0.45909813, 2.5171013, 1.3972894, 1.6334777, 0.28955626], [0.48968428, 1.5968574, 1.4545339, 0.4894877, 1.497189, 0.5177334, 1.0099314, 0.7748592, 0.45414335, 2.4870207, 1.3805732, 1.6122484, 0.2848547], [0.48389933, 1.5770893, 1.4365624, 0.4834115, 1.4792758, 0.5120699, 0.9964776, 0.76551783, 0.44926476, 2.4573243, 1.3640664, 1.591304, 0.28023875], [0.47820172, 1.557577, 1.4188275, 0.47742215, 1.461589, 0.50648624, 0.98321694, 0.7563077, 0.44446006, 2.4280055, 1.3477666, 1.5706404, 0.2757117], [0.47258776, 1.5383173, 1.4013277, 0.4715178, 1.4441249, 0.5009804, 0.97014594, 0.74722534, 0.4397276, 2.3990576, 1.3316709, 1.5502542, 0.2712681], [0.46705878, 1.5193062, 1.3840585, 0.46569756, 1.4268821, 0.49555138, 0.9572599, 0.7382668, 0.43506658, 2.3704753, 1.3157774, 1.5301421, 0.26690835], [0.4616112, 1.500541, 1.3670166, 0.45995823, 1.4098557, 0.49019957, 0.9445567, 0.7294293, 0.4304799, 2.3422549, 1.3000883, 1.5102986, 0.26262987], [0.4562442, 1.482019, 1.3502011, 0.45429966, 1.3930418, 0.48492375, 0.93203413, 0.72071314, 0.4259629, 2.3143897, 1.2845961, 1.4907185, 0.25842938], [0.45095578, 1.4637346, 1.3336043, 0.44872266, 1.3764379, 0.47972053, 0.91968757, 0.7121169, 0.42151284, 2.2868724, 1.2692971, 1.4713982, 0.2543069], [0.44574627, 1.4456838, 1.3172218, 0.4432242, 1.360038, 0.4745904, 0.9075143, 0.7036357, 0.41712934, 2.2596977, 1.254189, 1.4523337, 0.25026274]], 'val': [[8.119839, 6.4994555, 7.7370186, 4.9806237, 6.6764727, 3.4713752, 6.250762, 9.236686, 6.4858503, 9.213117, 8.219168, 5.1341176, 8.940729], [6.2416987, 6.7758536, 4.3092384, 4.7247176, 7.977654, 8.881838, 8.022008, 6.925857, 4.644946, 7.7387156, 8.905577, 7.608957, 8.872054], [9.38496, 7.8539925, 4.2984047, 7.064474, 6.1467776, 5.452308, 9.118808, 7.300804, 8.671402, 6.1330233, 3.3879247, 3.3705852, 6.9685717], [3.323182, 6.269959, 3.479879, 8.703268, 7.5794334, 3.372943, 5.5080504, 5.355778, 7.425199, 6.096744, 8.256565, 9.106838, 5.787902], [6.041958, 6.2036405, 6.2867866, 7.6012383, 4.0133996, 6.1381106, 4.5087605, 5.5709643, 6.8589916, 10.551996, 8.474627, 7.7777176, 8.197991], [7.3260193, 5.9156885, 6.6363773, 10.37748, 5.688651, 5.973253, 7.2139997, 6.570174, 3.9609213, 4.454914, 4.684477, 4.250682, 4.2073393], [4.362866, 5.956788, 6.0712137, 7.326288, 5.9517474, 5.1740375, 6.0361857, 4.550296, 5.931761, 5.841447, 8.74723, 4.3968277, 5.419058], [6.6875796, 6.7010374, 6.6649895, 4.5137415, 9.192666, 3.0412774, 8.096149, 4.410282, 6.6213613, 5.5976, 6.5782466, 7.9940434, 7.0838494], [3.649735, 9.682831, 6.9543524, 6.8391414, 7.086441, 11.012235, 8.143796, 4.492911, 6.9848137, 3.6997857, 3.067079, 5.6214995, 4.2191358], [6.8523746, 5.8512154, 7.182226, 4.1831317, 5.8997054, 2.931995, 3.718215, 7.854601, 5.8053885, 9.800747, 5.813057, 6.8384504, 5.4499083], [6.773993, 6.592623, 4.3364244, 11.074308, 6.0648875, 4.5782375, 7.5854707, 6.221571, 5.4020376, 3.6579647, 2.8225117, 6.5184045, 4.6002913], [4.28826, 5.515314, 6.9511843, 5.5263567, 5.4239435, 8.026075, 4.135061, 2.8152294, 5.011051, 5.0689945, 7.820992, 4.8389225, 5.288796], [6.5522947, 4.2199054, 5.5929585, 5.3870783, 7.153913, 6.318914, 5.1365714, 4.123452, 7.880489, 3.5223079, 6.10202, 6.397574, 9.528346], [7.912498, 3.882539, 6.612521, 6.5184546, 4.103771, 7.3508997, 6.6661005, 3.883677, 5.156327, 4.584896, 6.5206275, 6.5110726, 5.4111366], [6.030034, 7.879784, 5.0761895, 3.8866045, 2.669836, 2.7872858, 6.5447726, 5.849013, 6.622574, 2.79109, 10.867943, 4.996767, 5.553514], [10.230921, 6.496788, 3.2972827, 4.606064, 5.3689065, 2.696024, 4.5747647, 5.999721, 2.7654037, 7.2752676, 8.410018, 5.9108443, 2.7741017], [10.167195, 5.23883, 6.734459, 4.888363, 4.9994106, 5.1412373, 7.4675174, 6.149746, 5.105021, 5.764493, 5.7414947, 5.814148, 5.622064], [3.9929209, 6.1699805, 2.6900048, 5.1933937, 7.3326974, 7.3303823, 7.563312, 3.8322558, 4.992318, 3.0578413, 4.9412084, 5.853045, 6.094187], [3.6345458, 7.277839, 5.7046547, 4.8227215, 4.88309, 2.670917, 7.170952, 8.582713, 4.7626033, 6.882371, 5.0777655, 7.29911, 4.740118], [4.8475375, 4.7067785, 7.2892547, 3.756422, 7.3727155, 5.3136854, 4.2795, 4.466124, 4.321417, 6.5909386, 7.058676, 4.9263296, 4.0087276], [5.8883843, 4.599235, 4.8586164, 3.613031, 4.9433384, 4.9897738, 2.5335598, 3.6599321, 5.299724, 6.9709597, 3.7902396, 4.9623737, 4.6952534], [5.298141, 2.579736, 3.6642473, 7.1424456, 7.245219, 5.1972094, 3.019991, 6.2021637, 3.5381687, 5.4178853, 2.397914, 5.73787, 5.301505], [2.4601362, 5.81188, 4.7771077, 2.9630866, 5.2068825, 5.7084327, 5.160745, 3.5055456, 4.714322, 6.575879, 2.4950125, 6.4997797, 2.523746], [7.8604655, 4.8846273, 2.4345899, 4.548128, 6.728546, 2.5043702, 7.880673, 6.527494, 4.3552265, 2.4277554, 4.3227496, 4.327864, 4.0156364], [6.215248, 5.5082455, 6.608685, 6.5734096, 5.0772347, 3.6963573, 3.3560767, 6.935295, 3.2141979, 7.240966, 7.433609, 4.9715595, 5.6558104], [6.258034, 3.30713, 3.4700513, 3.9010348, 4.703101, 4.0187917, 6.4055037, 4.479992, 4.2985415, 5.499282, 2.8971193, 4.244819, 3.4733288], [5.5721965, 3.4840236, 7.4663234, 5.91378, 3.3733945, 6.3582134, 4.4434123, 4.511248, 5.4627414, 2.4462256, 6.27583, 5.5528336, 5.6931353], [7.485315, 4.3400993, 6.0106916, 6.1877756, 4.40325, 2.3254254, 2.7968214, 3.4284987, 2.135283, 2.8148522, 2.6813657, 3.1649158, 5.8790936], [6.628664, 6.473187, 6.4056835, 4.784267, 2.2073975, 5.1567907, 3.8190415, 6.4128056, 4.29411, 3.0397944, 4.2357545, 4.090508, 4.809333], [2.7035542, 4.2042627, 5.2366247, 6.3330827, 4.307979, 5.2314143, 2.772041, 3.2028856, 3.1258073, 3.0926542, 5.8746934, 6.1593914, 6.2217135], [3.3476405, 7.2678113, 2.9569921, 6.7890806, 5.6297054, 5.287313, 4.182618, 8.241093, 2.723484, 4.308379, 4.92523, 8.98729, 2.1852562], [3.619964, 1.9005227, 3.57795, 4.9132724, 3.7405996, 3.0173423, 4.949913, 6.1574426, 3.7464995, 4.0468698, 2.1155283, 6.6465645, 2.081764], [8.211598, 2.9196448, 6.5512323, 3.1795692, 3.0008729, 6.09697, 5.183025, 3.417541, 4.4630065, 5.2613325, 2.1427705, 2.9493296, 4.1664195], [4.1678534, 4.055002, 4.180874, 2.0898273, 3.8555489, 5.9551063, 6.444994, 6.872993, 3.970882, 3.6055322, 5.6668363, 7.0650277, 4.532648], [3.4684894, 2.9227471, 2.1014996, 2.0906897, 3.8836498, 2.9580674, 3.880932, 3.0720983, 3.887815, 3.4480524, 4.914378, 4.7087383, 3.9093685], [3.824954, 3.3543355, 5.9068685, 3.897081, 4.766068, 3.0206788, 4.0753756, 3.914877, 2.71393, 3.707001, 6.133161, 3.9496322, 2.8577745], [2.0535443, 5.036501, 3.9808297, 2.4062407, 6.6163177, 3.4766362, 5.8262377, 3.529927, 3.0425134, 2.826753, 2.7646158, 2.9631112, 5.9076996], [3.2936583, 2.8317394, 1.9863075, 2.8152657, 3.2434013, 4.7160406, 4.175349, 1.9100051, 2.7106512, 5.502504, 5.1500564, 6.553876, 6.659602], [4.267888, 3.8815885, 7.221616, 4.66714, 4.6153803, 4.702695, 3.4023678, 2.6737475, 3.6086452, 2.8340912, 2.812583, 5.61909, 5.7956505], [5.0125675, 3.6428242, 2.2949305, 4.5122113, 5.05127, 3.4952664, 1.7858021, 2.668006, 3.6244953, 2.8190203, 4.3997936, 5.428019, 4.748271], [2.824894, 3.7399766, 4.434328, 4.248302, 3.5980508, 2.67156, 7.7888117, 3.7581334, 1.898155, 4.5234613, 2.7072692, 7.1436205, 4.4847894], [3.2037082, 3.8370845, 3.4952006, 1.8264855, 3.6366456, 5.0738254, 5.376877, 4.342446, 2.6477418, 5.308022, 3.5329168, 5.0480576, 5.2070236], [4.0027847, 4.4613314, 3.6607509, 3.3673074, 2.6642091, 3.9100244, 2.8284068, 3.0334249, 2.1802893, 2.6005833, 3.3024936, 6.5288687, 4.489656], [4.3757524, 4.805817, 5.2092514, 3.6231527, 5.838603, 3.0982985, 2.204228, 1.7670723, 3.2960517, 5.2274427, 4.173381, 3.7307022, 2.2144003], [2.6272867, 2.4518144, 1.6852236, 4.1145988, 2.5036626, 2.4377446, 2.4258707, 4.9577727, 5.7941823, 5.6005683, 3.124955, 1.6539336, 3.0419405], [4.1585407, 4.8920293, 3.3596537, 1.7626355, 1.6872783, 3.973793, 3.3448017, 3.473588, 3.4792507, 5.467283, 3.4132671, 1.6450276, 3.443108], [2.4218786, 3.4015038, 4.087077, 1.6566064, 1.9540795, 4.8386397, 4.2205815, 3.7018456, 4.991972, 4.1161284, 2.6033926, 1.6560738, 4.0953035], [4.863361, 1.6762893, 3.3082285, 2.090065, 2.5798173, 2.9800086, 6.781434, 6.000047, 2.5359573, 2.3605602, 4.0680733, 3.2107542, 2.2750993], [3.3608763, 3.7897344, 2.4779248, 3.3836584, 2.4350827, 1.5852417, 3.339796, 4.2031374, 2.4316554, 3.239476, 3.650955, 3.2029433, 4.0397987], [2.453562, 2.7835553, 2.8167806, 2.0603151, 5.0556164, 3.2558868, 2.6979172, 3.3621461, 1.5833857, 4.90409, 4.7501197, 2.277845, 3.2051768], [3.45073, 4.6711664, 3.9753344, 2.809826, 4.2017655, 2.4447896, 1.9894801, 3.170608, 4.5323486, 3.3137221, 2.3807833, 5.8882213, 1.9159123], [5.048579, 2.89178, 3.8516586, 3.5222654, 2.375467, 4.757863, 4.101087, 4.6554785, 4.0996327, 5.24925, 4.0112877, 2.1846066, 2.6792798], [4.498232, 3.8773522, 3.2246451, 2.6929276, 2.6384783, 3.4725547, 2.4114108, 1.9119005, 4.7407303, 3.079848, 4.759697, 2.2979937, 3.10139], [1.9098713, 6.3210387, 1.9108238, 3.658124, 5.105767, 3.7457142, 4.4930897, 2.957511, 2.372565, 2.4378302, 1.4163015, 6.259585, 5.275598], [2.797668, 3.4602396, 3.052708, 3.9541347, 3.1356509, 5.3326116, 2.1542423, 3.8455968, 3.784421, 5.1438346, 3.8704772, 2.9861472, 5.1271553], [2.3630638, 3.6695368, 1.4155273, 4.273447, 4.578461, 3.6422422, 1.439596, 2.5362668, 2.8670983, 1.5962412, 2.657174, 3.616861, 4.4399157], [3.350139, 3.7782183, 2.956901, 2.7356339, 3.0009997, 1.405976, 3.7411819, 3.1306002, 1.4466846, 2.9136057, 2.8573964, 5.789577, 3.0700908], [2.1197162, 2.155733, 3.6497219, 2.2661655, 3.9861038, 2.9798794, 5.2094364, 3.6085758, 2.184394, 2.8572571, 2.7467563, 2.6239903, 2.945806], [1.3110954, 4.577046, 3.5052714, 1.3556812, 2.906805, 3.6925023, 2.894828, 2.7248564, 5.6545067, 2.776343, 1.4139252, 4.522539, 3.430616], [3.9097269, 2.5815594, 3.501486, 2.8511834, 2.071735, 1.4302197, 4.3521934, 3.5194864, 4.367197, 2.4793994, 2.2185912, 3.49866, 3.3957834], [3.1390514, 4.561515, 3.6499348, 4.8701344, 3.5485244, 4.8596272, 2.0346248, 3.817764, 1.9592001, 2.8545551, 4.5875287, 1.3135477, 2.6573882], [4.823054, 2.8114302, 5.8747144, 2.01017, 3.2993288, 2.9691727, 3.3516672, 2.6185148, 2.7094512, 1.7186126, 4.038575, 2.7339113, 2.9064662], [3.0272985, 2.6530833, 4.1701903, 2.2427633, 3.4758458, 1.9890054, 2.429048, 3.1902125, 1.2258213, 3.4405472, 2.5024667, 3.4182298, 1.6741688], [4.296204, 3.2524254, 2.406508, 3.2433386, 3.803592, 2.5411015, 3.3141797, 2.5499613, 1.9657199, 1.85946, 1.9948454, 3.3854105, 2.5593507], [2.5751717, 1.8667696, 2.5655594, 3.3675761, 4.081793, 1.2311786, 2.611972, 1.9319392, 2.5786164, 2.304294, 2.6038249, 3.1725407, 3.3321712], [2.214854, 2.5418482, 4.3072305, 1.1813167, 3.431757, 1.8044456, 3.2745757, 1.9621938, 1.6346253, 1.8642997, 2.5600176, 2.3223355, 1.8697903], [3.9057908, 1.910435, 1.2299714, 3.2378147, 1.7814257, 2.2515013, 4.4393067, 3.8379521, 3.4157162, 4.5835567, 3.6925406, 2.3131201, 2.832379], [2.5206017, 3.7523346, 1.5310882, 4.491617, 4.414247, 2.108542, 2.955158, 3.4548144, 2.1220312, 2.9098217, 1.7155373, 2.671396, 3.0846713], [4.1815314, 3.134931, 2.2398765, 3.2067766, 3.0172505, 2.8797934, 3.0892181, 2.6774971, 1.7472098, 2.504754, 2.4269218, 1.5102539, 3.7122726], [1.7535563, 2.5552492, 4.814304, 3.4302382, 2.4204993, 1.7284244, 4.648043, 2.863388, 3.2378945, 1.7071728, 3.2275252, 1.904506, 3.1127014], [2.361642, 1.7783241, 2.9873722, 1.7906048, 3.0356429, 2.0642858, 1.7145447, 2.5837317, 3.0356345, 2.4058816, 5.09138, 2.6230173, 1.4267666], [3.802431, 2.1278868, 2.2170858, 3.9909532, 2.31996, 3.031104, 1.0998031, 5.572918, 1.6746912, 2.4836388, 2.990594, 3.0934486, 1.0698906], [2.6549664, 1.1001471, 4.100344, 2.9341366, 1.4640653, 1.9894981, 4.1511965, 1.2021635, 2.9274218, 2.3485956, 1.1871035, 3.6256354, 2.81398], [3.0092633, 2.3912358, 3.0779076, 2.684291, 2.8191774, 2.7587013, 2.6764607, 2.9266973, 2.345122, 2.8063235, 4.0629625, 1.911133, 2.3481212], [1.1887796, 2.4229903, 3.5829966, 2.4384255, 2.3495529, 2.9265437, 1.1106049, 1.5995836, 2.4059398, 4.038942, 1.6383256, 2.2875955, 3.6678836], [1.833588, 2.8368256, 3.368125, 2.158268, 2.9354281, 2.788126, 3.5257332, 1.71118, 3.549203, 2.473007, 2.612344, 4.0569944, 1.637711], [1.7601713, 2.754009, 1.5984691, 1.0459644, 2.858636, 2.254262, 2.6430538, 2.5696025, 2.2477458, 2.3146002, 1.1357723, 2.8788586, 1.8090955], [4.845126, 4.601461, 1.6063797, 1.3809661, 2.1243415, 2.275611, 1.1551037, 3.0322914, 2.840712, 1.1167881, 3.9082274, 1.6949365, 2.7645516], [2.4534483, 2.6315339, 2.8706884, 2.8768702, 3.1744132, 1.6143835, 2.6074104, 2.840936, 1.3223214, 2.1638227, 2.0605707, 1.9954653, 2.841956], [3.8065193, 1.2628359, 2.9215446, 3.9280899, 1.3524033, 2.208263, 2.0522163, 2.6893926, 1.6210953, 1.3268751, 1.8540031, 1.8089308, 2.1604648], [1.3450905, 1.8042728, 2.2103903, 2.63627, 1.47878, 3.42198, 2.574697, 1.0159203, 2.05423, 2.78098, 1.6448568, 2.346024, 3.1384804], [1.2337177, 1.7167703, 0.9964603, 2.2772048, 1.5462452, 2.1671112, 3.5114422, 1.4545542, 1.4685522, 2.1943438, 2.063034, 2.0776825, 3.4271991], [1.975374, 1.4830278, 2.7989876, 2.032973, 2.7686536, 2.3758352, 2.3339276, 1.5669692, 0.9870068, 3.5175824, 1.5777019, 1.5440984, 1.5280123], [2.0878506, 2.783702, 1.2729824, 2.0643744, 0.894442, 3.6801453, 2.7472656, 1.2854928, 2.1122286, 1.9243239, 1.4884398, 1.0526711, 3.1944728], [1.4298694, 2.657575, 4.1865206, 1.632033, 1.5397714, 1.4577391, 1.4656086, 0.979368, 1.1446322, 1.9815742, 1.0968686, 1.6767783, 2.4143188], [0.8898601, 1.4404191, 3.5810814, 1.4919972, 2.3548412, 0.98155075, 3.000263, 0.9705274, 2.0337598, 2.4942188, 1.7402842, 0.975688, 1.3868539], [3.0233815, 1.9660472, 1.7030077, 3.1377048, 0.9319519, 2.5040216, 2.6915984, 2.0799444, 1.7745764, 1.5786631, 2.9614184, 1.9979826, 2.3752682], [2.30927, 1.1491437, 2.3820038, 2.8508596, 2.9698138, 1.4556359, 1.9410124, 1.3622491, 0.92899156, 3.103486, 2.453052, 1.3995236, 1.8728582], [3.0034766, 1.5779856, 2.4351177, 1.6724502, 1.821546, 1.5986592, 2.8605483, 1.9974024, 1.3136262, 2.0656343, 2.9324424, 3.377638, 2.5665736], [2.9902103, 0.97145283, 2.6257439, 2.0110257, 1.3146349, 1.1041152, 1.2970747, 1.990162, 0.94487226, 1.8356736, 1.2212403, 1.3545281, 1.932377], [2.2846003, 2.3819187, 2.4694428, 2.385377, 1.85823, 2.3012516, 2.384387, 3.9069462, 2.4278998, 1.8824962, 2.8806436, 1.4904357, 1.689392], [1.7530887, 1.5059578, 1.8515831, 3.3000655, 1.5742584, 2.6242392, 1.8374492, 2.5893366, 1.3204496, 2.5254958, 2.3024669, 3.0713549, 1.424787], [2.3844109, 2.0489268, 1.8924598, 3.830666, 3.2726471, 2.2939565, 2.3534887, 0.87327594, 2.0525422, 1.4382957, 2.3502383, 2.8233907, 0.8732159], [2.0615168, 1.0611645, 1.0788316, 2.232822, 1.9606434, 1.1294312, 1.2875245, 1.3339097, 1.2237142, 2.2041337, 1.8643284, 1.853383, 1.3281571], [2.2020872, 1.0316304, 2.2905185, 1.3052657, 2.584798, 1.7921679, 2.6028943, 2.7937388, 1.659909, 3.255298, 1.2064242, 0.9261302, 2.2964277], [0.91883826, 2.2258458, 2.3327792, 2.1669931, 3.4457083, 1.3388954, 2.9825792, 1.8291166, 1.2479181, 1.7792481, 2.3016708, 1.7029995, 1.3904043], [2.2234728, 1.7701259, 1.7368761, 1.7696166, 1.3519516, 2.153086, 0.79241055, 1.9365178, 1.8922406, 1.7350892, 2.2488005, 1.4842212, 1.2301052], [1.2128832, 2.9492166, 1.9490004, 1.2162639, 1.8365653, 1.295065, 2.083723, 1.812826, 1.220536, 0.9551269, 1.3240256, 1.7502569, 0.7426545], [2.1003585, 2.5867035, 1.5394783, 3.1523275, 2.1617417, 2.1418824, 1.5944788, 1.7513467, 2.1726415, 2.0606866, 2.2104542, 1.6225541, 1.2076449], [1.5733646, 1.7200519, 1.1788327, 1.8477234, 1.834882, 1.2406375, 1.2349683, 1.2743708, 1.3058004, 1.6176226, 0.7519336, 1.6940454, 0.9653162]], 'val_mse': [[6.3431063, 4.725146, 5.965098, 3.2110689, 4.909275, 1.7065291, 4.4882665, 7.4765387, 4.728043, 7.457646, 6.4660287, 3.3833072, 7.1922374], [4.495523, 5.031992, 2.5676904, 2.9854808, 6.240724, 7.147211, 6.2896833, 5.1958323, 2.917217, 6.013281, 7.1824327, 5.888102, 7.153481], [7.6686635, 6.139971, 2.5866566, 5.3549967, 4.4395676, 3.7473607, 7.416123, 5.600378, 6.9732313, 4.437107, 1.6942595, 1.6791692, 5.2793975], [1.6362462, 4.5852604, 1.7974149, 7.0230355, 5.901428, 1.6971607, 3.834489, 3.6844344, 5.756069, 4.4298267, 6.5918574, 7.4443383, 4.1276026], [4.383856, 4.5477347, 4.6330767, 5.9497213, 2.3640711, 4.4909663, 2.8637986, 3.9281816, 5.2183824, 8.913559, 6.8383594, 6.1436195, 6.5660563], [5.6962414, 4.288067, 5.0109105, 8.754166, 4.0674853, 4.354231, 5.5971203, 4.9554343, 2.3483164, 2.8444426, 3.076136, 2.644471, 2.6032505], [2.7608967, 4.356938, 4.473482, 5.7306714, 4.358241, 3.5826375, 4.4468913, 2.963101, 4.346661, 4.2584424, 7.166318, 2.818008, 3.8423233], [5.1129255, 5.128463, 5.0944934, 2.9453223, 7.626317, 1.4769962, 6.533934, 2.850131, 5.0632696, 4.041566, 5.024267, 6.442117, 5.5339694], [2.1018987, 8.137037, 5.4106007, 5.2974286, 5.5467615, 9.474583, 6.6081705, 2.9593089, 5.453231, 2.170222, 1.5395325, 4.09597, 2.6956167], [5.3308616, 4.331708, 5.664724, 2.667631, 4.3862023, 1.4204859, 2.2086985, 6.3470736, 4.2998457, 8.297188, 4.3114786, 5.338852, 3.9522843], [5.278341, 5.0989423, 2.8447127, 9.584563, 4.577105, 3.092414, 6.1016045, 4.7396584, 3.9220762, 2.1799533, 1.3464478, 5.044287, 3.128114], [2.8180187, 4.047009, 5.4848156, 4.06192, 3.9614353, 6.5654907, 2.6764, 1.3584895, 3.5562267, 3.6160834, 6.3699927, 3.389835, 3.8416135], [5.1070156, 2.776528, 4.1514826, 3.9475021, 5.7162313, 4.8831234, 3.702671, 2.6914396, 6.45036, 2.0940611, 4.6756525, 4.9730854, 8.105731], [6.4917536, 2.4636633, 5.195514, 5.103314, 2.6904926, 5.9394803, 5.2565384, 2.475969, 3.750469, 3.1808875, 5.1184645, 5.110756, 4.01266], [4.6333947, 6.4849815, 3.6832232, 2.4954722, 1.2805331, 1.3998085, 5.159119, 4.46518, 5.240558, 1.4108896, 9.489555, 3.6201942, 4.178749], [8.857961, 5.1256313, 1.927928, 3.23851, 4.003148, 1.3320575, 3.2125888, 4.6393332, 1.4067999, 5.918449, 7.0549817, 4.557589, 1.4226223], [8.817489, 3.8908966, 5.388298, 3.543973, 3.6567864, 3.800374, 6.128414, 4.8123994, 3.7694263, 4.43065, 4.409401, 4.483804, 4.2934628], [2.6660597, 4.84486, 1.3666254, 3.8717535, 6.0127926, 6.0122085, 6.2468667, 2.5175383, 3.6793244, 1.7465693, 3.6316562, 4.5452127, 4.7880683], [2.330139, 5.9751444, 4.403671, 3.523446, 3.5855196, 1.3750479, 5.8767834, 7.2902417, 3.4718246, 5.593285, 3.790371, 6.0134053, 3.456097], [3.5651975, 3.4261181, 6.010275, 2.47912, 6.0970864, 4.0397263, 3.0072098, 3.1954994, 3.0524538, 5.323638, 5.7930346, 3.6623495, 2.7464023], [4.6277103, 3.3402114, 3.6012425, 2.3573053, 3.6892576, 3.7373343, 1.2827593, 2.410769, 4.052194, 5.725062, 2.5459719, 3.719736, 3.4542391], [4.058748, 1.3419642, 2.4280975, 5.9079156, 6.012304, 3.9659054, 1.7902992, 4.9740815, 2.311692, 4.193013, 1.1746441, 4.516202, 4.0814323], [1.2416564, 4.594993, 3.5618136, 1.7493829, 3.9947655, 4.4979, 3.9517956, 2.2981777, 3.508532, 5.3716664, 1.2923746, 5.298715, 1.3242496], [6.6625347, 3.6882617, 1.2397898, 3.3548908, 5.536869, 1.31425, 6.692109, 5.3404837, 3.1697664, 1.2438443, 3.140386, 3.1470475, 2.8363602], [5.037509, 4.332044, 5.4340224, 5.4002833, 3.9056406, 2.5262914, 2.1875386, 5.7682834, 2.0487082, 6.0769987, 6.2711625, 3.8106332, 4.4963994], [5.1001363, 2.1507432, 2.3151758, 2.7476687, 3.5512412, 2.8684356, 5.25665, 3.3326373, 3.1526828, 4.354919, 1.7542486, 3.103442, 2.3334398], [4.433792, 2.3471055, 6.330891, 4.7798305, 2.2409244, 5.227219, 3.3138945, 3.3832052, 4.3361692, 1.321124, 5.152197, 4.4306693, 4.572434], [6.3660746, 3.2223198, 4.8943725, 5.072915, 3.2898455, 1.213473, 1.6863202, 2.3194473, 1.0276765, 1.7086915, 1.576649, 2.0616422, 4.7772593], [5.5282674, 5.374227, 5.308161, 3.6881785, 1.1127406, 4.0635624, 2.727242, 5.322432, 3.2051585, 1.9522654, 3.1496458, 3.0058205, 3.7260602], [1.6216916, 3.1238117, 4.157586, 5.2554545, 3.231758, 4.1565976, 1.6986288, 2.1308753, 2.055195, 2.0234387, 4.8068724, 5.092965, 5.1566763], [2.283991, 6.2055492, 1.8961166, 5.729592, 4.5716, 4.2305875, 3.127272, 7.187124, 1.6708885, 3.257158, 3.8753822, 7.9388137, 1.1381453], [2.5742168, 0.85613877, 2.5349295, 3.871614, 2.7002997, 1.9783993, 3.9123259, 5.1212077, 2.711614, 3.0133333, 1.0833391, 5.6157227, 1.0522656], [7.183439, 1.892825, 5.525752, 2.1554272, 1.9780654, 5.075494, 4.16288, 2.3987262, 3.4455187, 4.2451706, 1.1279335, 1.9358166, 3.154224], [3.156974, 3.0454395, 3.1726267, 1.082894, 2.8499262, 4.950792, 5.441987, 5.871292, 2.9704835, 2.6064365, 4.6690407, 6.0685315, 3.5374463], [2.47458, 1.9301299, 1.110175, 1.1006571, 2.894905, 1.9706082, 2.8947582, 2.0872078, 2.9042041, 2.4657204, 3.9333222, 3.7289596, 2.9308617], [2.847716, 2.3783674, 4.9321694, 2.9236493, 3.793902, 2.049776, 3.105735, 2.9464982, 1.7468098, 2.7411385, 5.1685543, 2.98628, 1.8956724], [1.0926907, 4.076897, 3.0224736, 1.4491313, 5.6604514, 2.5220106, 4.872853, 2.5777829, 2.0916054, 1.8770806, 1.8161772, 2.0159051, 4.9617224], [2.3489072, 1.8882141, 1.0440093, 1.8741922, 2.3035495, 3.7774086, 3.2379375, 0.9738106, 1.7756722, 4.56874, 4.2175055, 5.622538, 5.7294717], [3.338963, 2.9538698, 6.2951035, 3.7418315, 3.6912744, 3.7797892, 2.4806604, 1.7532378, 2.689329, 1.9159677, 1.8956516, 4.7033505, 4.881097], [4.0991974, 2.7306373, 1.3839276, 3.602391, 4.14263, 2.587804, 0.8795184, 1.7628989, 2.7205617, 1.916259, 3.4982018, 4.527596, 3.8490136], [1.9267998, 2.8430457, 3.5385602, 3.3536966, 2.7046037, 1.7792699, 6.8976784, 2.8681548, 1.0093296, 3.6357892, 1.8207473, 6.2582483, 3.600561], [2.3206227, 2.955143, 2.6144023, 0.94682795, 2.7581275, 4.1964445, 4.500633, 3.467338, 1.7737659, 4.4351783, 2.661205, 4.1774774, 4.33757], [3.134456, 3.594126, 2.7946692, 2.5023491, 1.8003709, 3.0473037, 1.9668043, 2.17294, 1.320918, 1.7423264, 2.445348, 5.6728344, 3.6347296], [3.5219328, 3.953102, 4.357641, 2.7726462, 4.9891977, 2.2499933, 1.3570216, 0.92096376, 2.4510388, 4.383526, 3.330558, 2.8889728, 1.3737594], [1.7877312, 1.6133461, 0.84784204, 3.2783024, 1.6684492, 1.6036114, 1.5928166, 4.1257977, 4.9632816, 4.7707424, 2.2962036, 0.8262569, 2.2153327], [3.3330014, 4.0675583, 2.5362496, 0.94029844, 0.8660065, 3.1535838, 2.525655, 2.6555018, 2.6622217, 4.651311, 2.598352, 0.831167, 2.6302977], [1.6101173, 2.5907917, 3.2774146, 0.84799165, 1.1465094, 4.0321136, 3.415099, 2.897404, 4.18857, 3.3137653, 1.8020661, 0.8557839, 3.296046], [4.0651336, 0.8790926, 2.512063, 1.2949297, 1.7857109, 2.1869278, 5.9893785, 5.2090163, 1.7459478, 1.5715712, 3.2801027, 2.4238021, 1.4891615], [2.5759516, 3.0058234, 1.6950257, 2.6017718, 1.6542063, 0.8053736, 2.5609362, 3.4252844, 1.6548051, 2.4636297, 2.8761103, 2.4290996, 3.266952], [1.6817101, 2.0126991, 2.0469217, 1.2914509, 4.2877445, 2.4890068, 1.9320289, 2.597247, 0.81947297, 4.1411643, 3.988178, 1.5168893, 2.4452028], [2.6917353, 3.9131515, 3.2182987, 2.053769, 3.4466858, 1.6906847, 1.2363491, 2.4184504, 3.7811615, 2.5635061, 1.6315367, 5.139943, 1.1685977], [4.3022285, 2.1463926, 3.1072338, 2.778803, 1.6329654, 4.0163193, 3.3605022, 3.9158514, 3.360959, 4.5115304, 3.2745223, 1.4487942, 1.944416], [3.764315, 3.1443818, 2.4926221, 1.9618518, 1.908346, 2.743364, 1.6831621, 1.1845939, 4.014363, 2.3544195, 4.0352044, 1.5744377, 2.378767], [1.1881794, 5.600277, 1.1909926, 2.9392223, 4.3877935, 3.0286674, 3.7769692, 2.242316, 1.6582924, 1.7244803, 0.70387244, 5.548077, 4.5650063], [2.0879915, 2.7514796, 2.3448632, 3.2472057, 2.4296346, 4.627506, 1.4500468, 3.1423116, 3.082042, 4.4423623, 3.1699095, 2.2864842, 4.428393], [1.665201, 2.9725742, 0.71946365, 3.578283, 3.8841941, 2.9488714, 0.74712133, 1.8446867, 2.176411, 0.9064459, 1.9682686, 2.9288456, 3.7527854], [2.6638927, 3.0928576, 2.2724254, 2.0520415, 2.318289, 0.72414577, 3.060231, 2.450529, 0.7674898, 2.2352867, 2.1799524, 5.113009, 2.3943925], [1.444886, 1.4817717, 2.9766297, 1.5939428, 3.314748, 2.3093884, 4.53981, 2.9398139, 1.5164932, 2.1902177, 2.080576, 1.9586678, 2.2813387], [0.6474814, 3.9142857, 2.8433645, 0.6946277, 2.2466035, 3.0331516, 2.236328, 2.0672052, 4.9977036, 2.1203876, 0.7588153, 3.8682742, 2.7771904], [3.25714, 1.9298127, 2.8505793, 2.2011144, 1.4225018, 0.78182083, 3.704629, 2.8727565, 3.7212994, 1.8343327, 1.5743542, 2.8552532, 2.7532034], [2.4972963, 3.920584, 3.0098286, 4.2308526, 2.9100637, 4.221986, 1.397804, 3.1817627, 1.3240156, 2.2201889, 3.9539788, 0.68081295, 2.0254657], [4.1919436, 2.1811304, 5.245226, 1.3814918, 2.6714582, 2.3421094, 2.725411, 1.993065, 2.084805, 1.0947701, 3.4155354, 2.1116748, 2.285028], [2.4066565, 2.0332394, 3.5511441, 1.6245133, 2.8583903, 1.3723427, 1.8131765, 2.575133, 0.6115313, 2.8270473, 1.8897556, 2.8063068, 1.06303], [3.6858475, 2.6428518, 1.7977169, 2.6353307, 3.1963656, 1.9346539, 2.7085116, 1.9450716, 1.3616054, 1.2561216, 1.3922808, 2.7836199, 1.9583304], [1.9749199, 1.2672883, 1.9668479, 2.769634, 3.4846172, 0.6347681, 2.0163276, 1.3370608, 1.9845015, 1.7109427, 2.0112345, 2.5807114, 2.7410986], [1.6245373, 1.9522885, 3.718428, 0.59327054, 2.8444655, 1.2179064, 2.6887891, 1.3771594, 1.0503399, 1.2807636, 1.9772305, 1.7402966, 1.288495], [3.3252394, 1.3306284, 0.6509084, 2.6594944, 1.2038473, 1.6746637, 3.8632097, 3.2625954, 2.841096, 4.0096736, 3.1193933, 1.7407075, 2.2606988], [1.9496522, 3.182116, 0.96160233, 3.922863, 3.846222, 1.5412456, 2.38859, 2.888973, 1.5569149, 2.3454313, 1.15187, 2.1084523, 2.5224478], [3.6200264, 2.5741448, 1.6798081, 2.6474252, 2.4586165, 2.321875, 2.5320158, 2.1210105, 1.1914356, 1.9496932, 1.8725731, 0.95661825, 3.1593447], [1.201335, 2.0037363, 4.263499, 2.8801405, 1.8711069, 1.1797358, 4.100059, 2.3161082, 2.6913168, 1.1612973, 2.6823497, 1.3600312, 2.5689235], [1.8185592, 1.2359366, 2.4456804, 1.2496083, 2.4953394, 1.5246754, 1.1756262, 2.0455046, 2.4980977, 1.8690354, 4.555222, 2.0875478, 0.8919811], [3.2683287, 1.5944687, 1.6843513, 3.4589016, 1.7885898, 2.5004134, 0.5697929, 5.043588, 1.1460394, 1.9556644, 2.4632967, 2.5668263, 0.54394096], [2.1296892, 0.5755423, 3.576411, 2.4108758, 0.941475, 1.4675754, 3.6299427, 0.68157935, 2.4075048, 1.8293456, 0.6685183, 3.1077147, 2.296721], [2.492665, 1.8752984, 2.5626304, 2.169674, 2.305219, 2.2454007, 2.1638176, 2.4147105, 1.8337892, 2.2956452, 3.5529368, 1.4017599, 1.8393965], [0.6807022, 1.9155617, 3.076217, 1.9322945, 1.8440685, 2.4217048, 0.60641193, 1.096037, 1.9030367, 3.5366812, 1.136706, 1.7866169, 3.1675425], [1.3338842, 2.3377588, 2.8696945, 1.6604757, 2.4382725, 2.2916055, 3.0298476, 1.2159281, 3.0545824, 1.9790187, 2.1189866, 3.5642662, 1.1456091], [1.268696, 2.2631612, 1.1082486, 0.55637074, 2.3696666, 1.765917, 2.1553328, 2.0825055, 1.7612699, 1.828745, 0.6505367, 2.3942435, 1.3250967], [4.361742, 4.1186934, 1.1242288, 0.899431, 1.6434209, 1.795303, 0.6754079, 2.5532086, 2.3622396, 0.6389263, 3.4309745, 1.218293, 2.288514], [1.9780147, 2.1567054, 2.396465, 2.4032526, 2.7013993, 1.1419713, 2.1356008, 2.3697293, 0.851713, 1.693814, 1.5911611, 1.5266544, 2.3737402], [3.3388987, 0.795811, 2.455115, 3.462256, 0.88716304, 1.7436144, 1.5881604, 2.2259295, 1.1582233, 0.864593, 1.3923093, 1.3478261, 1.6999454], [0.88515526, 1.3449224, 1.7516252, 2.1780894, 1.0211825, 2.9649634, 2.1182616, 0.5600665, 1.5989554, 2.326285, 1.1907393, 1.8924838, 2.6855142], [0.78132606, 1.264953, 0.5452172, 1.8265365, 1.0961499, 1.7175868, 3.0624897, 1.006173, 1.0207398, 1.747101, 1.6163598, 1.6315758, 2.981657], [1.5303941, 1.0386107, 2.3551352, 1.5896848, 2.3259275, 1.9336708, 1.8923246, 1.125928, 0.54652524, 3.0776608, 1.1383376, 1.1052914, 1.0897596], [1.6501511, 2.346557, 0.8363905, 1.6283355, 0.45895502, 3.2452095, 2.3128812, 0.85166097, 1.6789466, 1.4915913, 1.0562553, 0.62103426, 2.76338], [0.9993205, 2.2275712, 3.757061, 1.2031181, 1.1113985, 1.0299067, 1.0383184, 0.55262, 0.7184238, 1.5559059, 0.6717386, 1.2521865, 1.9902629], [0.46633846, 1.017432, 3.158629, 1.0700802, 1.933458, 0.5607002, 2.5799456, 0.55074245, 1.6145066, 2.0754967, 1.3220922, 0.5580254, 0.9697166], [2.6067698, 1.5499619, 1.2874482, 2.7226703, 0.5174418, 2.0900345, 2.2781358, 1.6670065, 1.36216, 1.166769, 2.5500462, 1.5871317, 1.9649343], [1.899452, 0.7398423, 1.9732196, 2.442593, 2.5620635, 1.0483997, 1.5342914, 0.95604473, 0.52330047, 2.6983097, 2.0483875, 0.9953715, 1.4692163], [2.600342, 1.1753591, 2.0330007, 1.2708421, 1.4204447, 1.1980648, 2.4604604, 1.5978208, 0.9145495, 1.6670634, 2.5343742, 2.9800735, 2.1695092], [2.5936449, 0.5753879, 2.2301793, 1.6159608, 0.92006874, 0.7100461, 0.9035037, 1.5970902, 0.5522956, 1.4435923, 0.8296537, 0.96343625, 1.5417758], [1.8944896, 1.9922996, 2.0803142, 1.9967399, 1.4700826, 1.9135928, 1.9972169, 3.5202649, 2.0417051, 1.4967898, 2.495423, 1.1056998, 1.3051382], [1.369317, 1.1226685, 1.4687761, 2.9177403, 1.1924126, 2.242874, 1.4565647, 2.208932, 0.94052273, 2.1460469, 1.9234957, 2.6928616, 1.0467683], [2.006865, 1.6718553, 1.5158625, 3.4545426, 2.896996, 1.918777, 1.9787806, 0.4990403, 1.678776, 1.0649993, 1.977411, 2.4510317, 0.5013234], [1.6900898, 0.69020313, 0.7083353, 1.8627913, 1.5910772, 0.760328, 0.91888535, 0.96573347, 0.85599834, 1.8368802, 1.4975348, 1.4870491, 0.962279], [1.8366648, 0.6666649, 1.9260113, 0.94121504, 2.2212026, 1.4290272, 2.2402081, 2.4315076, 1.2981315, 2.8939743, 0.8455531, 0.5657123, 1.936458], [0.559316, 1.8667732, 1.9741558, 1.8088185, 3.0879815, 0.98161465, 2.6257448, 1.4727292, 0.8919757, 1.4237512, 1.9466184, 1.3483907, 1.036235], [1.8697441, 1.4168375, 1.3840286, 1.4172105, 0.99998415, 1.8015575, 0.4413221, 1.5858691, 1.5420285, 1.385315, 1.899463, 1.1353198, 0.88163763], [0.8648484, 2.6016154, 1.601833, 0.8695299, 1.4902623, 0.9491937, 1.7382843, 1.4678199, 0.87596077, 0.61098176, 0.9803091, 1.4069697, 0.39979488], [1.7579241, 2.2446954, 1.197896, 2.8111713, 1.8210113, 1.8015759, 1.2545965, 1.4118887, 1.8336067, 1.7220749, 1.8722649, 1.2847868, 0.8702968], [1.2364346, 1.3835402, 0.8427393, 1.5120493, 1.4996254, 0.90579665, 0.900545, 0.9403639, 0.9722082, 1.2844466, 0.41917148, 1.3616976, 0.63337994]]}\n"
     ]
    }
   ],
   "source": [
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>4EBP1pS65</th>\n",
       "      <th>RbpS807</th>\n",
       "      <th>MAPKpT202</th>\n",
       "      <th>MEKpS217</th>\n",
       "      <th>S6</th>\n",
       "      <th>PAI-1</th>\n",
       "      <th>AKTpS473</th>\n",
       "      <th>AMPKpT172</th>\n",
       "      <th>b-Catenin</th>\n",
       "      <th>BIM</th>\n",
       "      <th>...</th>\n",
       "      <th>aHDAC</th>\n",
       "      <th>aMDM2</th>\n",
       "      <th>aJAK</th>\n",
       "      <th>aBRAFm</th>\n",
       "      <th>aPKC</th>\n",
       "      <th>aSTAT3</th>\n",
       "      <th>amTOR</th>\n",
       "      <th>aPI3K</th>\n",
       "      <th>aCDK4</th>\n",
       "      <th>aSRC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.872056</td>\n",
       "      <td>-0.411044</td>\n",
       "      <td>-2.620821</td>\n",
       "      <td>3.104337</td>\n",
       "      <td>-0.743947</td>\n",
       "      <td>-1.272077</td>\n",
       "      <td>0.682799</td>\n",
       "      <td>0.207566</td>\n",
       "      <td>-1.073564</td>\n",
       "      <td>2.039607</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.178446</td>\n",
       "      <td>-1.828852</td>\n",
       "      <td>-0.035953</td>\n",
       "      <td>3.698557</td>\n",
       "      <td>-0.128579</td>\n",
       "      <td>0.450156</td>\n",
       "      <td>0.881209</td>\n",
       "      <td>0.210576</td>\n",
       "      <td>3.003808</td>\n",
       "      <td>2.545527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.896746</td>\n",
       "      <td>-0.129591</td>\n",
       "      <td>-0.350443</td>\n",
       "      <td>0.459292</td>\n",
       "      <td>1.690510</td>\n",
       "      <td>1.403689</td>\n",
       "      <td>0.037429</td>\n",
       "      <td>-2.392194</td>\n",
       "      <td>-0.784808</td>\n",
       "      <td>0.676660</td>\n",
       "      <td>...</td>\n",
       "      <td>1.171187</td>\n",
       "      <td>-1.419314</td>\n",
       "      <td>1.225619</td>\n",
       "      <td>0.101867</td>\n",
       "      <td>0.759815</td>\n",
       "      <td>-0.589532</td>\n",
       "      <td>1.587433</td>\n",
       "      <td>0.790972</td>\n",
       "      <td>0.231998</td>\n",
       "      <td>1.461179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.950902</td>\n",
       "      <td>0.822978</td>\n",
       "      <td>0.196699</td>\n",
       "      <td>3.285514</td>\n",
       "      <td>-0.843176</td>\n",
       "      <td>-4.585529</td>\n",
       "      <td>3.835510</td>\n",
       "      <td>-1.913752</td>\n",
       "      <td>-2.905127</td>\n",
       "      <td>1.268464</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.072808</td>\n",
       "      <td>-0.842429</td>\n",
       "      <td>-0.849605</td>\n",
       "      <td>-1.310692</td>\n",
       "      <td>1.285581</td>\n",
       "      <td>-0.155561</td>\n",
       "      <td>-1.602297</td>\n",
       "      <td>2.279411</td>\n",
       "      <td>-1.784619</td>\n",
       "      <td>2.301683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.948739</td>\n",
       "      <td>-0.831350</td>\n",
       "      <td>-2.651083</td>\n",
       "      <td>2.771353</td>\n",
       "      <td>1.006357</td>\n",
       "      <td>0.159703</td>\n",
       "      <td>-0.475347</td>\n",
       "      <td>-0.478002</td>\n",
       "      <td>-1.284924</td>\n",
       "      <td>2.604939</td>\n",
       "      <td>...</td>\n",
       "      <td>0.483027</td>\n",
       "      <td>-3.533010</td>\n",
       "      <td>-0.044614</td>\n",
       "      <td>4.611651</td>\n",
       "      <td>0.107005</td>\n",
       "      <td>-0.612658</td>\n",
       "      <td>1.764507</td>\n",
       "      <td>-0.814713</td>\n",
       "      <td>2.538998</td>\n",
       "      <td>1.856650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-2.641239</td>\n",
       "      <td>-1.453549</td>\n",
       "      <td>0.429106</td>\n",
       "      <td>1.725165</td>\n",
       "      <td>-1.374680</td>\n",
       "      <td>-1.001256</td>\n",
       "      <td>-0.345604</td>\n",
       "      <td>-1.005148</td>\n",
       "      <td>-0.929291</td>\n",
       "      <td>0.137005</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.659138</td>\n",
       "      <td>-0.251867</td>\n",
       "      <td>0.349778</td>\n",
       "      <td>-1.672482</td>\n",
       "      <td>-1.083149</td>\n",
       "      <td>-1.345617</td>\n",
       "      <td>0.525038</td>\n",
       "      <td>-0.711037</td>\n",
       "      <td>1.817143</td>\n",
       "      <td>1.450305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-3.460640</td>\n",
       "      <td>0.617424</td>\n",
       "      <td>0.981222</td>\n",
       "      <td>-0.390109</td>\n",
       "      <td>-0.292220</td>\n",
       "      <td>-0.812624</td>\n",
       "      <td>-4.706495</td>\n",
       "      <td>-0.358261</td>\n",
       "      <td>3.002356</td>\n",
       "      <td>0.557624</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.008010</td>\n",
       "      <td>-0.174146</td>\n",
       "      <td>0.204835</td>\n",
       "      <td>-0.671163</td>\n",
       "      <td>0.820317</td>\n",
       "      <td>-0.889003</td>\n",
       "      <td>-0.988392</td>\n",
       "      <td>1.924198</td>\n",
       "      <td>3.164851</td>\n",
       "      <td>2.749420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-3.928865</td>\n",
       "      <td>1.214090</td>\n",
       "      <td>-1.411724</td>\n",
       "      <td>3.795213</td>\n",
       "      <td>-0.799834</td>\n",
       "      <td>-3.893729</td>\n",
       "      <td>3.077866</td>\n",
       "      <td>-0.360755</td>\n",
       "      <td>-2.813392</td>\n",
       "      <td>0.655698</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.624740</td>\n",
       "      <td>-0.776332</td>\n",
       "      <td>0.349412</td>\n",
       "      <td>-1.758432</td>\n",
       "      <td>-0.435169</td>\n",
       "      <td>-0.173215</td>\n",
       "      <td>-0.776527</td>\n",
       "      <td>2.436399</td>\n",
       "      <td>-2.214040</td>\n",
       "      <td>1.160645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.327380</td>\n",
       "      <td>-0.891904</td>\n",
       "      <td>0.024044</td>\n",
       "      <td>0.930231</td>\n",
       "      <td>1.151862</td>\n",
       "      <td>0.310351</td>\n",
       "      <td>1.353396</td>\n",
       "      <td>-0.843355</td>\n",
       "      <td>0.009305</td>\n",
       "      <td>1.002714</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.875759</td>\n",
       "      <td>0.192330</td>\n",
       "      <td>1.146967</td>\n",
       "      <td>-0.379891</td>\n",
       "      <td>0.878628</td>\n",
       "      <td>0.699762</td>\n",
       "      <td>0.325796</td>\n",
       "      <td>0.687188</td>\n",
       "      <td>0.829806</td>\n",
       "      <td>1.634269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-1.991185</td>\n",
       "      <td>-1.150827</td>\n",
       "      <td>-0.551300</td>\n",
       "      <td>1.773900</td>\n",
       "      <td>-0.389288</td>\n",
       "      <td>0.400987</td>\n",
       "      <td>-0.899439</td>\n",
       "      <td>-1.549552</td>\n",
       "      <td>-1.891864</td>\n",
       "      <td>0.429134</td>\n",
       "      <td>...</td>\n",
       "      <td>0.835332</td>\n",
       "      <td>-1.591986</td>\n",
       "      <td>-0.160672</td>\n",
       "      <td>-1.266939</td>\n",
       "      <td>-0.167861</td>\n",
       "      <td>-0.842814</td>\n",
       "      <td>1.888119</td>\n",
       "      <td>-0.172633</td>\n",
       "      <td>0.198788</td>\n",
       "      <td>0.826621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-2.259609</td>\n",
       "      <td>-0.637128</td>\n",
       "      <td>0.680766</td>\n",
       "      <td>-0.036886</td>\n",
       "      <td>-0.292708</td>\n",
       "      <td>-0.032379</td>\n",
       "      <td>-0.299428</td>\n",
       "      <td>-2.044994</td>\n",
       "      <td>-0.864947</td>\n",
       "      <td>1.392435</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.050174</td>\n",
       "      <td>0.042111</td>\n",
       "      <td>0.271335</td>\n",
       "      <td>-0.672674</td>\n",
       "      <td>0.016119</td>\n",
       "      <td>-0.503781</td>\n",
       "      <td>-0.477123</td>\n",
       "      <td>0.370982</td>\n",
       "      <td>0.999096</td>\n",
       "      <td>0.983332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-1.546800</td>\n",
       "      <td>-0.432313</td>\n",
       "      <td>0.629963</td>\n",
       "      <td>0.410558</td>\n",
       "      <td>0.705119</td>\n",
       "      <td>0.001445</td>\n",
       "      <td>0.591264</td>\n",
       "      <td>-1.847789</td>\n",
       "      <td>0.177765</td>\n",
       "      <td>0.384532</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.323283</td>\n",
       "      <td>-0.079195</td>\n",
       "      <td>1.736069</td>\n",
       "      <td>-0.303676</td>\n",
       "      <td>-0.155473</td>\n",
       "      <td>-1.092335</td>\n",
       "      <td>0.224352</td>\n",
       "      <td>0.252568</td>\n",
       "      <td>1.850353</td>\n",
       "      <td>2.084864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-2.363196</td>\n",
       "      <td>-0.684116</td>\n",
       "      <td>-0.216082</td>\n",
       "      <td>0.012231</td>\n",
       "      <td>1.439901</td>\n",
       "      <td>1.393731</td>\n",
       "      <td>-1.094260</td>\n",
       "      <td>-2.102693</td>\n",
       "      <td>-1.248412</td>\n",
       "      <td>1.324518</td>\n",
       "      <td>...</td>\n",
       "      <td>1.123578</td>\n",
       "      <td>-1.869890</td>\n",
       "      <td>0.800009</td>\n",
       "      <td>-0.004531</td>\n",
       "      <td>0.358553</td>\n",
       "      <td>-1.439327</td>\n",
       "      <td>1.205349</td>\n",
       "      <td>0.474192</td>\n",
       "      <td>-0.040106</td>\n",
       "      <td>0.487572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-2.781422</td>\n",
       "      <td>0.031471</td>\n",
       "      <td>-1.067497</td>\n",
       "      <td>3.933168</td>\n",
       "      <td>0.411822</td>\n",
       "      <td>-3.555287</td>\n",
       "      <td>3.235688</td>\n",
       "      <td>0.502516</td>\n",
       "      <td>-2.230639</td>\n",
       "      <td>1.547083</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.010213</td>\n",
       "      <td>-0.868846</td>\n",
       "      <td>0.262098</td>\n",
       "      <td>-1.327096</td>\n",
       "      <td>-0.080771</td>\n",
       "      <td>0.053265</td>\n",
       "      <td>-1.154866</td>\n",
       "      <td>1.307326</td>\n",
       "      <td>-2.081043</td>\n",
       "      <td>0.644857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.663403</td>\n",
       "      <td>-0.782446</td>\n",
       "      <td>-0.058910</td>\n",
       "      <td>0.944575</td>\n",
       "      <td>0.244926</td>\n",
       "      <td>-0.183967</td>\n",
       "      <td>0.238288</td>\n",
       "      <td>-0.451936</td>\n",
       "      <td>-0.541179</td>\n",
       "      <td>1.100496</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.612040</td>\n",
       "      <td>-0.261001</td>\n",
       "      <td>0.250238</td>\n",
       "      <td>-0.772283</td>\n",
       "      <td>0.347487</td>\n",
       "      <td>0.574438</td>\n",
       "      <td>0.198772</td>\n",
       "      <td>0.699943</td>\n",
       "      <td>0.696306</td>\n",
       "      <td>0.785633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-4.042816</td>\n",
       "      <td>-0.361369</td>\n",
       "      <td>-0.970381</td>\n",
       "      <td>4.609208</td>\n",
       "      <td>-1.774266</td>\n",
       "      <td>-4.414894</td>\n",
       "      <td>3.078842</td>\n",
       "      <td>0.085427</td>\n",
       "      <td>-3.110715</td>\n",
       "      <td>0.439969</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.877763</td>\n",
       "      <td>-0.504197</td>\n",
       "      <td>-0.146140</td>\n",
       "      <td>-2.342190</td>\n",
       "      <td>-0.936444</td>\n",
       "      <td>-0.829882</td>\n",
       "      <td>-0.818714</td>\n",
       "      <td>0.565387</td>\n",
       "      <td>-1.532950</td>\n",
       "      <td>0.913917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-2.296808</td>\n",
       "      <td>-0.708576</td>\n",
       "      <td>0.373424</td>\n",
       "      <td>-0.141053</td>\n",
       "      <td>-0.111972</td>\n",
       "      <td>-0.050830</td>\n",
       "      <td>-0.113379</td>\n",
       "      <td>-1.422165</td>\n",
       "      <td>-0.777803</td>\n",
       "      <td>0.888766</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.191344</td>\n",
       "      <td>-0.174255</td>\n",
       "      <td>0.802680</td>\n",
       "      <td>-0.524968</td>\n",
       "      <td>0.018229</td>\n",
       "      <td>-0.905221</td>\n",
       "      <td>-0.147846</td>\n",
       "      <td>0.604828</td>\n",
       "      <td>1.005505</td>\n",
       "      <td>0.715644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.324854</td>\n",
       "      <td>-0.728792</td>\n",
       "      <td>0.990268</td>\n",
       "      <td>0.921144</td>\n",
       "      <td>0.003153</td>\n",
       "      <td>-0.862987</td>\n",
       "      <td>1.173197</td>\n",
       "      <td>-1.999894</td>\n",
       "      <td>-0.892163</td>\n",
       "      <td>1.583683</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.406659</td>\n",
       "      <td>-0.318575</td>\n",
       "      <td>-0.942790</td>\n",
       "      <td>-0.717200</td>\n",
       "      <td>2.172976</td>\n",
       "      <td>1.120800</td>\n",
       "      <td>-0.157101</td>\n",
       "      <td>1.437608</td>\n",
       "      <td>0.544926</td>\n",
       "      <td>2.387476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.653109</td>\n",
       "      <td>-0.337680</td>\n",
       "      <td>-0.618155</td>\n",
       "      <td>1.430843</td>\n",
       "      <td>0.046495</td>\n",
       "      <td>-0.171187</td>\n",
       "      <td>0.415553</td>\n",
       "      <td>-0.446897</td>\n",
       "      <td>-0.800429</td>\n",
       "      <td>0.970917</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.958591</td>\n",
       "      <td>-0.252477</td>\n",
       "      <td>0.256227</td>\n",
       "      <td>-1.164940</td>\n",
       "      <td>0.452227</td>\n",
       "      <td>1.103147</td>\n",
       "      <td>0.668669</td>\n",
       "      <td>1.594595</td>\n",
       "      <td>0.115505</td>\n",
       "      <td>1.246438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-2.537582</td>\n",
       "      <td>-0.322856</td>\n",
       "      <td>0.547008</td>\n",
       "      <td>0.424902</td>\n",
       "      <td>-0.201817</td>\n",
       "      <td>-0.492872</td>\n",
       "      <td>-0.523845</td>\n",
       "      <td>-1.456370</td>\n",
       "      <td>-0.372719</td>\n",
       "      <td>0.482314</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.059564</td>\n",
       "      <td>-0.532526</td>\n",
       "      <td>0.839340</td>\n",
       "      <td>-0.696067</td>\n",
       "      <td>-0.686614</td>\n",
       "      <td>-1.217659</td>\n",
       "      <td>0.097329</td>\n",
       "      <td>0.265322</td>\n",
       "      <td>1.716853</td>\n",
       "      <td>1.236228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.899271</td>\n",
       "      <td>0.033521</td>\n",
       "      <td>0.615780</td>\n",
       "      <td>0.450205</td>\n",
       "      <td>0.541801</td>\n",
       "      <td>0.230352</td>\n",
       "      <td>-0.142771</td>\n",
       "      <td>-3.548732</td>\n",
       "      <td>-1.686276</td>\n",
       "      <td>1.257629</td>\n",
       "      <td>...</td>\n",
       "      <td>1.640286</td>\n",
       "      <td>-1.930219</td>\n",
       "      <td>-0.864138</td>\n",
       "      <td>-0.235442</td>\n",
       "      <td>2.054163</td>\n",
       "      <td>-0.168493</td>\n",
       "      <td>1.104536</td>\n",
       "      <td>1.541392</td>\n",
       "      <td>-0.052882</td>\n",
       "      <td>2.214386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-2.236514</td>\n",
       "      <td>-0.144888</td>\n",
       "      <td>1.355710</td>\n",
       "      <td>0.080829</td>\n",
       "      <td>-0.830868</td>\n",
       "      <td>-1.905962</td>\n",
       "      <td>-3.390527</td>\n",
       "      <td>1.190577</td>\n",
       "      <td>3.796469</td>\n",
       "      <td>0.883678</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.054956</td>\n",
       "      <td>1.437499</td>\n",
       "      <td>0.126183</td>\n",
       "      <td>-1.152921</td>\n",
       "      <td>0.939131</td>\n",
       "      <td>0.400291</td>\n",
       "      <td>-2.250029</td>\n",
       "      <td>1.820414</td>\n",
       "      <td>3.762659</td>\n",
       "      <td>2.922510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-2.358018</td>\n",
       "      <td>-1.519792</td>\n",
       "      <td>-1.844260</td>\n",
       "      <td>2.156663</td>\n",
       "      <td>0.110811</td>\n",
       "      <td>-0.800498</td>\n",
       "      <td>0.488954</td>\n",
       "      <td>0.100608</td>\n",
       "      <td>-0.727435</td>\n",
       "      <td>2.719263</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.143222</td>\n",
       "      <td>-1.834620</td>\n",
       "      <td>0.429176</td>\n",
       "      <td>4.377208</td>\n",
       "      <td>-0.103440</td>\n",
       "      <td>-0.803024</td>\n",
       "      <td>0.156252</td>\n",
       "      <td>-1.013611</td>\n",
       "      <td>3.446005</td>\n",
       "      <td>1.959750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-1.139071</td>\n",
       "      <td>-1.446429</td>\n",
       "      <td>0.158406</td>\n",
       "      <td>0.483169</td>\n",
       "      <td>0.901253</td>\n",
       "      <td>0.300392</td>\n",
       "      <td>0.221708</td>\n",
       "      <td>-0.553854</td>\n",
       "      <td>-0.454299</td>\n",
       "      <td>1.650572</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.923368</td>\n",
       "      <td>-0.258246</td>\n",
       "      <td>0.721357</td>\n",
       "      <td>-0.486289</td>\n",
       "      <td>0.477366</td>\n",
       "      <td>-0.150033</td>\n",
       "      <td>-0.056288</td>\n",
       "      <td>0.370408</td>\n",
       "      <td>0.557702</td>\n",
       "      <td>0.660661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-3.013250</td>\n",
       "      <td>-0.986838</td>\n",
       "      <td>0.764325</td>\n",
       "      <td>-0.036504</td>\n",
       "      <td>0.454510</td>\n",
       "      <td>-0.008513</td>\n",
       "      <td>-0.540425</td>\n",
       "      <td>-1.558288</td>\n",
       "      <td>-0.285840</td>\n",
       "      <td>1.032390</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.370892</td>\n",
       "      <td>-0.529771</td>\n",
       "      <td>1.310459</td>\n",
       "      <td>-0.410074</td>\n",
       "      <td>-0.556735</td>\n",
       "      <td>-1.942130</td>\n",
       "      <td>-0.157731</td>\n",
       "      <td>-0.064213</td>\n",
       "      <td>1.578249</td>\n",
       "      <td>1.111256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-1.877235</td>\n",
       "      <td>0.424633</td>\n",
       "      <td>-0.992643</td>\n",
       "      <td>0.959904</td>\n",
       "      <td>0.585144</td>\n",
       "      <td>0.922152</td>\n",
       "      <td>-0.900415</td>\n",
       "      <td>-1.995735</td>\n",
       "      <td>-1.594542</td>\n",
       "      <td>0.644863</td>\n",
       "      <td>...</td>\n",
       "      <td>0.088354</td>\n",
       "      <td>-1.864121</td>\n",
       "      <td>0.334880</td>\n",
       "      <td>-0.683182</td>\n",
       "      <td>0.333413</td>\n",
       "      <td>-0.186147</td>\n",
       "      <td>1.930306</td>\n",
       "      <td>1.698380</td>\n",
       "      <td>-0.482302</td>\n",
       "      <td>1.073349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-1.308550</td>\n",
       "      <td>-0.654921</td>\n",
       "      <td>1.422602</td>\n",
       "      <td>-0.164485</td>\n",
       "      <td>-0.353745</td>\n",
       "      <td>-0.729850</td>\n",
       "      <td>0.821530</td>\n",
       "      <td>-2.970122</td>\n",
       "      <td>-1.128787</td>\n",
       "      <td>1.371953</td>\n",
       "      <td>...</td>\n",
       "      <td>1.014037</td>\n",
       "      <td>-0.231829</td>\n",
       "      <td>-0.390347</td>\n",
       "      <td>-0.469886</td>\n",
       "      <td>1.843718</td>\n",
       "      <td>-0.358859</td>\n",
       "      <td>-0.503719</td>\n",
       "      <td>1.342493</td>\n",
       "      <td>0.854125</td>\n",
       "      <td>2.317487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.894093</td>\n",
       "      <td>-0.802156</td>\n",
       "      <td>-1.012399</td>\n",
       "      <td>2.594638</td>\n",
       "      <td>-0.787289</td>\n",
       "      <td>-1.963878</td>\n",
       "      <td>1.440443</td>\n",
       "      <td>-1.345431</td>\n",
       "      <td>-1.165298</td>\n",
       "      <td>2.652373</td>\n",
       "      <td>...</td>\n",
       "      <td>0.373486</td>\n",
       "      <td>-1.894949</td>\n",
       "      <td>-1.234971</td>\n",
       "      <td>4.146297</td>\n",
       "      <td>1.592171</td>\n",
       "      <td>0.467810</td>\n",
       "      <td>0.055439</td>\n",
       "      <td>0.053588</td>\n",
       "      <td>3.433229</td>\n",
       "      <td>3.686564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-2.400465</td>\n",
       "      <td>-1.839269</td>\n",
       "      <td>0.255522</td>\n",
       "      <td>1.159209</td>\n",
       "      <td>-1.284835</td>\n",
       "      <td>-0.559214</td>\n",
       "      <td>0.064862</td>\n",
       "      <td>-0.970943</td>\n",
       "      <td>-1.334375</td>\n",
       "      <td>0.543458</td>\n",
       "      <td>...</td>\n",
       "      <td>0.209082</td>\n",
       "      <td>0.106403</td>\n",
       "      <td>0.313118</td>\n",
       "      <td>-1.501383</td>\n",
       "      <td>-0.378307</td>\n",
       "      <td>-1.033180</td>\n",
       "      <td>0.279864</td>\n",
       "      <td>-0.371531</td>\n",
       "      <td>1.105794</td>\n",
       "      <td>0.929721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-1.767398</td>\n",
       "      <td>0.058631</td>\n",
       "      <td>0.600139</td>\n",
       "      <td>0.470849</td>\n",
       "      <td>-1.029416</td>\n",
       "      <td>-0.508532</td>\n",
       "      <td>-0.365592</td>\n",
       "      <td>-2.220076</td>\n",
       "      <td>-0.990583</td>\n",
       "      <td>1.066363</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.676062</td>\n",
       "      <td>0.135583</td>\n",
       "      <td>-0.436097</td>\n",
       "      <td>-1.024359</td>\n",
       "      <td>-0.114698</td>\n",
       "      <td>0.399229</td>\n",
       "      <td>-0.368508</td>\n",
       "      <td>0.596515</td>\n",
       "      <td>1.134850</td>\n",
       "      <td>1.227357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-1.810846</td>\n",
       "      <td>0.400173</td>\n",
       "      <td>-0.403137</td>\n",
       "      <td>0.806620</td>\n",
       "      <td>-0.966730</td>\n",
       "      <td>-0.522409</td>\n",
       "      <td>0.080466</td>\n",
       "      <td>-1.315207</td>\n",
       "      <td>-1.123932</td>\n",
       "      <td>0.209111</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.226567</td>\n",
       "      <td>-0.168487</td>\n",
       "      <td>0.337551</td>\n",
       "      <td>-1.203619</td>\n",
       "      <td>-0.006910</td>\n",
       "      <td>0.347959</td>\n",
       "      <td>0.577111</td>\n",
       "      <td>1.829015</td>\n",
       "      <td>0.563309</td>\n",
       "      <td>1.301421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>-1.379845</td>\n",
       "      <td>-1.060709</td>\n",
       "      <td>0.331991</td>\n",
       "      <td>1.049124</td>\n",
       "      <td>0.811408</td>\n",
       "      <td>-0.141650</td>\n",
       "      <td>-0.188758</td>\n",
       "      <td>-0.588060</td>\n",
       "      <td>-0.049215</td>\n",
       "      <td>1.244120</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.791588</td>\n",
       "      <td>-0.616516</td>\n",
       "      <td>0.758017</td>\n",
       "      <td>-0.657388</td>\n",
       "      <td>-0.227477</td>\n",
       "      <td>-0.462471</td>\n",
       "      <td>0.188887</td>\n",
       "      <td>0.030902</td>\n",
       "      <td>1.269050</td>\n",
       "      <td>1.181245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.157900</td>\n",
       "      <td>-0.100397</td>\n",
       "      <td>1.288241</td>\n",
       "      <td>0.282577</td>\n",
       "      <td>-0.103136</td>\n",
       "      <td>-0.719892</td>\n",
       "      <td>1.953219</td>\n",
       "      <td>-3.259623</td>\n",
       "      <td>-0.665183</td>\n",
       "      <td>0.724095</td>\n",
       "      <td>...</td>\n",
       "      <td>1.061646</td>\n",
       "      <td>0.218746</td>\n",
       "      <td>0.035263</td>\n",
       "      <td>-0.363488</td>\n",
       "      <td>2.244980</td>\n",
       "      <td>0.490936</td>\n",
       "      <td>-0.121636</td>\n",
       "      <td>1.659273</td>\n",
       "      <td>1.126229</td>\n",
       "      <td>3.291094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>-2.948377</td>\n",
       "      <td>0.659866</td>\n",
       "      <td>-0.769525</td>\n",
       "      <td>3.294601</td>\n",
       "      <td>0.305532</td>\n",
       "      <td>-3.412192</td>\n",
       "      <td>4.015710</td>\n",
       "      <td>-0.757214</td>\n",
       "      <td>-2.003659</td>\n",
       "      <td>0.687495</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.541908</td>\n",
       "      <td>-0.331525</td>\n",
       "      <td>1.240151</td>\n",
       "      <td>-0.973384</td>\n",
       "      <td>-0.008768</td>\n",
       "      <td>-0.576599</td>\n",
       "      <td>-1.119400</td>\n",
       "      <td>1.528991</td>\n",
       "      <td>-1.499740</td>\n",
       "      <td>1.548476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>-4.005547</td>\n",
       "      <td>0.793784</td>\n",
       "      <td>-1.441985</td>\n",
       "      <td>3.462229</td>\n",
       "      <td>0.950470</td>\n",
       "      <td>-2.461949</td>\n",
       "      <td>1.919720</td>\n",
       "      <td>-1.046323</td>\n",
       "      <td>-3.024752</td>\n",
       "      <td>1.221029</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.963267</td>\n",
       "      <td>-2.480490</td>\n",
       "      <td>0.340751</td>\n",
       "      <td>-0.845338</td>\n",
       "      <td>-0.199584</td>\n",
       "      <td>-1.236029</td>\n",
       "      <td>0.106771</td>\n",
       "      <td>1.411110</td>\n",
       "      <td>-2.678851</td>\n",
       "      <td>0.471768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>-0.679875</td>\n",
       "      <td>0.017368</td>\n",
       "      <td>1.367725</td>\n",
       "      <td>0.293294</td>\n",
       "      <td>-1.047504</td>\n",
       "      <td>-1.319338</td>\n",
       "      <td>0.982858</td>\n",
       "      <td>-3.107867</td>\n",
       "      <td>-1.270007</td>\n",
       "      <td>0.896686</td>\n",
       "      <td>...</td>\n",
       "      <td>0.511988</td>\n",
       "      <td>-0.243498</td>\n",
       "      <td>-1.046177</td>\n",
       "      <td>-0.747351</td>\n",
       "      <td>1.996471</td>\n",
       "      <td>0.450203</td>\n",
       "      <td>-0.303758</td>\n",
       "      <td>1.786237</td>\n",
       "      <td>0.969292</td>\n",
       "      <td>2.690464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>-4.000369</td>\n",
       "      <td>-0.041893</td>\n",
       "      <td>-3.070163</td>\n",
       "      <td>5.606662</td>\n",
       "      <td>-0.378620</td>\n",
       "      <td>-4.656178</td>\n",
       "      <td>3.502934</td>\n",
       "      <td>1.156978</td>\n",
       "      <td>-2.503775</td>\n",
       "      <td>2.615774</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.230067</td>\n",
       "      <td>-2.445220</td>\n",
       "      <td>-0.030082</td>\n",
       "      <td>3.536401</td>\n",
       "      <td>-0.661577</td>\n",
       "      <td>-0.599726</td>\n",
       "      <td>-0.942326</td>\n",
       "      <td>-0.076693</td>\n",
       "      <td>0.807260</td>\n",
       "      <td>1.943946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>-2.527288</td>\n",
       "      <td>0.121911</td>\n",
       "      <td>-0.012236</td>\n",
       "      <td>0.911170</td>\n",
       "      <td>-0.400248</td>\n",
       "      <td>-0.480092</td>\n",
       "      <td>-0.346580</td>\n",
       "      <td>-1.451330</td>\n",
       "      <td>-0.631969</td>\n",
       "      <td>0.352734</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.406115</td>\n",
       "      <td>-0.524003</td>\n",
       "      <td>0.845330</td>\n",
       "      <td>-1.088724</td>\n",
       "      <td>-0.581875</td>\n",
       "      <td>-0.688950</td>\n",
       "      <td>0.567225</td>\n",
       "      <td>1.159975</td>\n",
       "      <td>1.136052</td>\n",
       "      <td>1.697033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>-3.357053</td>\n",
       "      <td>0.664412</td>\n",
       "      <td>1.878070</td>\n",
       "      <td>-0.439225</td>\n",
       "      <td>-2.024829</td>\n",
       "      <td>-2.238734</td>\n",
       "      <td>-3.911663</td>\n",
       "      <td>-0.300563</td>\n",
       "      <td>3.385821</td>\n",
       "      <td>0.625541</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.181762</td>\n",
       "      <td>1.737855</td>\n",
       "      <td>-0.323839</td>\n",
       "      <td>-1.339306</td>\n",
       "      <td>0.477884</td>\n",
       "      <td>0.046543</td>\n",
       "      <td>-2.670864</td>\n",
       "      <td>1.820988</td>\n",
       "      <td>4.204053</td>\n",
       "      <td>3.245181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>-1.814393</td>\n",
       "      <td>0.246916</td>\n",
       "      <td>-0.210434</td>\n",
       "      <td>0.639063</td>\n",
       "      <td>-0.898355</td>\n",
       "      <td>-0.526813</td>\n",
       "      <td>0.019384</td>\n",
       "      <td>-1.316944</td>\n",
       "      <td>-1.034600</td>\n",
       "      <td>0.253761</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.107153</td>\n",
       "      <td>-0.171424</td>\n",
       "      <td>0.335487</td>\n",
       "      <td>-1.068318</td>\n",
       "      <td>-0.043001</td>\n",
       "      <td>0.165777</td>\n",
       "      <td>0.415194</td>\n",
       "      <td>1.520738</td>\n",
       "      <td>0.763440</td>\n",
       "      <td>1.142637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>-3.939159</td>\n",
       "      <td>0.769324</td>\n",
       "      <td>-0.852479</td>\n",
       "      <td>3.308945</td>\n",
       "      <td>-0.601403</td>\n",
       "      <td>-3.906509</td>\n",
       "      <td>2.900601</td>\n",
       "      <td>-0.365795</td>\n",
       "      <td>-2.554142</td>\n",
       "      <td>0.785277</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.278189</td>\n",
       "      <td>-0.784855</td>\n",
       "      <td>0.343422</td>\n",
       "      <td>-1.365775</td>\n",
       "      <td>-0.539908</td>\n",
       "      <td>-0.701923</td>\n",
       "      <td>-1.246424</td>\n",
       "      <td>1.541746</td>\n",
       "      <td>-1.633240</td>\n",
       "      <td>0.699840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>-3.394251</td>\n",
       "      <td>0.592965</td>\n",
       "      <td>1.570727</td>\n",
       "      <td>-0.543393</td>\n",
       "      <td>-1.844093</td>\n",
       "      <td>-2.257185</td>\n",
       "      <td>-3.725614</td>\n",
       "      <td>0.322266</td>\n",
       "      <td>3.472966</td>\n",
       "      <td>0.121872</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.322932</td>\n",
       "      <td>1.521489</td>\n",
       "      <td>0.207507</td>\n",
       "      <td>-1.191601</td>\n",
       "      <td>0.479994</td>\n",
       "      <td>-0.354897</td>\n",
       "      <td>-2.341587</td>\n",
       "      <td>2.054834</td>\n",
       "      <td>4.210462</td>\n",
       "      <td>2.977492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>-1.986007</td>\n",
       "      <td>-1.986503</td>\n",
       "      <td>-2.179479</td>\n",
       "      <td>3.918332</td>\n",
       "      <td>-1.718379</td>\n",
       "      <td>-1.793242</td>\n",
       "      <td>0.683774</td>\n",
       "      <td>0.653748</td>\n",
       "      <td>-1.370886</td>\n",
       "      <td>1.823878</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.431468</td>\n",
       "      <td>-1.556717</td>\n",
       "      <td>-0.531506</td>\n",
       "      <td>3.114799</td>\n",
       "      <td>-0.629854</td>\n",
       "      <td>-0.206511</td>\n",
       "      <td>0.839022</td>\n",
       "      <td>-1.660436</td>\n",
       "      <td>3.684899</td>\n",
       "      <td>2.298799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>-2.598793</td>\n",
       "      <td>-1.134072</td>\n",
       "      <td>-1.670676</td>\n",
       "      <td>2.722619</td>\n",
       "      <td>0.020966</td>\n",
       "      <td>-1.242541</td>\n",
       "      <td>0.078488</td>\n",
       "      <td>0.066403</td>\n",
       "      <td>-0.322351</td>\n",
       "      <td>2.312810</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.011442</td>\n",
       "      <td>-2.192891</td>\n",
       "      <td>0.465836</td>\n",
       "      <td>4.206109</td>\n",
       "      <td>-0.808283</td>\n",
       "      <td>-1.115461</td>\n",
       "      <td>0.401427</td>\n",
       "      <td>-1.353117</td>\n",
       "      <td>4.157353</td>\n",
       "      <td>2.480334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>-1.886160</td>\n",
       "      <td>-0.906302</td>\n",
       "      <td>-2.199611</td>\n",
       "      <td>2.761086</td>\n",
       "      <td>-0.531649</td>\n",
       "      <td>-1.331519</td>\n",
       "      <td>0.543024</td>\n",
       "      <td>0.297307</td>\n",
       "      <td>-0.811179</td>\n",
       "      <td>2.283122</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.828899</td>\n",
       "      <td>-1.940721</td>\n",
       "      <td>-0.065191</td>\n",
       "      <td>4.396338</td>\n",
       "      <td>-0.240891</td>\n",
       "      <td>-0.072191</td>\n",
       "      <td>0.430240</td>\n",
       "      <td>-0.784813</td>\n",
       "      <td>3.736512</td>\n",
       "      <td>2.162158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>-0.626205</td>\n",
       "      <td>-0.710999</td>\n",
       "      <td>0.248432</td>\n",
       "      <td>1.048743</td>\n",
       "      <td>0.064190</td>\n",
       "      <td>-0.165516</td>\n",
       "      <td>0.052239</td>\n",
       "      <td>-1.074766</td>\n",
       "      <td>-0.628323</td>\n",
       "      <td>1.604165</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.470870</td>\n",
       "      <td>-0.044635</td>\n",
       "      <td>-0.281108</td>\n",
       "      <td>-0.919989</td>\n",
       "      <td>0.345378</td>\n",
       "      <td>0.975879</td>\n",
       "      <td>-0.130505</td>\n",
       "      <td>0.466096</td>\n",
       "      <td>0.689897</td>\n",
       "      <td>1.053322</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45 rows  99 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "0   4EBP1pS65   RbpS807  MAPKpT202  MEKpS217        S6     PAI-1  AKTpS473  \\\n",
       "0   -1.872056 -0.411044  -2.620821  3.104337 -0.743947 -1.272077  0.682799   \n",
       "1   -0.896746 -0.129591  -0.350443  0.459292  1.690510  1.403689  0.037429   \n",
       "2   -2.950902  0.822978   0.196699  3.285514 -0.843176 -4.585529  3.835510   \n",
       "3   -1.948739 -0.831350  -2.651083  2.771353  1.006357  0.159703 -0.475347   \n",
       "4   -2.641239 -1.453549   0.429106  1.725165 -1.374680 -1.001256 -0.345604   \n",
       "5   -3.460640  0.617424   0.981222 -0.390109 -0.292220 -0.812624 -4.706495   \n",
       "6   -3.928865  1.214090  -1.411724  3.795213 -0.799834 -3.893729  3.077866   \n",
       "7    0.327380 -0.891904   0.024044  0.930231  1.151862  0.310351  1.353396   \n",
       "8   -1.991185 -1.150827  -0.551300  1.773900 -0.389288  0.400987 -0.899439   \n",
       "9   -2.259609 -0.637128   0.680766 -0.036886 -0.292708 -0.032379 -0.299428   \n",
       "10  -1.546800 -0.432313   0.629963  0.410558  0.705119  0.001445  0.591264   \n",
       "11  -2.363196 -0.684116  -0.216082  0.012231  1.439901  1.393731 -1.094260   \n",
       "12  -2.781422  0.031471  -1.067497  3.933168  0.411822 -3.555287  3.235688   \n",
       "13  -0.663403 -0.782446  -0.058910  0.944575  0.244926 -0.183967  0.238288   \n",
       "14  -4.042816 -0.361369  -0.970381  4.609208 -1.774266 -4.414894  3.078842   \n",
       "15  -2.296808 -0.708576   0.373424 -0.141053 -0.111972 -0.050830 -0.113379   \n",
       "16   0.324854 -0.728792   0.990268  0.921144  0.003153 -0.862987  1.173197   \n",
       "17  -0.653109 -0.337680  -0.618155  1.430843  0.046495 -0.171187  0.415553   \n",
       "18  -2.537582 -0.322856   0.547008  0.424902 -0.201817 -0.492872 -0.523845   \n",
       "19  -0.899271  0.033521   0.615780  0.450205  0.541801  0.230352 -0.142771   \n",
       "20  -2.236514 -0.144888   1.355710  0.080829 -0.830868 -1.905962 -3.390527   \n",
       "21  -2.358018 -1.519792  -1.844260  2.156663  0.110811 -0.800498  0.488954   \n",
       "22  -1.139071 -1.446429   0.158406  0.483169  0.901253  0.300392  0.221708   \n",
       "23  -3.013250 -0.986838   0.764325 -0.036504  0.454510 -0.008513 -0.540425   \n",
       "24  -1.877235  0.424633  -0.992643  0.959904  0.585144  0.922152 -0.900415   \n",
       "25  -1.308550 -0.654921   1.422602 -0.164485 -0.353745 -0.729850  0.821530   \n",
       "26  -0.894093 -0.802156  -1.012399  2.594638 -0.787289 -1.963878  1.440443   \n",
       "27  -2.400465 -1.839269   0.255522  1.159209 -1.284835 -0.559214  0.064862   \n",
       "28  -1.767398  0.058631   0.600139  0.470849 -1.029416 -0.508532 -0.365592   \n",
       "29  -1.810846  0.400173  -0.403137  0.806620 -0.966730 -0.522409  0.080466   \n",
       "30  -1.379845 -1.060709   0.331991  1.049124  0.811408 -0.141650 -0.188758   \n",
       "31   0.157900 -0.100397   1.288241  0.282577 -0.103136 -0.719892  1.953219   \n",
       "32  -2.948377  0.659866  -0.769525  3.294601  0.305532 -3.412192  4.015710   \n",
       "33  -4.005547  0.793784  -1.441985  3.462229  0.950470 -2.461949  1.919720   \n",
       "34  -0.679875  0.017368   1.367725  0.293294 -1.047504 -1.319338  0.982858   \n",
       "35  -4.000369 -0.041893  -3.070163  5.606662 -0.378620 -4.656178  3.502934   \n",
       "36  -2.527288  0.121911  -0.012236  0.911170 -0.400248 -0.480092 -0.346580   \n",
       "37  -3.357053  0.664412   1.878070 -0.439225 -2.024829 -2.238734 -3.911663   \n",
       "38  -1.814393  0.246916  -0.210434  0.639063 -0.898355 -0.526813  0.019384   \n",
       "39  -3.939159  0.769324  -0.852479  3.308945 -0.601403 -3.906509  2.900601   \n",
       "40  -3.394251  0.592965   1.570727 -0.543393 -1.844093 -2.257185 -3.725614   \n",
       "41  -1.986007 -1.986503  -2.179479  3.918332 -1.718379 -1.793242  0.683774   \n",
       "42  -2.598793 -1.134072  -1.670676  2.722619  0.020966 -1.242541  0.078488   \n",
       "43  -1.886160 -0.906302  -2.199611  2.761086 -0.531649 -1.331519  0.543024   \n",
       "44  -0.626205 -0.710999   0.248432  1.048743  0.064190 -0.165516  0.052239   \n",
       "\n",
       "0   AMPKpT172  b-Catenin       BIM  ...     aHDAC     aMDM2      aJAK  \\\n",
       "0    0.207566  -1.073564  2.039607  ... -1.178446 -1.828852 -0.035953   \n",
       "1   -2.392194  -0.784808  0.676660  ...  1.171187 -1.419314  1.225619   \n",
       "2   -1.913752  -2.905127  1.268464  ... -1.072808 -0.842429 -0.849605   \n",
       "3   -0.478002  -1.284924  2.604939  ...  0.483027 -3.533010 -0.044614   \n",
       "4   -1.005148  -0.929291  0.137005  ... -0.659138 -0.251867  0.349778   \n",
       "5   -0.358261   3.002356  0.557624  ... -1.008010 -0.174146  0.204835   \n",
       "6   -0.360755  -2.813392  0.655698  ... -2.624740 -0.776332  0.349412   \n",
       "7   -0.843355   0.009305  1.002714  ... -0.875759  0.192330  1.146967   \n",
       "8   -1.549552  -1.891864  0.429134  ...  0.835332 -1.591986 -0.160672   \n",
       "9   -2.044994  -0.864947  1.392435  ... -0.050174  0.042111  0.271335   \n",
       "10  -1.847789   0.177765  0.384532  ... -0.323283 -0.079195  1.736069   \n",
       "11  -2.102693  -1.248412  1.324518  ...  1.123578 -1.869890  0.800009   \n",
       "12   0.502516  -2.230639  1.547083  ... -3.010213 -0.868846  0.262098   \n",
       "13  -0.451936  -0.541179  1.100496  ... -1.612040 -0.261001  0.250238   \n",
       "14   0.085427  -3.110715  0.439969  ... -1.877763 -0.504197 -0.146140   \n",
       "15  -1.422165  -0.777803  0.888766  ... -0.191344 -0.174255  0.802680   \n",
       "16  -1.999894  -0.892163  1.583683  ... -0.406659 -0.318575 -0.942790   \n",
       "17  -0.446897  -0.800429  0.970917  ... -1.958591 -0.252477  0.256227   \n",
       "18  -1.456370  -0.372719  0.482314  ... -1.059564 -0.532526  0.839340   \n",
       "19  -3.548732  -1.686276  1.257629  ...  1.640286 -1.930219 -0.864138   \n",
       "20   1.190577   3.796469  0.883678  ... -3.054956  1.437499  0.126183   \n",
       "21   0.100608  -0.727435  2.719263  ... -0.143222 -1.834620  0.429176   \n",
       "22  -0.553854  -0.454299  1.650572  ... -0.923368 -0.258246  0.721357   \n",
       "23  -1.558288  -0.285840  1.032390  ... -0.370892 -0.529771  1.310459   \n",
       "24  -1.995735  -1.594542  0.644863  ...  0.088354 -1.864121  0.334880   \n",
       "25  -2.970122  -1.128787  1.371953  ...  1.014037 -0.231829 -0.390347   \n",
       "26  -1.345431  -1.165298  2.652373  ...  0.373486 -1.894949 -1.234971   \n",
       "27  -0.970943  -1.334375  0.543458  ...  0.209082  0.106403  0.313118   \n",
       "28  -2.220076  -0.990583  1.066363  ... -0.676062  0.135583 -0.436097   \n",
       "29  -1.315207  -1.123932  0.209111  ... -1.226567 -0.168487  0.337551   \n",
       "30  -0.588060  -0.049215  1.244120  ... -1.791588 -0.616516  0.758017   \n",
       "31  -3.259623  -0.665183  0.724095  ...  1.061646  0.218746  0.035263   \n",
       "32  -0.757214  -2.003659  0.687495  ... -1.541908 -0.331525  1.240151   \n",
       "33  -1.046323  -3.024752  1.221029  ... -0.963267 -2.480490  0.340751   \n",
       "34  -3.107867  -1.270007  0.896686  ...  0.511988 -0.243498 -1.046177   \n",
       "35   1.156978  -2.503775  2.615774  ... -2.230067 -2.445220 -0.030082   \n",
       "36  -1.451330  -0.631969  0.352734  ... -1.406115 -0.524003  0.845330   \n",
       "37  -0.300563   3.385821  0.625541  ... -2.181762  1.737855 -0.323839   \n",
       "38  -1.316944  -1.034600  0.253761  ... -1.107153 -0.171424  0.335487   \n",
       "39  -0.365795  -2.554142  0.785277  ... -2.278189 -0.784855  0.343422   \n",
       "40   0.322266   3.472966  0.121872  ... -2.322932  1.521489  0.207507   \n",
       "41   0.653748  -1.370886  1.823878  ... -0.431468 -1.556717 -0.531506   \n",
       "42   0.066403  -0.322351  2.312810  ... -1.011442 -2.192891  0.465836   \n",
       "43   0.297307  -0.811179  2.283122  ... -0.828899 -1.940721 -0.065191   \n",
       "44  -1.074766  -0.628323  1.604165  ... -1.470870 -0.044635 -0.281108   \n",
       "\n",
       "0     aBRAFm      aPKC    aSTAT3     amTOR     aPI3K     aCDK4      aSRC  \n",
       "0   3.698557 -0.128579  0.450156  0.881209  0.210576  3.003808  2.545527  \n",
       "1   0.101867  0.759815 -0.589532  1.587433  0.790972  0.231998  1.461179  \n",
       "2  -1.310692  1.285581 -0.155561 -1.602297  2.279411 -1.784619  2.301683  \n",
       "3   4.611651  0.107005 -0.612658  1.764507 -0.814713  2.538998  1.856650  \n",
       "4  -1.672482 -1.083149 -1.345617  0.525038 -0.711037  1.817143  1.450305  \n",
       "5  -0.671163  0.820317 -0.889003 -0.988392  1.924198  3.164851  2.749420  \n",
       "6  -1.758432 -0.435169 -0.173215 -0.776527  2.436399 -2.214040  1.160645  \n",
       "7  -0.379891  0.878628  0.699762  0.325796  0.687188  0.829806  1.634269  \n",
       "8  -1.266939 -0.167861 -0.842814  1.888119 -0.172633  0.198788  0.826621  \n",
       "9  -0.672674  0.016119 -0.503781 -0.477123  0.370982  0.999096  0.983332  \n",
       "10 -0.303676 -0.155473 -1.092335  0.224352  0.252568  1.850353  2.084864  \n",
       "11 -0.004531  0.358553 -1.439327  1.205349  0.474192 -0.040106  0.487572  \n",
       "12 -1.327096 -0.080771  0.053265 -1.154866  1.307326 -2.081043  0.644857  \n",
       "13 -0.772283  0.347487  0.574438  0.198772  0.699943  0.696306  0.785633  \n",
       "14 -2.342190 -0.936444 -0.829882 -0.818714  0.565387 -1.532950  0.913917  \n",
       "15 -0.524968  0.018229 -0.905221 -0.147846  0.604828  1.005505  0.715644  \n",
       "16 -0.717200  2.172976  1.120800 -0.157101  1.437608  0.544926  2.387476  \n",
       "17 -1.164940  0.452227  1.103147  0.668669  1.594595  0.115505  1.246438  \n",
       "18 -0.696067 -0.686614 -1.217659  0.097329  0.265322  1.716853  1.236228  \n",
       "19 -0.235442  2.054163 -0.168493  1.104536  1.541392 -0.052882  2.214386  \n",
       "20 -1.152921  0.939131  0.400291 -2.250029  1.820414  3.762659  2.922510  \n",
       "21  4.377208 -0.103440 -0.803024  0.156252 -1.013611  3.446005  1.959750  \n",
       "22 -0.486289  0.477366 -0.150033 -0.056288  0.370408  0.557702  0.660661  \n",
       "23 -0.410074 -0.556735 -1.942130 -0.157731 -0.064213  1.578249  1.111256  \n",
       "24 -0.683182  0.333413 -0.186147  1.930306  1.698380 -0.482302  1.073349  \n",
       "25 -0.469886  1.843718 -0.358859 -0.503719  1.342493  0.854125  2.317487  \n",
       "26  4.146297  1.592171  0.467810  0.055439  0.053588  3.433229  3.686564  \n",
       "27 -1.501383 -0.378307 -1.033180  0.279864 -0.371531  1.105794  0.929721  \n",
       "28 -1.024359 -0.114698  0.399229 -0.368508  0.596515  1.134850  1.227357  \n",
       "29 -1.203619 -0.006910  0.347959  0.577111  1.829015  0.563309  1.301421  \n",
       "30 -0.657388 -0.227477 -0.462471  0.188887  0.030902  1.269050  1.181245  \n",
       "31 -0.363488  2.244980  0.490936 -0.121636  1.659273  1.126229  3.291094  \n",
       "32 -0.973384 -0.008768 -0.576599 -1.119400  1.528991 -1.499740  1.548476  \n",
       "33 -0.845338 -0.199584 -1.236029  0.106771  1.411110 -2.678851  0.471768  \n",
       "34 -0.747351  1.996471  0.450203 -0.303758  1.786237  0.969292  2.690464  \n",
       "35  3.536401 -0.661577 -0.599726 -0.942326 -0.076693  0.807260  1.943946  \n",
       "36 -1.088724 -0.581875 -0.688950  0.567225  1.159975  1.136052  1.697033  \n",
       "37 -1.339306  0.477884  0.046543 -2.670864  1.820988  4.204053  3.245181  \n",
       "38 -1.068318 -0.043001  0.165777  0.415194  1.520738  0.763440  1.142637  \n",
       "39 -1.365775 -0.539908 -0.701923 -1.246424  1.541746 -1.633240  0.699840  \n",
       "40 -1.191601  0.479994 -0.354897 -2.341587  2.054834  4.210462  2.977492  \n",
       "41  3.114799 -0.629854 -0.206511  0.839022 -1.660436  3.684899  2.298799  \n",
       "42  4.206109 -0.808283 -1.115461  0.401427 -1.353117  4.157353  2.480334  \n",
       "43  4.396338 -0.240891 -0.072191  0.430240 -0.784813  3.736512  2.162158  \n",
       "44 -0.919989  0.345378  0.975879 -0.130505  0.466096  0.689897  1.053322  \n",
       "\n",
       "[45 rows x 99 columns]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "screenshot[\"y_hat\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow draft code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add_2:0' shape=(?, 99) dtype=float32>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval_yhat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check out Pytorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-28 00:53:01.144094: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-28 00:53:01.346694: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-06-28 00:53:05.200251: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cm/shared/apps/lsf10/10.1/linux3.10-glibc2.17-x86_64/lib:/data/weirauchlab/opt/lib:/data/weirauchlab/opt/lib64:/data/weirauchlab/local/lib\n",
      "2023-06-28 00:53:05.200481: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cm/shared/apps/lsf10/10.1/linux3.10-glibc2.17-x86_64/lib:/data/weirauchlab/opt/lib:/data/weirauchlab/opt/lib64:/data/weirauchlab/local/lib\n",
      "2023-06-28 00:53:05.200509: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /users/ngun7t/anaconda3/envs/cellbox-3.6-2/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "================================================================================\n",
      "   _____     _ _ ____              \n",
      "  / ____|   | | |  _ \\             \n",
      " | |     ___| | | |_) | _____  __  \n",
      " | |    / _ \\ | |  _ < / _ \\ \\/ /  \n",
      " | |___|  __/ | | |_) | (_) >  <   \n",
      "  \\_____\\___|_|_|____/ \\___/_/\\_\\  \n",
      "Running CellBox scripts developed in Sander lab\n",
      "Maintained by Bo Yuan, Judy Shen, and Augustin Luna; contributions by Daniel Ritter\n",
      "\n",
      "        version 0.3.2\n",
      "        -- Feb 10, 2023 --\n",
      "        * Modify CellBox to support TF2     \n",
      "        \n",
      "Tutorials and documentations are available at https://github.com/sanderlab/CellBox\n",
      "If you want to discuss the usage or to report a bug, please use the 'Issues' function at GitHub.\n",
      "If you find CellBox useful for your research, please consider citing the corresponding publication.\n",
      "For more information, please email us at boyuan@g.harvard.edu and c_shen@g.harvard.edu, augustin_luna@hms.harvard.edu\n",
      " --------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import cellbox\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import argparse\n",
    "import json\n",
    "import glob\n",
    "import time\n",
    "from cellbox.utils import TimeLogger\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "from tensorflow.compat.v1.errors import OutOfRangeError\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'experiment_id': 'Example_RP', 'model_prefix': 'seed', 'ckpt_name': 'model11.ckpt', 'export_verbose': 3, 'experiment_type': 'random partition', 'sparse_data': False, 'batchsize': 4, 'trainset_ratio': 0.7, 'validset_ratio': 0.8, 'n_batches_eval': None, 'add_noise_level': 0, 'dT': 0.1, 'ode_solver': 'heun', 'envelope_form': 'tanh', 'envelope': 0, 'pert_form': 'by u', 'ode_degree': 1, 'ode_last_steps': 2, 'n_iter_buffer': 50, 'n_iter_patience': 100, 'weight_loss': 'None', 'l1lambda': 0.0001, 'l2lambda': 0.0001, 'model': 'CellBox', 'pert_file': '/users/ngun7t/Documents/cellbox-jun-6/data/pert.csv', 'expr_file': '/users/ngun7t/Documents/cellbox-jun-6/data/expr.csv', 'node_index_file': '/users/ngun7t/Documents/cellbox-jun-6/data/node_Index.csv', 'n_protein_nodes': 82, 'n_activity_nodes': 87, 'n_x': 99, 'envelop_form': 'tanh', 'envelop': 0, 'n_epoch': 100, 'n_iter': 100, 'stages': [{'nT': 200, 'sub_stages': [{'lr_val': 0.001, 'l1lambda': 0.0001}]}], 'ckpt_path_full': './model11.ckpt', 'drug_index': 5, 'seed': 1000}\n",
      "Working directory is ready at results/Example_RP_d2f1204ae85ed795160c7d3209f04b32.\n"
     ]
    }
   ],
   "source": [
    "def set_seed(in_seed):\n",
    "    int_seed = int(in_seed)\n",
    "    tf.compat.v1.set_random_seed(int_seed)\n",
    "    np.random.seed(int_seed)\n",
    "\n",
    "\n",
    "def prepare_workdir(in_cfg):\n",
    "    # Read Data\n",
    "    in_cfg.root_dir = os.getcwd()\n",
    "    in_cfg.node_index = pd.read_csv(in_cfg.node_index_file, header=None, names=None) \\\n",
    "        if hasattr(in_cfg, 'node_index_file') else pd.DataFrame(np.arange(in_cfg.n_x))\n",
    "\n",
    "    # Create Output Folder\n",
    "    experiment_path = 'results/{}_{}'.format(in_cfg.experiment_id, md5)\n",
    "    try:\n",
    "        os.makedirs(experiment_path)\n",
    "    except Exception:\n",
    "        pass\n",
    "    out_cfg = vars(in_cfg)\n",
    "    out_cfg = {key: out_cfg[key] for key in out_cfg if type(out_cfg[key]) is not pd.DataFrame}\n",
    "    os.chdir(experiment_path)\n",
    "    json.dump(out_cfg, open('config.json', 'w'), indent=4)\n",
    "\n",
    "    if \"leave one out\" in in_cfg.experiment_type:\n",
    "        try:\n",
    "            in_cfg.model_prefix = '{}_{}'.format(in_cfg.model_prefix, in_cfg.drug_index)\n",
    "        except Exception('Drug index not specified') as e:\n",
    "            raise e\n",
    "\n",
    "    in_cfg.working_index = in_cfg.model_prefix + \"_\" + str(working_index).zfill(3)\n",
    "\n",
    "    try:\n",
    "        shutil.rmtree(in_cfg.working_index)\n",
    "    except Exception:\n",
    "        pass\n",
    "    os.makedirs(in_cfg.working_index)\n",
    "    os.chdir(in_cfg.working_index)\n",
    "\n",
    "    with open(\"record_eval.csv\", 'w') as f:\n",
    "        f.write(\"epoch,iter,train_loss,valid_loss,train_mse,valid_mse,test_mse,time_elapsed\\n\")\n",
    "\n",
    "    print('Working directory is ready at {}.'.format(experiment_path))\n",
    "    return 0\n",
    "\n",
    "experiment_config_path = \"/users/ngun7t/Documents/cellbox-jun-6/configs_dev/Example.random_partition.CellBox.json\"\n",
    "working_index = 0\n",
    "stage = {\n",
    "    \"nT\": 100,\n",
    "    \"sub_stages\":[\n",
    "        {\"lr_val\": 0.1,\"l1lambda\": 0.01, \"n_iter_patience\":1000},\n",
    "        {\"lr_val\": 0.01,\"l1lambda\": 0.01},\n",
    "        {\"lr_val\": 0.01,\"l1lambda\": 0.0001},\n",
    "        {\"lr_val\": 0.001,\"l1lambda\": 0.00001}\n",
    "    ]}\n",
    "\n",
    "cfg = cellbox.config.Config(experiment_config_path)\n",
    "cfg.ckpt_path_full = os.path.join('./', cfg.ckpt_name)\n",
    "md5 = cellbox.utils.md5(cfg)\n",
    "cfg.drug_index = 5         # Change this for testing purposes\n",
    "cfg.seed = working_index + cfg.seed if hasattr(cfg, \"seed\") else working_index + 1000\n",
    "set_seed(cfg.seed)\n",
    "print(vars(cfg))\n",
    "\n",
    "prepare_workdir(cfg)\n",
    "logger = cellbox.utils.TimeLogger(time_logger_step=1, hierachy=3)\n",
    "args = cfg\n",
    "for i, stage in enumerate(cfg.stages):\n",
    "    set_seed(cfg.seed)\n",
    "    cfg = cellbox.dataset_torch.factory(cfg)\n",
    "    args.sub_stages = stage['sub_stages']\n",
    "    args.n_T = stage['nT']\n",
    "    model = cellbox.model_torch.factory(args)\n",
    "    if i == 0: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_substage(model, lr_val, l1_lambda, l2_lambda, n_epoch, n_iter, n_iter_buffer, n_iter_patience, args):\n",
    "\n",
    "    # Let's just assume that args contains also the loss function, dataloaders, and the optimizer\n",
    "    stages = glob.glob(\"*best*.csv\")\n",
    "    try:\n",
    "        substage_i = 1 + max([int(stage[0]) for stage in stages])\n",
    "    except Exception:\n",
    "        substage_i = 1\n",
    "\n",
    "    best_params = Screenshot(args, n_iter_buffer)\n",
    "\n",
    "    n_unchanged = 0\n",
    "    idx_iter = 0\n",
    "    #for key in args.feed_dicts:\n",
    "    #    args.feed_dicts[key].update({\n",
    "    #        model.lr: lr_val,\n",
    "    #        model.l1_lambda: l1_lambda,\n",
    "    #        model.l2_lambda: l2_lambda\n",
    "    #    })\n",
    "    args.logger.log(\"--------- lr: {}\\tl1: {}\\tl2: {}\\t\".format(lr_val, l1_lambda, l2_lambda))\n",
    "\n",
    "    #sess.run(model.iter_monitor.initializer, feed_dict=args.feed_dicts['valid_set'])\n",
    "    for idx_epoch in range(n_epoch):\n",
    "\n",
    "        if idx_iter > n_iter or n_unchanged > n_iter_patience:\n",
    "            break\n",
    "\n",
    "        for i, train_minibatch in enumerate(args.iter_train):\n",
    "            # Each train_minibatch has shape of (batch_size, num_features)\n",
    "            x_train, y_train = train_minibatch\n",
    "\n",
    "            if idx_iter > n_iter or n_unchanged > n_iter_patience:\n",
    "                break\n",
    "\n",
    "            # Do one forward pass\n",
    "            t0 = time.perf_counter()\n",
    "            args.optimizer.zero_grad()\n",
    "            prediction = model(None, x_train.to(args.device))\n",
    "            loss_train_i, loss_train_mse_i = args.loss_fn(y_train.to(args.device), prediction, model.W.weight)\n",
    "            loss_train_i.backward()\n",
    "            args.optimizer.step()\n",
    "\n",
    "            # Record validation results\n",
    "            with torch.no_grad():\n",
    "                # Very questionable for validation\n",
    "                loss_valid, loss_valid_mse = 0, 0\n",
    "                valid_minibatch = iter(args.iter_monitor)\n",
    "                x_valid, y_valid = next(valid_minibatch)\n",
    "                loss_valid_i, loss_valid_mse_i = args.loss_fn(y_valid.to(args.device), model(None, x_valid.to(args.device)), model.W.weight)\n",
    "\n",
    "            # Record results to screenshot\n",
    "            new_loss = best_params.avg_n_iters_loss(loss_valid_i)\n",
    "            if args.export_verbose > 0:\n",
    "                print((\"Substage:{}\\tEpoch:{}/{}\\tIteration: {}/{}\" + \"\\tloss (train):{:1.6f}\\tloss (buffer on valid):\"\n",
    "                       \"{:1.6f}\" + \"\\tbest:{:1.6f}\\tTolerance: {}/{}\").format(substage_i, idx_epoch, n_epoch, idx_iter,\n",
    "                                                                              n_iter, loss_train_i, new_loss,\n",
    "                                                                              best_params.loss_min, n_unchanged,\n",
    "                                                                              n_iter_patience))\n",
    "            \n",
    "            append_record(\"record_eval.csv\",\n",
    "                          [idx_epoch, idx_iter, loss_train_i.item(), loss_valid_i.item(), loss_train_mse_i.item(),\n",
    "                           loss_valid_mse_i.item(), None, time.perf_counter() - t0])\n",
    "\n",
    "            # Early stopping\n",
    "            idx_iter += 1\n",
    "            if new_loss < best_params.loss_min:\n",
    "                n_unchanged = 0\n",
    "                best_params.screenshot(model, substage_i, args=args,\n",
    "                                       node_index=args.dataset['node_index'], loss_min=new_loss)\n",
    "            else:\n",
    "                n_unchanged += 1\n",
    "\n",
    "\n",
    "    #best_params.save()\n",
    "    args.logger.log(\"------------------ Substage {} finished!-------------------\".format(substage_i))\n",
    "    #save_model(args.saver, sess, './' + args.ckpt_name)\n",
    "\n",
    "    return best_params\n",
    "\n",
    "\n",
    "def append_record(filename, contents):\n",
    "    \"\"\"define function for appending training record\"\"\"\n",
    "    with open(filename, 'a') as f:\n",
    "        for content in contents:\n",
    "            f.write('{},'.format(content))\n",
    "        f.write('\\n')\n",
    "\n",
    "\n",
    "def eval_model(device, eval_iter, model, return_avg=True, n_batches_eval=None):\n",
    "    \"\"\" Simulate the model for prediction \"\"\"\n",
    "\n",
    "    with torch.no_grad():\n",
    "        counter = 0\n",
    "        eval_results = []\n",
    "        for item in eval_iter:\n",
    "            pert, expr = item\n",
    "            pred = model(None, pert.to(device))\n",
    "            eval_results.append(pred.detach().cpu().numpy())\n",
    "            counter += 1\n",
    "            if n_batches_eval is not None and counter > n_batches_eval:\n",
    "                break\n",
    "\n",
    "        if return_avg:\n",
    "            return np.mean(np.array(eval_results), axis=0)\n",
    "        return np.vstack(eval_results)\n",
    "\n",
    "\n",
    "def train_model(model, args):\n",
    "    \"\"\"Train the model\"\"\"\n",
    "    args.logger = TimeLogger(time_logger_step=1, hierachy=2)\n",
    "    model = model[0].to(args.device)\n",
    "\n",
    "    # Check if all variables in scope\n",
    "    # TODO: put variables under appropriate scopes\n",
    "    #try:\n",
    "    #    args.saver.restore(sess, './' + args.ckpt_name)\n",
    "    #    print('Load existing model at {}...'.format(args.ckpt_name))\n",
    "    #except Exception:\n",
    "    #    print('Create new model at {}...'.format(args.ckpt_name))\n",
    "\n",
    "    # Training\n",
    "    for substage in args.sub_stages:\n",
    "        n_iter_buffer = substage['n_iter_buffer'] if 'n_iter_buffer' in substage else args.n_iter_buffer\n",
    "        n_iter = substage['n_iter'] if 'n_iter' in substage else args.n_iter\n",
    "        n_iter_patience = substage['n_iter_patience'] if 'n_iter_patience' in substage else args.n_iter_patience\n",
    "        n_epoch = substage['n_epoch'] if 'n_epoch' in substage else args.n_epoch\n",
    "        l1 = substage['l1lambda'] if 'l1lambda' in substage else args.l1lambda if hasattr(args, 'l1lambda') else 0\n",
    "        l2 = substage['l2lambda'] if 'l2lambda' in substage else args.l2lambda if hasattr(args, 'l2lambda') else 0\n",
    "        screenshot = train_substage(model, substage['lr_val'], l1_lambda=l1, l2_lambda=l2, n_epoch=n_epoch,\n",
    "                       n_iter=n_iter, n_iter_buffer=n_iter_buffer, n_iter_patience=n_iter_patience, args=args)\n",
    "\n",
    "    return screenshot\n",
    "        \n",
    "\n",
    "class Screenshot(dict):\n",
    "    \"\"\"summarize the model\"\"\"\n",
    "    def __init__(self, args, n_iter_buffer):\n",
    "        # initialize loss_min\n",
    "        super().__init__()\n",
    "        self.loss_min = 1000\n",
    "        # initialize tuning_metric\n",
    "        self.saved_losses = [self.loss_min]\n",
    "        self.n_iter_buffer = n_iter_buffer\n",
    "        # initialize verbose\n",
    "        self.summary = {}\n",
    "        self.summary = {}\n",
    "        self.substage_i = []\n",
    "        self.export_verbose = args.export_verbose\n",
    "\n",
    "    def avg_n_iters_loss(self, new_loss):\n",
    "        \"\"\"average the last few losses\"\"\"\n",
    "        self.saved_losses = self.saved_losses + [new_loss]\n",
    "        self.saved_losses = self.saved_losses[-self.n_iter_buffer:]\n",
    "        return sum(self.saved_losses) / len(self.saved_losses)\n",
    "\n",
    "    def screenshot(self, model, substage_i, node_index, loss_min, args):\n",
    "        \"\"\"evaluate models\"\"\"\n",
    "        self.substage_i = substage_i\n",
    "        self.loss_min = loss_min\n",
    "\n",
    "        # Save the variable weights associated with each of the conditions in a csv file\n",
    "        if self.export_verbose > 0:\n",
    "            layer = model.W\n",
    "            params = layer.state_dict()\n",
    "            new_params = {}\n",
    "            for item in params:\n",
    "                try:\n",
    "                    new_params[item] = pd.DataFrame(params[item].numpy(), index=node_index[0])\n",
    "                except Exception:\n",
    "                    new_params[item] = pd.DataFrame(params[item].numpy())\n",
    "            self.update(params)\n",
    "\n",
    "        if self.export_verbose > 1 or self.export_verbose == -1:  # no params but y_hat\n",
    "            y_hat = eval_model(args.device, args.iter_eval, model, return_avg=False)\n",
    "            y_hat = pd.DataFrame(y_hat, columns=node_index[0])\n",
    "            self.update({'y_hat': y_hat})\n",
    "\n",
    "        if self.export_verbose > 2:\n",
    "            try:\n",
    "                # TODO: not yet support data iterators\n",
    "                summary_train = sess.run(model.convergence_metric,\n",
    "                                         feed_dict={model.in_pert: args.dataset['pert_train']})\n",
    "                summary_test = sess.run(model.convergence_metric, feed_dict={model.in_pert: args.dataset['pert_test']})\n",
    "                summary_valid = sess.run(model.convergence_metric,\n",
    "                                         feed_dict={model.in_pert: args.dataset['pert_valid']})\n",
    "                summary_train = pd.DataFrame(summary_train, columns=[node_index.values + '_mean', node_index.values +\n",
    "                                                                     '_sd', node_index.values + '_dxdt'])\n",
    "                summary_test = pd.DataFrame(summary_test, columns=[node_index.values + '_mean', node_index.values +\n",
    "                                                                   '_sd', node_index.values + '_dxdt'])\n",
    "                summary_valid = pd.DataFrame(summary_valid, columns=[node_index.values + '_mean', node_index.values +\n",
    "                                                                     '_sd', node_index.values + '_dxdt'])\n",
    "                self.update(\n",
    "                    {'summary_train': summary_train, 'summary_test': summary_test, 'summary_valid': summary_valid}\n",
    "                )\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    def save(self):\n",
    "        \"\"\"save model parameters\"\"\"\n",
    "        for file in glob.glob(str(self.substage_i) + \"_best.*.csv\"):\n",
    "            os.remove(file)\n",
    "        for key in self:\n",
    "            self[key].to_csv(\"{}_best.{}.loss.{}.csv\".format(self.substage_i, key, self.loss_min))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########   --------- lr: 0.001\tl1: 0.0001\tl2: 0.0001\t   --time elapsed: 0.00\n",
      "Substage:1\tEpoch:0/100\tIteration: 0/100\tloss (train):0.135471\tloss (buffer on valid):500.177246\tbest:1000.000000\tTolerance: 0/100\n",
      "Substage:1\tEpoch:0/100\tIteration: 1/100\tloss (train):0.096996\tloss (buffer on valid):333.517914\tbest:500.177246\tTolerance: 0/100\n",
      "Substage:1\tEpoch:0/100\tIteration: 2/100\tloss (train):0.224137\tloss (buffer on valid):250.201248\tbest:333.517914\tTolerance: 0/100\n",
      "Substage:1\tEpoch:0/100\tIteration: 3/100\tloss (train):0.383463\tloss (buffer on valid):200.196426\tbest:250.201248\tTolerance: 0/100\n",
      "Substage:1\tEpoch:0/100\tIteration: 4/100\tloss (train):0.228764\tloss (buffer on valid):166.866867\tbest:200.196426\tTolerance: 0/100\n",
      "Substage:1\tEpoch:0/100\tIteration: 5/100\tloss (train):0.269860\tloss (buffer on valid):143.060318\tbest:166.866867\tTolerance: 0/100\n",
      "Substage:1\tEpoch:0/100\tIteration: 6/100\tloss (train):0.286096\tloss (buffer on valid):125.197006\tbest:143.060318\tTolerance: 0/100\n",
      "Substage:1\tEpoch:0/100\tIteration: 7/100\tloss (train):0.158426\tloss (buffer on valid):111.310295\tbest:125.197006\tTolerance: 0/100\n",
      "Substage:1\tEpoch:0/100\tIteration: 8/100\tloss (train):0.127044\tloss (buffer on valid):100.198669\tbest:111.310295\tTolerance: 0/100\n",
      "Substage:1\tEpoch:0/100\tIteration: 9/100\tloss (train):0.260002\tloss (buffer on valid):91.105011\tbest:100.198669\tTolerance: 0/100\n",
      "Substage:1\tEpoch:0/100\tIteration: 10/100\tloss (train):0.169351\tloss (buffer on valid):83.532982\tbest:91.105011\tTolerance: 0/100\n",
      "Substage:1\tEpoch:0/100\tIteration: 11/100\tloss (train):0.100415\tloss (buffer on valid):77.122231\tbest:83.532982\tTolerance: 0/100\n",
      "Substage:1\tEpoch:0/100\tIteration: 12/100\tloss (train):0.407176\tloss (buffer on valid):71.636925\tbest:77.122231\tTolerance: 0/100\n",
      "Substage:1\tEpoch:1/100\tIteration: 13/100\tloss (train):0.336498\tloss (buffer on valid):66.874802\tbest:71.636925\tTolerance: 0/100\n",
      "Substage:1\tEpoch:1/100\tIteration: 14/100\tloss (train):0.173141\tloss (buffer on valid):62.711926\tbest:66.874802\tTolerance: 0/100\n",
      "Substage:1\tEpoch:1/100\tIteration: 15/100\tloss (train):0.199548\tloss (buffer on valid):59.032555\tbest:62.711926\tTolerance: 0/100\n",
      "Substage:1\tEpoch:1/100\tIteration: 16/100\tloss (train):0.218105\tloss (buffer on valid):55.766132\tbest:59.032555\tTolerance: 0/100\n",
      "Substage:1\tEpoch:1/100\tIteration: 17/100\tloss (train):0.142049\tloss (buffer on valid):52.841015\tbest:55.766132\tTolerance: 0/100\n",
      "Substage:1\tEpoch:1/100\tIteration: 18/100\tloss (train):0.266355\tloss (buffer on valid):50.216900\tbest:52.841015\tTolerance: 0/100\n",
      "Substage:1\tEpoch:1/100\tIteration: 19/100\tloss (train):0.180828\tloss (buffer on valid):47.834557\tbest:50.216900\tTolerance: 0/100\n",
      "Substage:1\tEpoch:1/100\tIteration: 20/100\tloss (train):0.227304\tloss (buffer on valid):45.675385\tbest:47.834557\tTolerance: 0/100\n",
      "Substage:1\tEpoch:1/100\tIteration: 21/100\tloss (train):0.179613\tloss (buffer on valid):43.699512\tbest:45.675385\tTolerance: 0/100\n",
      "Substage:1\tEpoch:1/100\tIteration: 22/100\tloss (train):0.258406\tloss (buffer on valid):41.887779\tbest:43.699512\tTolerance: 0/100\n",
      "Substage:1\tEpoch:1/100\tIteration: 23/100\tloss (train):0.153609\tloss (buffer on valid):40.219109\tbest:41.887779\tTolerance: 0/100\n",
      "Substage:1\tEpoch:1/100\tIteration: 24/100\tloss (train):0.140919\tloss (buffer on valid):38.679600\tbest:40.219109\tTolerance: 0/100\n",
      "Substage:1\tEpoch:1/100\tIteration: 25/100\tloss (train):0.261765\tloss (buffer on valid):37.260017\tbest:38.679600\tTolerance: 0/100\n",
      "Substage:1\tEpoch:2/100\tIteration: 26/100\tloss (train):0.183677\tloss (buffer on valid):35.938705\tbest:37.260017\tTolerance: 0/100\n",
      "Substage:1\tEpoch:2/100\tIteration: 27/100\tloss (train):0.180235\tloss (buffer on valid):34.709248\tbest:35.938705\tTolerance: 0/100\n",
      "Substage:1\tEpoch:2/100\tIteration: 28/100\tloss (train):0.241558\tloss (buffer on valid):33.561260\tbest:34.709248\tTolerance: 0/100\n",
      "Substage:1\tEpoch:2/100\tIteration: 29/100\tloss (train):0.262184\tloss (buffer on valid):32.485512\tbest:33.561260\tTolerance: 0/100\n",
      "Substage:1\tEpoch:2/100\tIteration: 30/100\tloss (train):0.259403\tloss (buffer on valid):31.480131\tbest:32.485512\tTolerance: 0/100\n",
      "Substage:1\tEpoch:2/100\tIteration: 31/100\tloss (train):0.215685\tloss (buffer on valid):30.530535\tbest:31.480131\tTolerance: 0/100\n",
      "Substage:1\tEpoch:2/100\tIteration: 32/100\tloss (train):0.252031\tloss (buffer on valid):29.640106\tbest:30.530535\tTolerance: 0/100\n",
      "Substage:1\tEpoch:2/100\tIteration: 33/100\tloss (train):0.157210\tloss (buffer on valid):28.800545\tbest:29.640106\tTolerance: 0/100\n",
      "Substage:1\tEpoch:2/100\tIteration: 34/100\tloss (train):0.149344\tloss (buffer on valid):28.004036\tbest:28.800545\tTolerance: 0/100\n",
      "Substage:1\tEpoch:2/100\tIteration: 35/100\tloss (train):0.128977\tloss (buffer on valid):27.250479\tbest:28.004036\tTolerance: 0/100\n",
      "Substage:1\tEpoch:2/100\tIteration: 36/100\tloss (train):0.201529\tloss (buffer on valid):26.541941\tbest:27.250479\tTolerance: 0/100\n",
      "Substage:1\tEpoch:2/100\tIteration: 37/100\tloss (train):0.285324\tloss (buffer on valid):25.869621\tbest:26.541941\tTolerance: 0/100\n",
      "Substage:1\tEpoch:2/100\tIteration: 38/100\tloss (train):0.098645\tloss (buffer on valid):25.228901\tbest:25.869621\tTolerance: 0/100\n",
      "Substage:1\tEpoch:3/100\tIteration: 39/100\tloss (train):0.136384\tloss (buffer on valid):24.616879\tbest:25.228901\tTolerance: 0/100\n",
      "Substage:1\tEpoch:3/100\tIteration: 40/100\tloss (train):0.399126\tloss (buffer on valid):24.035448\tbest:24.616879\tTolerance: 0/100\n",
      "Substage:1\tEpoch:3/100\tIteration: 41/100\tloss (train):0.170705\tloss (buffer on valid):23.481062\tbest:24.035448\tTolerance: 0/100\n",
      "Substage:1\tEpoch:3/100\tIteration: 42/100\tloss (train):0.147834\tloss (buffer on valid):22.953493\tbest:23.481062\tTolerance: 0/100\n",
      "Substage:1\tEpoch:3/100\tIteration: 43/100\tloss (train):0.212837\tloss (buffer on valid):22.446758\tbest:22.953493\tTolerance: 0/100\n",
      "Substage:1\tEpoch:3/100\tIteration: 44/100\tloss (train):0.282531\tloss (buffer on valid):21.966394\tbest:22.446758\tTolerance: 0/100\n",
      "Substage:1\tEpoch:3/100\tIteration: 45/100\tloss (train):0.096931\tloss (buffer on valid):21.503727\tbest:21.966394\tTolerance: 0/100\n",
      "Substage:1\tEpoch:3/100\tIteration: 46/100\tloss (train):0.114808\tloss (buffer on valid):21.061285\tbest:21.503727\tTolerance: 0/100\n",
      "Substage:1\tEpoch:3/100\tIteration: 47/100\tloss (train):0.112443\tloss (buffer on valid):20.634373\tbest:21.061285\tTolerance: 0/100\n",
      "Substage:1\tEpoch:3/100\tIteration: 48/100\tloss (train):0.276488\tloss (buffer on valid):20.226572\tbest:20.634373\tTolerance: 0/100\n",
      "Substage:1\tEpoch:3/100\tIteration: 49/100\tloss (train):0.291675\tloss (buffer on valid):0.229330\tbest:20.226572\tTolerance: 0/100\n",
      "Substage:1\tEpoch:3/100\tIteration: 50/100\tloss (train):0.170925\tloss (buffer on valid):0.227165\tbest:0.229330\tTolerance: 0/100\n",
      "Substage:1\tEpoch:3/100\tIteration: 51/100\tloss (train):0.516519\tloss (buffer on valid):0.229541\tbest:0.227165\tTolerance: 0/100\n",
      "Substage:1\tEpoch:4/100\tIteration: 52/100\tloss (train):0.193632\tloss (buffer on valid):0.229016\tbest:0.227165\tTolerance: 1/100\n",
      "Substage:1\tEpoch:4/100\tIteration: 53/100\tloss (train):0.267313\tloss (buffer on valid):0.229196\tbest:0.227165\tTolerance: 2/100\n",
      "Substage:1\tEpoch:4/100\tIteration: 54/100\tloss (train):0.206012\tloss (buffer on valid):0.226480\tbest:0.227165\tTolerance: 3/100\n",
      "Substage:1\tEpoch:4/100\tIteration: 55/100\tloss (train):0.249984\tloss (buffer on valid):0.228146\tbest:0.226480\tTolerance: 0/100\n",
      "Substage:1\tEpoch:4/100\tIteration: 56/100\tloss (train):0.276382\tloss (buffer on valid):0.228430\tbest:0.226480\tTolerance: 1/100\n",
      "Substage:1\tEpoch:4/100\tIteration: 57/100\tloss (train):0.142549\tloss (buffer on valid):0.227166\tbest:0.226480\tTolerance: 2/100\n",
      "Substage:1\tEpoch:4/100\tIteration: 58/100\tloss (train):0.047372\tloss (buffer on valid):0.228394\tbest:0.226480\tTolerance: 3/100\n",
      "Substage:1\tEpoch:4/100\tIteration: 59/100\tloss (train):0.166547\tloss (buffer on valid):0.228826\tbest:0.226480\tTolerance: 4/100\n",
      "Substage:1\tEpoch:4/100\tIteration: 60/100\tloss (train):0.342208\tloss (buffer on valid):0.229962\tbest:0.226480\tTolerance: 5/100\n",
      "Substage:1\tEpoch:4/100\tIteration: 61/100\tloss (train):0.212493\tloss (buffer on valid):0.233071\tbest:0.226480\tTolerance: 6/100\n",
      "Substage:1\tEpoch:4/100\tIteration: 62/100\tloss (train):0.160024\tloss (buffer on valid):0.230090\tbest:0.226480\tTolerance: 7/100\n",
      "Substage:1\tEpoch:4/100\tIteration: 63/100\tloss (train):0.202640\tloss (buffer on valid):0.229056\tbest:0.226480\tTolerance: 8/100\n",
      "Substage:1\tEpoch:4/100\tIteration: 64/100\tloss (train):0.298644\tloss (buffer on valid):0.228494\tbest:0.226480\tTolerance: 9/100\n",
      "Substage:1\tEpoch:5/100\tIteration: 65/100\tloss (train):0.152922\tloss (buffer on valid):0.230570\tbest:0.226480\tTolerance: 10/100\n",
      "Substage:1\tEpoch:5/100\tIteration: 66/100\tloss (train):0.114993\tloss (buffer on valid):0.229681\tbest:0.226480\tTolerance: 11/100\n",
      "Substage:1\tEpoch:5/100\tIteration: 67/100\tloss (train):0.208834\tloss (buffer on valid):0.230843\tbest:0.226480\tTolerance: 12/100\n",
      "Substage:1\tEpoch:5/100\tIteration: 68/100\tloss (train):0.180399\tloss (buffer on valid):0.228627\tbest:0.226480\tTolerance: 13/100\n",
      "Substage:1\tEpoch:5/100\tIteration: 69/100\tloss (train):0.154376\tloss (buffer on valid):0.231429\tbest:0.226480\tTolerance: 14/100\n",
      "Substage:1\tEpoch:5/100\tIteration: 70/100\tloss (train):0.267961\tloss (buffer on valid):0.228266\tbest:0.226480\tTolerance: 15/100\n",
      "Substage:1\tEpoch:5/100\tIteration: 71/100\tloss (train):0.218234\tloss (buffer on valid):0.228747\tbest:0.226480\tTolerance: 16/100\n",
      "Substage:1\tEpoch:5/100\tIteration: 72/100\tloss (train):0.148033\tloss (buffer on valid):0.229312\tbest:0.226480\tTolerance: 17/100\n",
      "Substage:1\tEpoch:5/100\tIteration: 73/100\tloss (train):0.164490\tloss (buffer on valid):0.230401\tbest:0.226480\tTolerance: 18/100\n",
      "Substage:1\tEpoch:5/100\tIteration: 74/100\tloss (train):0.164120\tloss (buffer on valid):0.229753\tbest:0.226480\tTolerance: 19/100\n",
      "Substage:1\tEpoch:5/100\tIteration: 75/100\tloss (train):0.317576\tloss (buffer on valid):0.226605\tbest:0.226480\tTolerance: 20/100\n",
      "Substage:1\tEpoch:5/100\tIteration: 76/100\tloss (train):0.365715\tloss (buffer on valid):0.223918\tbest:0.226480\tTolerance: 21/100\n",
      "Substage:1\tEpoch:5/100\tIteration: 77/100\tloss (train):0.336659\tloss (buffer on valid):0.220590\tbest:0.223918\tTolerance: 0/100\n",
      "Substage:1\tEpoch:6/100\tIteration: 78/100\tloss (train):0.180068\tloss (buffer on valid):0.218525\tbest:0.220590\tTolerance: 0/100\n",
      "Substage:1\tEpoch:6/100\tIteration: 79/100\tloss (train):0.298073\tloss (buffer on valid):0.219211\tbest:0.218525\tTolerance: 0/100\n",
      "Substage:1\tEpoch:6/100\tIteration: 80/100\tloss (train):0.136480\tloss (buffer on valid):0.218354\tbest:0.218525\tTolerance: 1/100\n",
      "Substage:1\tEpoch:6/100\tIteration: 81/100\tloss (train):0.293956\tloss (buffer on valid):0.219940\tbest:0.218354\tTolerance: 0/100\n",
      "Substage:1\tEpoch:6/100\tIteration: 82/100\tloss (train):0.087258\tloss (buffer on valid):0.218180\tbest:0.218354\tTolerance: 1/100\n",
      "Substage:1\tEpoch:6/100\tIteration: 83/100\tloss (train):0.087474\tloss (buffer on valid):0.216149\tbest:0.218180\tTolerance: 0/100\n",
      "Substage:1\tEpoch:6/100\tIteration: 84/100\tloss (train):0.231693\tloss (buffer on valid):0.216766\tbest:0.216149\tTolerance: 0/100\n",
      "Substage:1\tEpoch:6/100\tIteration: 85/100\tloss (train):0.179196\tloss (buffer on valid):0.218482\tbest:0.216149\tTolerance: 1/100\n",
      "Substage:1\tEpoch:6/100\tIteration: 86/100\tloss (train):0.317133\tloss (buffer on valid):0.216659\tbest:0.216149\tTolerance: 2/100\n",
      "Substage:1\tEpoch:6/100\tIteration: 87/100\tloss (train):0.309243\tloss (buffer on valid):0.214120\tbest:0.216149\tTolerance: 3/100\n",
      "Substage:1\tEpoch:6/100\tIteration: 88/100\tloss (train):0.258566\tloss (buffer on valid):0.214050\tbest:0.214120\tTolerance: 0/100\n",
      "Substage:1\tEpoch:6/100\tIteration: 89/100\tloss (train):0.124106\tloss (buffer on valid):0.213951\tbest:0.214050\tTolerance: 0/100\n",
      "Substage:1\tEpoch:6/100\tIteration: 90/100\tloss (train):0.154280\tloss (buffer on valid):0.213285\tbest:0.213951\tTolerance: 0/100\n",
      "Substage:1\tEpoch:7/100\tIteration: 91/100\tloss (train):0.175451\tloss (buffer on valid):0.214955\tbest:0.213285\tTolerance: 0/100\n",
      "Substage:1\tEpoch:7/100\tIteration: 92/100\tloss (train):0.168932\tloss (buffer on valid):0.213005\tbest:0.213285\tTolerance: 1/100\n",
      "Substage:1\tEpoch:7/100\tIteration: 93/100\tloss (train):0.210170\tloss (buffer on valid):0.213709\tbest:0.213005\tTolerance: 0/100\n",
      "Substage:1\tEpoch:7/100\tIteration: 94/100\tloss (train):0.285759\tloss (buffer on valid):0.212842\tbest:0.213005\tTolerance: 1/100\n",
      "Substage:1\tEpoch:7/100\tIteration: 95/100\tloss (train):0.183460\tloss (buffer on valid):0.213289\tbest:0.212842\tTolerance: 0/100\n",
      "Substage:1\tEpoch:7/100\tIteration: 96/100\tloss (train):0.151965\tloss (buffer on valid):0.213742\tbest:0.212842\tTolerance: 1/100\n",
      "Substage:1\tEpoch:7/100\tIteration: 97/100\tloss (train):0.267145\tloss (buffer on valid):0.215390\tbest:0.212842\tTolerance: 2/100\n",
      "Substage:1\tEpoch:7/100\tIteration: 98/100\tloss (train):0.106375\tloss (buffer on valid):0.214327\tbest:0.212842\tTolerance: 3/100\n",
      "Substage:1\tEpoch:7/100\tIteration: 99/100\tloss (train):0.157291\tloss (buffer on valid):0.217139\tbest:0.212842\tTolerance: 4/100\n",
      "Substage:1\tEpoch:7/100\tIteration: 100/100\tloss (train):0.166038\tloss (buffer on valid):0.218033\tbest:0.212842\tTolerance: 5/100\n",
      "########   ------------------ Substage 1 finished!-------------------   --time elapsed: 6.59\n"
     ]
    }
   ],
   "source": [
    "screenshot = train_model(model, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinReg(\n",
       "  (W): Linear(in_features=99, out_features=99, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99, 99)\n",
      "(99,)\n"
     ]
    }
   ],
   "source": [
    "a = model[0].W.state_dict()\n",
    "for item in a:\n",
    "    print(a[item].numpy().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['weight', 'bias', 'y_hat'])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "screenshot.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>4EBP1pS65</th>\n",
       "      <th>RbpS807</th>\n",
       "      <th>MAPKpT202</th>\n",
       "      <th>MEKpS217</th>\n",
       "      <th>S6</th>\n",
       "      <th>PAI-1</th>\n",
       "      <th>AKTpS473</th>\n",
       "      <th>AMPKpT172</th>\n",
       "      <th>b-Catenin</th>\n",
       "      <th>BIM</th>\n",
       "      <th>...</th>\n",
       "      <th>aHDAC</th>\n",
       "      <th>aMDM2</th>\n",
       "      <th>aJAK</th>\n",
       "      <th>aBRAFm</th>\n",
       "      <th>aPKC</th>\n",
       "      <th>aSTAT3</th>\n",
       "      <th>amTOR</th>\n",
       "      <th>aPI3K</th>\n",
       "      <th>aCDK4</th>\n",
       "      <th>aSRC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.034876</td>\n",
       "      <td>-0.183665</td>\n",
       "      <td>0.143622</td>\n",
       "      <td>-0.082877</td>\n",
       "      <td>0.001758</td>\n",
       "      <td>-0.003982</td>\n",
       "      <td>0.003231</td>\n",
       "      <td>0.034118</td>\n",
       "      <td>0.093920</td>\n",
       "      <td>0.200523</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.122743</td>\n",
       "      <td>-0.030154</td>\n",
       "      <td>-0.080675</td>\n",
       "      <td>-0.072349</td>\n",
       "      <td>0.040038</td>\n",
       "      <td>-0.025492</td>\n",
       "      <td>0.180109</td>\n",
       "      <td>0.051663</td>\n",
       "      <td>0.151754</td>\n",
       "      <td>-0.064242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.294122</td>\n",
       "      <td>-0.093160</td>\n",
       "      <td>0.246818</td>\n",
       "      <td>-0.189556</td>\n",
       "      <td>-0.158506</td>\n",
       "      <td>0.135892</td>\n",
       "      <td>0.134888</td>\n",
       "      <td>0.073156</td>\n",
       "      <td>-0.123043</td>\n",
       "      <td>0.012068</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085567</td>\n",
       "      <td>-0.244929</td>\n",
       "      <td>-0.160039</td>\n",
       "      <td>0.101084</td>\n",
       "      <td>-0.258373</td>\n",
       "      <td>0.125785</td>\n",
       "      <td>0.151400</td>\n",
       "      <td>-0.253566</td>\n",
       "      <td>0.086212</td>\n",
       "      <td>0.281912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.080961</td>\n",
       "      <td>0.007904</td>\n",
       "      <td>0.031583</td>\n",
       "      <td>-0.107364</td>\n",
       "      <td>0.011465</td>\n",
       "      <td>0.017704</td>\n",
       "      <td>-0.015875</td>\n",
       "      <td>-0.095701</td>\n",
       "      <td>0.049683</td>\n",
       "      <td>0.073469</td>\n",
       "      <td>...</td>\n",
       "      <td>0.104831</td>\n",
       "      <td>-0.120854</td>\n",
       "      <td>-0.177551</td>\n",
       "      <td>-0.053515</td>\n",
       "      <td>-0.108378</td>\n",
       "      <td>0.036214</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>-0.080562</td>\n",
       "      <td>0.007598</td>\n",
       "      <td>0.000651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.097761</td>\n",
       "      <td>0.064553</td>\n",
       "      <td>0.112396</td>\n",
       "      <td>-0.099148</td>\n",
       "      <td>0.078064</td>\n",
       "      <td>0.081907</td>\n",
       "      <td>-0.059240</td>\n",
       "      <td>-0.025711</td>\n",
       "      <td>0.138789</td>\n",
       "      <td>0.032307</td>\n",
       "      <td>...</td>\n",
       "      <td>0.170786</td>\n",
       "      <td>-0.126457</td>\n",
       "      <td>-0.184178</td>\n",
       "      <td>-0.106063</td>\n",
       "      <td>-0.071826</td>\n",
       "      <td>-0.018416</td>\n",
       "      <td>0.020251</td>\n",
       "      <td>-0.095692</td>\n",
       "      <td>-0.029040</td>\n",
       "      <td>0.025155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.066278</td>\n",
       "      <td>-0.015814</td>\n",
       "      <td>0.045784</td>\n",
       "      <td>-0.067259</td>\n",
       "      <td>0.013513</td>\n",
       "      <td>0.027357</td>\n",
       "      <td>-0.000470</td>\n",
       "      <td>-0.074069</td>\n",
       "      <td>0.052443</td>\n",
       "      <td>0.112367</td>\n",
       "      <td>...</td>\n",
       "      <td>0.098900</td>\n",
       "      <td>-0.094037</td>\n",
       "      <td>-0.082585</td>\n",
       "      <td>-0.008271</td>\n",
       "      <td>-0.105168</td>\n",
       "      <td>-0.003399</td>\n",
       "      <td>0.054829</td>\n",
       "      <td>-0.021752</td>\n",
       "      <td>0.056861</td>\n",
       "      <td>0.055012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.088922</td>\n",
       "      <td>0.045411</td>\n",
       "      <td>-0.218302</td>\n",
       "      <td>-0.342581</td>\n",
       "      <td>-0.189004</td>\n",
       "      <td>0.107046</td>\n",
       "      <td>0.031690</td>\n",
       "      <td>-0.262903</td>\n",
       "      <td>0.130680</td>\n",
       "      <td>-0.132162</td>\n",
       "      <td>...</td>\n",
       "      <td>0.090093</td>\n",
       "      <td>-0.297775</td>\n",
       "      <td>-0.152631</td>\n",
       "      <td>-0.134203</td>\n",
       "      <td>0.102528</td>\n",
       "      <td>0.238491</td>\n",
       "      <td>-0.151672</td>\n",
       "      <td>0.062936</td>\n",
       "      <td>0.141745</td>\n",
       "      <td>-0.143992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.039349</td>\n",
       "      <td>-0.044991</td>\n",
       "      <td>0.051317</td>\n",
       "      <td>-0.078153</td>\n",
       "      <td>0.107723</td>\n",
       "      <td>0.101997</td>\n",
       "      <td>0.064801</td>\n",
       "      <td>-0.048378</td>\n",
       "      <td>0.015586</td>\n",
       "      <td>0.130743</td>\n",
       "      <td>...</td>\n",
       "      <td>0.092250</td>\n",
       "      <td>-0.085436</td>\n",
       "      <td>-0.153277</td>\n",
       "      <td>-0.012049</td>\n",
       "      <td>-0.101245</td>\n",
       "      <td>-0.067190</td>\n",
       "      <td>0.156362</td>\n",
       "      <td>-0.049721</td>\n",
       "      <td>0.134039</td>\n",
       "      <td>0.150682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.028227</td>\n",
       "      <td>-0.041266</td>\n",
       "      <td>0.079579</td>\n",
       "      <td>-0.081062</td>\n",
       "      <td>0.106948</td>\n",
       "      <td>0.074448</td>\n",
       "      <td>0.086504</td>\n",
       "      <td>-0.073258</td>\n",
       "      <td>0.043288</td>\n",
       "      <td>0.161920</td>\n",
       "      <td>...</td>\n",
       "      <td>0.094312</td>\n",
       "      <td>-0.085749</td>\n",
       "      <td>-0.142496</td>\n",
       "      <td>0.019146</td>\n",
       "      <td>-0.127461</td>\n",
       "      <td>-0.077492</td>\n",
       "      <td>0.135330</td>\n",
       "      <td>-0.030512</td>\n",
       "      <td>0.151584</td>\n",
       "      <td>0.113620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.038961</td>\n",
       "      <td>0.014991</td>\n",
       "      <td>0.065171</td>\n",
       "      <td>-0.075859</td>\n",
       "      <td>0.053893</td>\n",
       "      <td>0.046696</td>\n",
       "      <td>-0.041396</td>\n",
       "      <td>-0.082587</td>\n",
       "      <td>0.085177</td>\n",
       "      <td>0.041151</td>\n",
       "      <td>...</td>\n",
       "      <td>0.144618</td>\n",
       "      <td>-0.110327</td>\n",
       "      <td>-0.143367</td>\n",
       "      <td>-0.057226</td>\n",
       "      <td>-0.100280</td>\n",
       "      <td>0.002345</td>\n",
       "      <td>0.042485</td>\n",
       "      <td>-0.089582</td>\n",
       "      <td>0.020044</td>\n",
       "      <td>0.028817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.106964</td>\n",
       "      <td>-0.021721</td>\n",
       "      <td>0.013824</td>\n",
       "      <td>-0.099157</td>\n",
       "      <td>-0.027562</td>\n",
       "      <td>-0.001915</td>\n",
       "      <td>0.024394</td>\n",
       "      <td>-0.088323</td>\n",
       "      <td>0.019013</td>\n",
       "      <td>0.143317</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060745</td>\n",
       "      <td>-0.105131</td>\n",
       "      <td>-0.118476</td>\n",
       "      <td>-0.005167</td>\n",
       "      <td>-0.113995</td>\n",
       "      <td>0.030314</td>\n",
       "      <td>0.061448</td>\n",
       "      <td>-0.014393</td>\n",
       "      <td>0.043757</td>\n",
       "      <td>0.024685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.133641</td>\n",
       "      <td>0.009385</td>\n",
       "      <td>-0.053364</td>\n",
       "      <td>-0.068929</td>\n",
       "      <td>-0.043665</td>\n",
       "      <td>0.077559</td>\n",
       "      <td>-0.081948</td>\n",
       "      <td>0.031465</td>\n",
       "      <td>-0.023470</td>\n",
       "      <td>0.078005</td>\n",
       "      <td>...</td>\n",
       "      <td>0.172636</td>\n",
       "      <td>-0.099422</td>\n",
       "      <td>-0.046056</td>\n",
       "      <td>-0.032141</td>\n",
       "      <td>-0.119280</td>\n",
       "      <td>0.063709</td>\n",
       "      <td>0.060066</td>\n",
       "      <td>-0.012535</td>\n",
       "      <td>-0.044327</td>\n",
       "      <td>0.134915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.269532</td>\n",
       "      <td>-0.104382</td>\n",
       "      <td>0.153612</td>\n",
       "      <td>-0.195874</td>\n",
       "      <td>-0.237885</td>\n",
       "      <td>0.155002</td>\n",
       "      <td>0.127835</td>\n",
       "      <td>0.101133</td>\n",
       "      <td>-0.156294</td>\n",
       "      <td>0.000354</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028525</td>\n",
       "      <td>-0.283081</td>\n",
       "      <td>-0.136008</td>\n",
       "      <td>0.028519</td>\n",
       "      <td>-0.245955</td>\n",
       "      <td>0.167604</td>\n",
       "      <td>0.192651</td>\n",
       "      <td>-0.259907</td>\n",
       "      <td>0.058155</td>\n",
       "      <td>0.336046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.076811</td>\n",
       "      <td>0.043565</td>\n",
       "      <td>-0.169813</td>\n",
       "      <td>-0.321625</td>\n",
       "      <td>-0.172342</td>\n",
       "      <td>0.075025</td>\n",
       "      <td>0.048773</td>\n",
       "      <td>-0.269452</td>\n",
       "      <td>0.149295</td>\n",
       "      <td>-0.082685</td>\n",
       "      <td>...</td>\n",
       "      <td>0.092733</td>\n",
       "      <td>-0.280586</td>\n",
       "      <td>-0.136766</td>\n",
       "      <td>-0.094882</td>\n",
       "      <td>0.060746</td>\n",
       "      <td>0.208325</td>\n",
       "      <td>-0.153187</td>\n",
       "      <td>0.073233</td>\n",
       "      <td>0.150504</td>\n",
       "      <td>-0.160805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.163692</td>\n",
       "      <td>0.056500</td>\n",
       "      <td>-0.008522</td>\n",
       "      <td>-0.082305</td>\n",
       "      <td>0.002655</td>\n",
       "      <td>0.087847</td>\n",
       "      <td>-0.108957</td>\n",
       "      <td>0.020616</td>\n",
       "      <td>-0.047645</td>\n",
       "      <td>0.141893</td>\n",
       "      <td>...</td>\n",
       "      <td>0.205972</td>\n",
       "      <td>-0.086591</td>\n",
       "      <td>-0.004137</td>\n",
       "      <td>0.038187</td>\n",
       "      <td>-0.096085</td>\n",
       "      <td>0.076807</td>\n",
       "      <td>0.066676</td>\n",
       "      <td>0.013016</td>\n",
       "      <td>-0.059826</td>\n",
       "      <td>0.123454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.228586</td>\n",
       "      <td>-0.219441</td>\n",
       "      <td>0.283416</td>\n",
       "      <td>-0.253732</td>\n",
       "      <td>-0.203619</td>\n",
       "      <td>0.071845</td>\n",
       "      <td>0.103320</td>\n",
       "      <td>0.166107</td>\n",
       "      <td>-0.086552</td>\n",
       "      <td>0.196615</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.176795</td>\n",
       "      <td>-0.170434</td>\n",
       "      <td>-0.126624</td>\n",
       "      <td>0.073871</td>\n",
       "      <td>-0.091670</td>\n",
       "      <td>0.154736</td>\n",
       "      <td>0.326371</td>\n",
       "      <td>-0.157226</td>\n",
       "      <td>0.143123</td>\n",
       "      <td>0.170747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.175574</td>\n",
       "      <td>0.025295</td>\n",
       "      <td>0.034415</td>\n",
       "      <td>-0.119930</td>\n",
       "      <td>-0.001263</td>\n",
       "      <td>0.060563</td>\n",
       "      <td>-0.015810</td>\n",
       "      <td>-0.007708</td>\n",
       "      <td>0.046987</td>\n",
       "      <td>0.101929</td>\n",
       "      <td>...</td>\n",
       "      <td>0.086484</td>\n",
       "      <td>-0.121515</td>\n",
       "      <td>-0.171775</td>\n",
       "      <td>-0.085806</td>\n",
       "      <td>-0.060053</td>\n",
       "      <td>0.019699</td>\n",
       "      <td>0.059106</td>\n",
       "      <td>-0.041373</td>\n",
       "      <td>-0.023529</td>\n",
       "      <td>0.055924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.013189</td>\n",
       "      <td>-0.162207</td>\n",
       "      <td>0.125341</td>\n",
       "      <td>-0.115467</td>\n",
       "      <td>-0.000955</td>\n",
       "      <td>-0.059252</td>\n",
       "      <td>-0.015986</td>\n",
       "      <td>0.016241</td>\n",
       "      <td>0.131337</td>\n",
       "      <td>0.257338</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.117108</td>\n",
       "      <td>0.007259</td>\n",
       "      <td>-0.096602</td>\n",
       "      <td>-0.017131</td>\n",
       "      <td>0.022367</td>\n",
       "      <td>-0.014859</td>\n",
       "      <td>0.180033</td>\n",
       "      <td>0.066580</td>\n",
       "      <td>0.136753</td>\n",
       "      <td>-0.092314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.130534</td>\n",
       "      <td>-0.045560</td>\n",
       "      <td>0.028521</td>\n",
       "      <td>-0.080279</td>\n",
       "      <td>0.003858</td>\n",
       "      <td>0.003071</td>\n",
       "      <td>0.018743</td>\n",
       "      <td>-0.066084</td>\n",
       "      <td>0.033713</td>\n",
       "      <td>0.072565</td>\n",
       "      <td>...</td>\n",
       "      <td>0.105038</td>\n",
       "      <td>-0.078016</td>\n",
       "      <td>-0.176690</td>\n",
       "      <td>-0.018009</td>\n",
       "      <td>-0.126947</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>0.032713</td>\n",
       "      <td>-0.041610</td>\n",
       "      <td>0.049194</td>\n",
       "      <td>0.025586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.054134</td>\n",
       "      <td>-0.047149</td>\n",
       "      <td>-0.004463</td>\n",
       "      <td>-0.057609</td>\n",
       "      <td>-0.035221</td>\n",
       "      <td>0.023905</td>\n",
       "      <td>0.008529</td>\n",
       "      <td>-0.048733</td>\n",
       "      <td>0.061595</td>\n",
       "      <td>0.061077</td>\n",
       "      <td>...</td>\n",
       "      <td>0.088212</td>\n",
       "      <td>-0.105387</td>\n",
       "      <td>-0.105986</td>\n",
       "      <td>-0.062483</td>\n",
       "      <td>-0.131918</td>\n",
       "      <td>-0.001990</td>\n",
       "      <td>0.046313</td>\n",
       "      <td>-0.036101</td>\n",
       "      <td>0.051785</td>\n",
       "      <td>0.072986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.089847</td>\n",
       "      <td>-0.039652</td>\n",
       "      <td>0.060481</td>\n",
       "      <td>-0.048380</td>\n",
       "      <td>0.044933</td>\n",
       "      <td>0.032344</td>\n",
       "      <td>-0.006121</td>\n",
       "      <td>-0.051830</td>\n",
       "      <td>0.067143</td>\n",
       "      <td>0.041614</td>\n",
       "      <td>...</td>\n",
       "      <td>0.143193</td>\n",
       "      <td>-0.066922</td>\n",
       "      <td>-0.140798</td>\n",
       "      <td>-0.021114</td>\n",
       "      <td>-0.118120</td>\n",
       "      <td>-0.033508</td>\n",
       "      <td>0.026094</td>\n",
       "      <td>-0.048969</td>\n",
       "      <td>0.062298</td>\n",
       "      <td>0.055913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.080036</td>\n",
       "      <td>-0.050899</td>\n",
       "      <td>0.019357</td>\n",
       "      <td>-0.110051</td>\n",
       "      <td>0.066648</td>\n",
       "      <td>0.072724</td>\n",
       "      <td>0.089666</td>\n",
       "      <td>-0.062632</td>\n",
       "      <td>-0.017844</td>\n",
       "      <td>0.161694</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054096</td>\n",
       "      <td>-0.096530</td>\n",
       "      <td>-0.189168</td>\n",
       "      <td>-0.008945</td>\n",
       "      <td>-0.110072</td>\n",
       "      <td>-0.033477</td>\n",
       "      <td>0.162981</td>\n",
       "      <td>-0.042362</td>\n",
       "      <td>0.120935</td>\n",
       "      <td>0.120355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.012815</td>\n",
       "      <td>-0.132582</td>\n",
       "      <td>0.143100</td>\n",
       "      <td>-0.123674</td>\n",
       "      <td>0.038073</td>\n",
       "      <td>-0.039632</td>\n",
       "      <td>-0.056255</td>\n",
       "      <td>0.008863</td>\n",
       "      <td>0.162006</td>\n",
       "      <td>0.187490</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.073022</td>\n",
       "      <td>-0.008463</td>\n",
       "      <td>-0.155677</td>\n",
       "      <td>-0.065479</td>\n",
       "      <td>0.027984</td>\n",
       "      <td>-0.008959</td>\n",
       "      <td>0.168829</td>\n",
       "      <td>0.000411</td>\n",
       "      <td>0.100594</td>\n",
       "      <td>-0.116348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.243624</td>\n",
       "      <td>-0.098500</td>\n",
       "      <td>0.237654</td>\n",
       "      <td>-0.219328</td>\n",
       "      <td>-0.095716</td>\n",
       "      <td>0.205545</td>\n",
       "      <td>0.205810</td>\n",
       "      <td>0.076608</td>\n",
       "      <td>-0.174601</td>\n",
       "      <td>0.101197</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034625</td>\n",
       "      <td>-0.263443</td>\n",
       "      <td>-0.172518</td>\n",
       "      <td>0.110149</td>\n",
       "      <td>-0.241498</td>\n",
       "      <td>0.092102</td>\n",
       "      <td>0.281668</td>\n",
       "      <td>-0.254318</td>\n",
       "      <td>0.157954</td>\n",
       "      <td>0.376681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.339162</td>\n",
       "      <td>-0.022305</td>\n",
       "      <td>0.252712</td>\n",
       "      <td>-0.229207</td>\n",
       "      <td>-0.163628</td>\n",
       "      <td>0.193384</td>\n",
       "      <td>0.100335</td>\n",
       "      <td>0.131532</td>\n",
       "      <td>-0.109769</td>\n",
       "      <td>0.041432</td>\n",
       "      <td>...</td>\n",
       "      <td>0.067013</td>\n",
       "      <td>-0.288428</td>\n",
       "      <td>-0.155125</td>\n",
       "      <td>0.033287</td>\n",
       "      <td>-0.191479</td>\n",
       "      <td>0.145278</td>\n",
       "      <td>0.177793</td>\n",
       "      <td>-0.253329</td>\n",
       "      <td>0.013489</td>\n",
       "      <td>0.312250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.075790</td>\n",
       "      <td>0.008505</td>\n",
       "      <td>-0.248321</td>\n",
       "      <td>-0.309065</td>\n",
       "      <td>-0.220301</td>\n",
       "      <td>0.099121</td>\n",
       "      <td>0.036069</td>\n",
       "      <td>-0.219236</td>\n",
       "      <td>0.130745</td>\n",
       "      <td>-0.165152</td>\n",
       "      <td>...</td>\n",
       "      <td>0.079984</td>\n",
       "      <td>-0.291623</td>\n",
       "      <td>-0.170948</td>\n",
       "      <td>-0.180290</td>\n",
       "      <td>0.060211</td>\n",
       "      <td>0.220036</td>\n",
       "      <td>-0.140672</td>\n",
       "      <td>0.039676</td>\n",
       "      <td>0.127883</td>\n",
       "      <td>-0.105770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-0.145784</td>\n",
       "      <td>0.040720</td>\n",
       "      <td>-0.003117</td>\n",
       "      <td>-0.078579</td>\n",
       "      <td>0.005069</td>\n",
       "      <td>0.081011</td>\n",
       "      <td>-0.090947</td>\n",
       "      <td>0.006129</td>\n",
       "      <td>-0.032623</td>\n",
       "      <td>0.129295</td>\n",
       "      <td>...</td>\n",
       "      <td>0.183324</td>\n",
       "      <td>-0.088072</td>\n",
       "      <td>-0.022654</td>\n",
       "      <td>0.022072</td>\n",
       "      <td>-0.092530</td>\n",
       "      <td>0.062300</td>\n",
       "      <td>0.068582</td>\n",
       "      <td>0.001814</td>\n",
       "      <td>-0.039251</td>\n",
       "      <td>0.116942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.096836</td>\n",
       "      <td>0.005750</td>\n",
       "      <td>0.100170</td>\n",
       "      <td>-0.101835</td>\n",
       "      <td>0.133247</td>\n",
       "      <td>0.136926</td>\n",
       "      <td>0.046300</td>\n",
       "      <td>0.007357</td>\n",
       "      <td>0.071262</td>\n",
       "      <td>0.120531</td>\n",
       "      <td>...</td>\n",
       "      <td>0.120050</td>\n",
       "      <td>-0.102133</td>\n",
       "      <td>-0.195795</td>\n",
       "      <td>-0.061493</td>\n",
       "      <td>-0.073519</td>\n",
       "      <td>-0.088107</td>\n",
       "      <td>0.132988</td>\n",
       "      <td>-0.057492</td>\n",
       "      <td>0.084297</td>\n",
       "      <td>0.144859</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27 rows  99 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "0   4EBP1pS65   RbpS807  MAPKpT202  MEKpS217        S6     PAI-1  AKTpS473  \\\n",
       "0   -0.034876 -0.183665   0.143622 -0.082877  0.001758 -0.003982  0.003231   \n",
       "1   -0.294122 -0.093160   0.246818 -0.189556 -0.158506  0.135892  0.134888   \n",
       "2   -0.080961  0.007904   0.031583 -0.107364  0.011465  0.017704 -0.015875   \n",
       "3   -0.097761  0.064553   0.112396 -0.099148  0.078064  0.081907 -0.059240   \n",
       "4   -0.066278 -0.015814   0.045784 -0.067259  0.013513  0.027357 -0.000470   \n",
       "5   -0.088922  0.045411  -0.218302 -0.342581 -0.189004  0.107046  0.031690   \n",
       "6   -0.039349 -0.044991   0.051317 -0.078153  0.107723  0.101997  0.064801   \n",
       "7   -0.028227 -0.041266   0.079579 -0.081062  0.106948  0.074448  0.086504   \n",
       "8   -0.038961  0.014991   0.065171 -0.075859  0.053893  0.046696 -0.041396   \n",
       "9   -0.106964 -0.021721   0.013824 -0.099157 -0.027562 -0.001915  0.024394   \n",
       "10  -0.133641  0.009385  -0.053364 -0.068929 -0.043665  0.077559 -0.081948   \n",
       "11  -0.269532 -0.104382   0.153612 -0.195874 -0.237885  0.155002  0.127835   \n",
       "12  -0.076811  0.043565  -0.169813 -0.321625 -0.172342  0.075025  0.048773   \n",
       "13  -0.163692  0.056500  -0.008522 -0.082305  0.002655  0.087847 -0.108957   \n",
       "14  -0.228586 -0.219441   0.283416 -0.253732 -0.203619  0.071845  0.103320   \n",
       "15  -0.175574  0.025295   0.034415 -0.119930 -0.001263  0.060563 -0.015810   \n",
       "16  -0.013189 -0.162207   0.125341 -0.115467 -0.000955 -0.059252 -0.015986   \n",
       "17  -0.130534 -0.045560   0.028521 -0.080279  0.003858  0.003071  0.018743   \n",
       "18  -0.054134 -0.047149  -0.004463 -0.057609 -0.035221  0.023905  0.008529   \n",
       "19  -0.089847 -0.039652   0.060481 -0.048380  0.044933  0.032344 -0.006121   \n",
       "20  -0.080036 -0.050899   0.019357 -0.110051  0.066648  0.072724  0.089666   \n",
       "21   0.012815 -0.132582   0.143100 -0.123674  0.038073 -0.039632 -0.056255   \n",
       "22  -0.243624 -0.098500   0.237654 -0.219328 -0.095716  0.205545  0.205810   \n",
       "23  -0.339162 -0.022305   0.252712 -0.229207 -0.163628  0.193384  0.100335   \n",
       "24  -0.075790  0.008505  -0.248321 -0.309065 -0.220301  0.099121  0.036069   \n",
       "25  -0.145784  0.040720  -0.003117 -0.078579  0.005069  0.081011 -0.090947   \n",
       "26  -0.096836  0.005750   0.100170 -0.101835  0.133247  0.136926  0.046300   \n",
       "\n",
       "0   AMPKpT172  b-Catenin       BIM  ...     aHDAC     aMDM2      aJAK  \\\n",
       "0    0.034118   0.093920  0.200523  ... -0.122743 -0.030154 -0.080675   \n",
       "1    0.073156  -0.123043  0.012068  ...  0.085567 -0.244929 -0.160039   \n",
       "2   -0.095701   0.049683  0.073469  ...  0.104831 -0.120854 -0.177551   \n",
       "3   -0.025711   0.138789  0.032307  ...  0.170786 -0.126457 -0.184178   \n",
       "4   -0.074069   0.052443  0.112367  ...  0.098900 -0.094037 -0.082585   \n",
       "5   -0.262903   0.130680 -0.132162  ...  0.090093 -0.297775 -0.152631   \n",
       "6   -0.048378   0.015586  0.130743  ...  0.092250 -0.085436 -0.153277   \n",
       "7   -0.073258   0.043288  0.161920  ...  0.094312 -0.085749 -0.142496   \n",
       "8   -0.082587   0.085177  0.041151  ...  0.144618 -0.110327 -0.143367   \n",
       "9   -0.088323   0.019013  0.143317  ...  0.060745 -0.105131 -0.118476   \n",
       "10   0.031465  -0.023470  0.078005  ...  0.172636 -0.099422 -0.046056   \n",
       "11   0.101133  -0.156294  0.000354  ...  0.028525 -0.283081 -0.136008   \n",
       "12  -0.269452   0.149295 -0.082685  ...  0.092733 -0.280586 -0.136766   \n",
       "13   0.020616  -0.047645  0.141893  ...  0.205972 -0.086591 -0.004137   \n",
       "14   0.166107  -0.086552  0.196615  ... -0.176795 -0.170434 -0.126624   \n",
       "15  -0.007708   0.046987  0.101929  ...  0.086484 -0.121515 -0.171775   \n",
       "16   0.016241   0.131337  0.257338  ... -0.117108  0.007259 -0.096602   \n",
       "17  -0.066084   0.033713  0.072565  ...  0.105038 -0.078016 -0.176690   \n",
       "18  -0.048733   0.061595  0.061077  ...  0.088212 -0.105387 -0.105986   \n",
       "19  -0.051830   0.067143  0.041614  ...  0.143193 -0.066922 -0.140798   \n",
       "20  -0.062632  -0.017844  0.161694  ...  0.054096 -0.096530 -0.189168   \n",
       "21   0.008863   0.162006  0.187490  ... -0.073022 -0.008463 -0.155677   \n",
       "22   0.076608  -0.174601  0.101197  ...  0.034625 -0.263443 -0.172518   \n",
       "23   0.131532  -0.109769  0.041432  ...  0.067013 -0.288428 -0.155125   \n",
       "24  -0.219236   0.130745 -0.165152  ...  0.079984 -0.291623 -0.170948   \n",
       "25   0.006129  -0.032623  0.129295  ...  0.183324 -0.088072 -0.022654   \n",
       "26   0.007357   0.071262  0.120531  ...  0.120050 -0.102133 -0.195795   \n",
       "\n",
       "0     aBRAFm      aPKC    aSTAT3     amTOR     aPI3K     aCDK4      aSRC  \n",
       "0  -0.072349  0.040038 -0.025492  0.180109  0.051663  0.151754 -0.064242  \n",
       "1   0.101084 -0.258373  0.125785  0.151400 -0.253566  0.086212  0.281912  \n",
       "2  -0.053515 -0.108378  0.036214  0.050245 -0.080562  0.007598  0.000651  \n",
       "3  -0.106063 -0.071826 -0.018416  0.020251 -0.095692 -0.029040  0.025155  \n",
       "4  -0.008271 -0.105168 -0.003399  0.054829 -0.021752  0.056861  0.055012  \n",
       "5  -0.134203  0.102528  0.238491 -0.151672  0.062936  0.141745 -0.143992  \n",
       "6  -0.012049 -0.101245 -0.067190  0.156362 -0.049721  0.134039  0.150682  \n",
       "7   0.019146 -0.127461 -0.077492  0.135330 -0.030512  0.151584  0.113620  \n",
       "8  -0.057226 -0.100280  0.002345  0.042485 -0.089582  0.020044  0.028817  \n",
       "9  -0.005167 -0.113995  0.030314  0.061448 -0.014393  0.043757  0.024685  \n",
       "10 -0.032141 -0.119280  0.063709  0.060066 -0.012535 -0.044327  0.134915  \n",
       "11  0.028519 -0.245955  0.167604  0.192651 -0.259907  0.058155  0.336046  \n",
       "12 -0.094882  0.060746  0.208325 -0.153187  0.073233  0.150504 -0.160805  \n",
       "13  0.038187 -0.096085  0.076807  0.066676  0.013016 -0.059826  0.123454  \n",
       "14  0.073871 -0.091670  0.154736  0.326371 -0.157226  0.143123  0.170747  \n",
       "15 -0.085806 -0.060053  0.019699  0.059106 -0.041373 -0.023529  0.055924  \n",
       "16 -0.017131  0.022367 -0.014859  0.180033  0.066580  0.136753 -0.092314  \n",
       "17 -0.018009 -0.126947  0.000206  0.032713 -0.041610  0.049194  0.025586  \n",
       "18 -0.062483 -0.131918 -0.001990  0.046313 -0.036101  0.051785  0.072986  \n",
       "19 -0.021114 -0.118120 -0.033508  0.026094 -0.048969  0.062298  0.055913  \n",
       "20 -0.008945 -0.110072 -0.033477  0.162981 -0.042362  0.120935  0.120355  \n",
       "21 -0.065479  0.027984 -0.008959  0.168829  0.000411  0.100594 -0.116348  \n",
       "22  0.110149 -0.241498  0.092102  0.281668 -0.254318  0.157954  0.376681  \n",
       "23  0.033287 -0.191479  0.145278  0.177793 -0.253329  0.013489  0.312250  \n",
       "24 -0.180290  0.060211  0.220036 -0.140672  0.039676  0.127883 -0.105770  \n",
       "25  0.022072 -0.092530  0.062300  0.068582  0.001814 -0.039251  0.116942  \n",
       "26 -0.061493 -0.073519 -0.088107  0.132988 -0.057492  0.084297  0.144859  \n",
       "\n",
       "[27 rows x 99 columns]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "screenshot[\"y_hat\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cellbox-3.6-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
