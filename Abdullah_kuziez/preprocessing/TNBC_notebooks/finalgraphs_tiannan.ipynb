{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cceb5a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading in data\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import IPython\n",
    "print(IPython.get_ipython().config)\n",
    "import os\n",
    "os.getcwd()\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from scipy.cluster.hierarchy import linkage, leaves_list, fcluster\n",
    "from collections import defaultdict\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "import requests\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '../../../'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "#functions written by AK:\n",
    "from Abdullah_kuziez.preprocessing.pre_processing_py_fxns.filtering_functions import *\n",
    "from Abdullah_kuziez.preprocessing.pre_processing_py_fxns.graphing_fxns import *\n",
    "from Abdullah_kuziez.preprocessing.pre_processing_py_fxns.making_cellbox_files import *\n",
    "from Abdullah_kuziez.preprocessing.pre_processing_py_fxns.Initial_structuring import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ca3556",
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_dir_48hr = Path(\"intermediate_files_TNBC/48hr\")\n",
    "data_48hr=load_data(intermediate_dir_48hr, \"48hr\")\n",
    "intermediate_dir_6hr = Path(\"intermediate_files_TNBC/6hr\")\n",
    "data_6hr=load_data(intermediate_dir_6hr, \"6hr\")\n",
    "intermediate_dir_24hr = Path(\"intermediate_files_TNBC/24hr\")\n",
    "data_24hr=load_data(intermediate_dir_24hr, \"24hr\")\n",
    "#splitting the data into targeted and non_targeted proteins:\n",
    "tgt_prots_raw_6hr,non_tgt_prots_raw_6hr=split_tgt_and_non_tgt_prots(data_6hr,data_6hr['cell_lines'])\n",
    "tgt_prots_raw_24hr,non_tgt_prots_raw_24hr=split_tgt_and_non_tgt_prots(data_24hr,data_24hr['cell_lines'])\n",
    "tgt_prots_raw_48hr,non_tgt_prots_raw_48hr=split_tgt_and_non_tgt_prots(data_48hr,data_48hr['cell_lines'])\n",
    "\n",
    "\n",
    "data_6hr['data_by_cell_line_raw']['HS578T'].to_csv('HS578T_6hr_data_by_cell_line_raw.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddb5896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plots for number of perturbations (rows) and number of target proteins (columns) per cell line\n",
    "# Now using 24hr proteins\n",
    "\n",
    "cell_lines = list(tgt_prots_raw_24hr.keys())\n",
    "\n",
    "# Count number of perturbations (rows) and number of target proteins (columns) for each cell line\n",
    "n_perturbations = []\n",
    "n_target_proteins = []\n",
    "cell_lines = ['DU4475', 'MCF7', 'HS578T', 'HCC70', 'BT549', 'MDA-MB-453']\n",
    "for cell_line in cell_lines:\n",
    "    # Remove meta columns for accurate counts\n",
    "    meta_cols_tgt = [col for col in tgt_prots_raw_24hr[cell_line].columns if col.startswith('meta_')]\n",
    "    df_tgt = tgt_prots_raw_24hr[cell_line].drop(columns=meta_cols_tgt)\n",
    "    n_perturbations.append(df_tgt.shape[0])\n",
    "    n_target_proteins.append(df_tgt.shape[1])\n",
    "\n",
    "# Create a DataFrame for plotting\n",
    "df_counts = pd.DataFrame({\n",
    "    'Cell Line': cell_lines,\n",
    "    'Perturbations': n_perturbations,\n",
    "    'Target Proteins': n_target_proteins\n",
    "})\n",
    "\n",
    "# Select 14 colors from the palette, then use colors 0,1,4,7,9,11 (0-based indexing, decremented by 1)\n",
    "palette_14 = sns.color_palette(\"viridis\", n_colors=14)\n",
    "selected_colors = [palette_14[i] for i in [0, 1, 4, 7, 9, 11]]\n",
    "pert_palette = selected_colors\n",
    "prot_palette = selected_colors\n",
    "\n",
    "# Plotting: Separate bar plots for perturbations and target proteins\n",
    "\n",
    "# Plot for number of perturbations (rows) - each bar a different color\n",
    "fig1, ax1 = plt.subplots(figsize=(16, 4))\n",
    "x = np.arange(len(cell_lines))\n",
    "bar_width = 0.5  # Reduce bar width to increase spacing\n",
    "ax1.bar(x, df_counts['Perturbations'], color=pert_palette, width=bar_width)\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(cell_lines, rotation=45)\n",
    "ax1.set_ylabel('Number of Perturbations')\n",
    "ax1.set_title('Number of Perturbations per Cell Line')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot for number of target proteins (columns) - each bar a different color\n",
    "fig2, ax2 = plt.subplots(figsize=(16, 4))\n",
    "ax2.bar(x, df_counts['Target Proteins'], color=prot_palette, width=bar_width)\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(cell_lines, rotation=45)\n",
    "ax2.set_ylabel('Number of Target Proteins')\n",
    "ax2.set_title('Number of Target Proteins per Cell Line')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02db78d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# 1. Pure Gaussian\n",
    "gaussian = np.random.normal(loc=0, scale=1, size=1000)\n",
    "\n",
    "# 2. Gaussian with more outliers (increase number of outliers)\n",
    "n_outliers_2 = 20  # Increased from 5 to 20\n",
    "outliers_1 = np.random.uniform(low=6, high=8, size=n_outliers_2)\n",
    "outliers_2 = np.random.uniform(low=-8, high=-6, size=n_outliers_2)\n",
    "gaussian_with_outliers = np.concatenate([gaussian, outliers_1, outliers_2])\n",
    "\n",
    "# 3. Gaussian with previous outliers + even more, larger outliers\n",
    "n_more_outliers_3 = 10  # Increased from 3 to 10\n",
    "# These new outliers have larger magnitude than those in graph 2\n",
    "more_outliers_1 = np.random.uniform(low=16, high=20, size=n_more_outliers_3)\n",
    "more_outliers_2 = np.random.uniform(low=-20, high=-16, size=n_more_outliers_3)\n",
    "gaussian_with_more_outliers = np.concatenate([gaussian_with_outliers, more_outliers_1, more_outliers_2])\n",
    "\n",
    "# Plotting - all the same color\n",
    "hist_color = '#8856a7'\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(18, 5), sharey=True)\n",
    "\n",
    "# Flip the order of the graphs: rightmost is pure Gaussian, leftmost is Gaussian + even more, larger outliers\n",
    "\n",
    "# Plot 1: Gaussian + Even More, Larger Outliers (leftmost)\n",
    "sns.histplot(gaussian_with_more_outliers, bins=40, color=hist_color, edgecolor='black', ax=axs[0])\n",
    "axs[0].set_title('Step 1')\n",
    "axs[0].set_xlabel('Value')\n",
    "axs[0].set_ylabel('Frequency')\n",
    "\n",
    "# Plot 2: Gaussian + More Outliers (middle)\n",
    "sns.histplot(gaussian_with_outliers, bins=40, color=hist_color, edgecolor='black', ax=axs[1])\n",
    "axs[1].set_title('Step i')\n",
    "axs[1].set_xlabel('Value')\n",
    "\n",
    "# Plot 3: Pure Gaussian (rightmost)\n",
    "sns.histplot(gaussian, bins=40, color=hist_color, edgecolor='black', ax=axs[2])\n",
    "axs[2].set_title('End')\n",
    "axs[2].set_xlabel('Value')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86837ff3",
   "metadata": {},
   "source": [
    "# Independent code from prior block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b78112",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"PCA-ing different perturbations to see if the individual replicates from a perturbation cluster together as compared to with other experiments\"\"\"\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from scipy.cluster.hierarchy import linkage, leaves_list, fcluster\n",
    "from collections import defaultdict\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "import requests\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "#//////////////////////////////////////////Part 1: loading in data //////////////////////////////\n",
    "#/////////////////////////////////////////////////////////////////////////////////////////////\n",
    "#/////////////////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "with open(r'C:\\Users\\abdul\\OneDrive - University of Cambridge\\Desktop\\MDRA\\cellbox_torch\\Abdullah_kuziez\\Experiments\\raw_data\\TNBC_set\\symbol_to_uniprot.json', 'r') as f:\n",
    "    symbol_to_uniprot = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(symbol_to_uniprot)} symbol-to-uniprot mappings\")\n",
    "\n",
    "data=pd.read_csv(\n",
    "r'C:\\Users\\abdul\\OneDrive - University of Cambridge\\Desktop\\MDRA\\cellbox_torch\\Abdullah_kuziez\\Experiments\\raw_data\\TNBC_set\\PTV1_protein_matrix_test.cleaned.tsv',\n",
    "index_col=0,sep='\\t')\n",
    "\n",
    "viability_raw=pd.read_excel(\n",
    "r'C:\\Users\\abdul\\OneDrive - University of Cambridge\\Desktop\\MDRA\\cellbox_torch\\Abdullah_kuziez\\Experiments\\raw_data\\TNBC_set\\cell_viability_sampleinfo_for_chris.xlsx', \n",
    "sheet_name=0)\n",
    "\n",
    "screen_info = pd.read_excel(\n",
    "    r\"C:\\Users\\abdul\\OneDrive - University of Cambridge\\Desktop\\MDRA\\cellbox_torch\\Abdullah_kuziez\\Experiments\\raw_data\\TNBC_set\\PTV1_sample_info_test.xlsx\", \n",
    "    sheet_name=0)  # First sheet is single perturbation info\n",
    "\n",
    "\n",
    "#//////////////////////////////////////////Part 2: structuring data //////////////////////////////\n",
    "#/////////////////////////////////////////////////////////////////////////////////////////////\n",
    "#/////////////////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "#making data_by_cell_line_raw; control_data_by_cell_line; control_data_by_cell_line_coeffvar:\n",
    "data_T=data.transpose()\n",
    "data_T['Sample_ID']=data_T.index\n",
    "data_and_screen_info=pd.merge(data_T,screen_info,on='Sample_ID',how='left')\n",
    "data_and_screen_info_and_viability = pd.merge(data_and_screen_info, viability_raw, on='Sample_ID', how='left')\n",
    "\n",
    "#averaging over replicates:(pandas mean ignores NaN's)\n",
    "#preserving the control rows since they drop out for some reason:\n",
    "control_rows=data_and_screen_info_and_viability[data_and_screen_info_and_viability['pert_id']=='no']\n",
    "r_conts = control_rows.copy()\n",
    "data_and_screen_info_and_viability_grouped = (\n",
    "    data_and_screen_info_and_viability\n",
    "    .groupby(['pert_id', 'Cell', 'pert_time_x'])\n",
    "    .mean(numeric_only=True)\n",
    "    .reset_index()\n",
    ")\n",
    "r_conts.loc[r_conts['pert_id'] == 'no', 'pert_id'] = 'control'\n",
    "r_conts_grouped_mean=r_conts.groupby(['pert_id', 'cell_line']).mean(numeric_only=True).reset_index()\n",
    "#dropping unneeded columns:\n",
    "data_dropped=data_and_screen_info_and_viability_grouped.drop(columns=['BioRep_y','pert_time_y'])\n",
    "\n",
    "#getting coeff var for control data for filtering:\n",
    "intermediate=r_conts.groupby(['cell_line']).std(numeric_only=True)\n",
    "inter_idx=intermediate.index\n",
    "r_conts_grouped_std=intermediate.reset_index()\n",
    "# Calculate coefficient of variation, but if mean exists and std is NaN, set coeff_var to 3\n",
    "mean_df = r_conts_grouped_mean.select_dtypes(include=[float, int])\n",
    "std_df = r_conts_grouped_std.select_dtypes(include=[float, int])\n",
    "coeff_var = std_df / mean_df\n",
    "\n",
    "# Find where mean is not NaN and std is NaN, set coeff_var to 3 in those places\n",
    "mask = mean_df.notna() & std_df.isna()\n",
    "coeff_var[mask] = -.25\n",
    "coeff_var['cell_line']=inter_idx\n",
    "\n",
    "#dropping the irrelevant timepoints, (6 and 48)\n",
    "data_dropped=data_dropped[data_dropped['pert_time_x']==6]\n",
    "\n",
    "#//////////metadata codeblock\n",
    "# Move the first three columns to the end of data_dropped\n",
    "first_three_cols = data_dropped.columns[:3]\n",
    "other_cols = data_dropped.columns[3:]\n",
    "data_dropped = data_dropped.loc[:, list(other_cols) + list(first_three_cols)]\n",
    "# Rename the last 10 columns of data_dropped by adding 'meta_' as a prefix\n",
    "cols = list(data_dropped.columns)\n",
    "last_9 = cols[-8:]\n",
    "new_last_9 = ['meta_' + col for col in last_9]\n",
    "rename_dict = dict(zip(last_9, new_last_9))\n",
    "data_dropped = data_dropped.rename(columns=rename_dict)\n",
    "#////////////////////////\n",
    "\n",
    "#generating the dictionaries by cell line\n",
    "data_by_cell_line_raw={}\n",
    "control_data_by_cell_line={}\n",
    "control_data_by_cell_line_coeffvar={}\n",
    "cell_lines=pd.unique(data_and_screen_info_and_viability_grouped['Cell'])\n",
    "for cell in cell_lines:\n",
    "    data_by_cell_line_raw[cell]=data_dropped[data_dropped['meta_Cell']==cell]\n",
    "    control_data_by_cell_line[cell]=r_conts_grouped_mean[r_conts_grouped_mean['cell_line']==cell]\n",
    "    control_data_by_cell_line_coeffvar[cell]=coeff_var[coeff_var['cell_line']==cell]\n",
    "    control_data_by_cell_line_coeffvar[cell].drop(columns=['cell_line'],inplace=True)\n",
    "\n",
    "#//////////////////////////////////////////Part 3: renaming from pert-ID to drug name to group trials //////////////////////////////\n",
    "#/////////////////////////////////////////////////////////////////////////////////////////////\n",
    "#/////////////////////////////////////////////////////////////////////////////////////////////\n",
    "#making drug_pert_id_targets_dict\n",
    "drugs_and_targets = pd.read_csv(\n",
    "    r'C:\\Users\\abdul\\OneDrive - University of Cambridge\\Desktop\\MDRA\\cellbox_torch\\Abdullah_kuziez\\Experiments\\raw_data\\TNBC_set\\ptv1_unique_drug_names.csv'\n",
    ")\n",
    "drugs_and_targets.rename(columns={'original_drug_names': 'pert_name'}, inplace=True)\n",
    "drugs_and_targets.dropna(inplace=True,subset=['pert_name'])\n",
    "\n",
    "# Read and process drugs_and_pert_ids\n",
    "drugs_and_pert_ids = pd.read_csv(\n",
    "    r'C:\\Users\\abdul\\OneDrive - University of Cambridge\\Desktop\\MDRA\\cellbox_torch\\Abdullah_kuziez\\Experiments\\raw_data\\TNBC_set\\PTV1_sample_info_test.csv'\n",
    ")\n",
    "\n",
    "drugs_and_pert_ids = (\n",
    "    drugs_and_pert_ids\n",
    "    .drop_duplicates(subset=['pert_id'])\n",
    "    .drop_duplicates(subset=['pert_name'])\n",
    "    .loc[:, ['pert_id', 'pert_name']]\n",
    "    .assign(pert_id=lambda df: df['pert_id'].str.replace('#', ''))\n",
    "    .dropna()\n",
    "    .assign(pert_id=lambda df: df['pert_id'].astype(int))\n",
    "    .sort_values('pert_id')\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Merge and drop unnecessary columns\n",
    "drugs_pert_ids_targets = (\n",
    "    pd.merge(drugs_and_pert_ids, drugs_and_targets, on='pert_name', how='left')\n",
    "    .drop(columns=['corrected_drug_name', 'drugbank_targets_manual_check'], errors='ignore')\n",
    ")\n",
    "\n",
    "# Build dictionary mapping pert_id and pert_name to list of targets\n",
    "drug_pert_id_targets_dict = {}\n",
    "for _, row in drugs_pert_ids_targets.iterrows():\n",
    "    targets = []\n",
    "    if pd.notnull(row.get('target_uniprot_ids')):\n",
    "        targets = [t.strip() for t in str(row['target_uniprot_ids']).split(',') if t.strip()]\n",
    "    drug_pert_id_targets_dict[row['pert_id']] = targets\n",
    "    drug_pert_id_targets_dict[row['pert_name']] = targets\n",
    "\n",
    "\n",
    "#//////////////////////////////////////////Part 4: making targeted_prots_raw and non_targeted_prots_raw: //////////////////////////////\n",
    "#/////////////////////////////////////////////////////////////////////////////////////////////\n",
    "#/////////////////////////////////////////////////////////////////////////////////////////////\n",
    "#making targeted_prots_raw and non_targeted_prots_raw:\n",
    "graph_flag=False\n",
    "#Main method-essential filtering:\n",
    "# graph_flag = False  #set to true to plot in the filter lines\n",
    "cell_lines=['HS578T','HCC70','BT549','MDA-MB-453','MCF7','DU4475']\n",
    "\n",
    "#basic completeness filters #TODO graphs are broken but that's fine\n",
    "targeted_prots_raw={}\n",
    "non_targeted_prots_raw={}\n",
    "all_targeted_prots=[item for sublist in drug_pert_id_targets_dict.values() for item in sublist]\n",
    "\n",
    "for cell in cell_lines:\n",
    "    meta_cols=data_by_cell_line_raw[cell].columns[data_by_cell_line_raw[cell].columns.str.contains('meta_')]\n",
    "    intersection=list(set(all_targeted_prots).intersection(set(data_by_cell_line_raw[cell].columns)))\n",
    "\n",
    "\n",
    "    targeted_prots_raw[cell] = data_by_cell_line_raw[cell][list(intersection) + list(meta_cols)]\n",
    "    \n",
    "    non_targeted_prots_raw[cell]=data_by_cell_line_raw[cell].drop(columns=intersection)\n",
    "\n",
    "\n",
    "#//////////////////////////////////////////Part 5: writing data to files///////////////////////\n",
    "#/////////////////////////////////////////////////////////////////////////////////////////////\n",
    "#/////////////////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "# Set a directory for intermediate files\n",
    "intermediate_dir = Path(\"intermediate_files_TNBC/6hr\")\n",
    "intermediate_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save data_by_cell_line_raw, control_data_by_cell_line, control_data_by_cell_line_coeffvar, targeted_prots_raw, non_targeted_prots_raw\n",
    "with open(intermediate_dir / \"data_by_cell_line_raw_6hr.pkl\", \"wb\") as f:\n",
    "    pickle.dump(data_by_cell_line_raw, f)\n",
    "with open(intermediate_dir / \"control_data_by_cell_line_6hr.pkl\", \"wb\") as f:\n",
    "    pickle.dump(control_data_by_cell_line, f)\n",
    "with open(intermediate_dir / \"control_data_by_cell_line_coeffvar_6hr.pkl\", \"wb\") as f:\n",
    "    pickle.dump(control_data_by_cell_line_coeffvar, f)\n",
    "with open(intermediate_dir / \"targeted_prots_raw_6hr.pkl\", \"wb\") as f:\n",
    "    pickle.dump(targeted_prots_raw, f)\n",
    "with open(intermediate_dir / \"non_targeted_prots_raw_6hr.pkl\", \"wb\") as f:\n",
    "    pickle.dump(non_targeted_prots_raw, f)\n",
    "with open(intermediate_dir / \"cell_lines_6hr.pkl\", \"wb\") as f:\n",
    "    pickle.dump(cell_lines, f)\n",
    "with open(intermediate_dir / \"drug_pert_id_targets_dict_6hr.pkl\", \"wb\") as f:\n",
    "    pickle.dump(drug_pert_id_targets_dict, f)\n",
    "\n",
    "# Save symbol_to_uniprot as JSON\n",
    "with open(intermediate_dir / \"symbol_to_uniprot.json\", \"w\") as f:\n",
    "    json.dump(symbol_to_uniprot, f)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
