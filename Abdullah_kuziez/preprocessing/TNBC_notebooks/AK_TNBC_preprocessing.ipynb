{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# getting started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'IPKernelApp': {'connection_file': '/run/user/210498/jupyter/runtime/kernel-v3be4b4875eb70caac42ed045173941ec1c014a589.json'}, 'Completer': {'use_jedi': False}}\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'graphing_fxns'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m     sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m, project_root)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m#functions written by AK:\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mAbdullah_kuziez\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpre_processing_py_fxns\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfiltering_functions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m~/cellbox_torch/Abdullah_kuziez/preprocessing/pre_processing_py_fxns/filtering_functions.py:22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mhashlib\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgraphing_fxns\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m step_dicts_to_summary_df\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgraphing_fxns\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m plot_grid_graphs\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgraphing_fxns\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m print_summary_df\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'graphing_fxns'"
     ]
    }
   ],
   "source": [
    "#reading in data\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "import IPython\n",
    "print(IPython.get_ipython().config)\n",
    "import os\n",
    "os.getcwd()\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from scipy.cluster.hierarchy import linkage, leaves_list, fcluster\n",
    "from collections import defaultdict\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "import requests\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '../../../'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "#functions written by AK:\n",
    "from Abdullah_kuziez.preprocessing.pre_processing_py_fxns.filtering_functions import *\n",
    "# from Abdullah_kuziez.preprocessing.pre_processing_py_fxns.graphing_fxns import *\n",
    "# from Abdullah_kuziez.preprocessing.pre_processing_py_fxns.making_cellbox_files import *\n",
    "# from Abdullah_kuziez.preprocessing.pre_processing_py_fxns.Initial_structuring import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_dir_48hr = Path(\"intermediate_files_TNBC/48hr\")\n",
    "data_48hr=load_data(intermediate_dir_48hr, \"48hr\")\n",
    "intermediate_dir_6hr = Path(\"intermediate_files_TNBC/6hr\")\n",
    "data_6hr=load_data(intermediate_dir_6hr, \"6hr\")\n",
    "intermediate_dir_24hr = Path(\"intermediate_files_TNBC/24hr\")\n",
    "data_24hr=load_data(intermediate_dir_24hr, \"24hr\")\n",
    "\n",
    "#splitting the data into targeted and non_targeted proteins:\n",
    "all_targeted_prots=[item for sublist in data_48hr['drug_pert_id_targets_dict'].values() for item in sublist]\n",
    "\n",
    "targeted_prots_raw_6hr,targeted_prots_raw_24hr,targeted_prots_raw_48hr={},{},{}\n",
    "non_targeted_prots_raw_6hr,non_targeted_prots_raw_24hr,non_targeted_prots_raw_48hr={},{},{}\n",
    "cell_lines=data_48hr['cell_lines']\n",
    "for cell in cell_lines:\n",
    "    meta_cols_48hr=data_48hr['data_by_cell_line_raw'][cell].columns[data_48hr['data_by_cell_line_raw'][cell].columns.str.contains('meta_')]\n",
    "    meta_cols_6hr=data_6hr['data_by_cell_line_raw'][cell].columns[data_6hr['data_by_cell_line_raw'][cell].columns.str.contains('meta_')]\n",
    "    meta_cols_24hr=data_24hr['data_by_cell_line_raw'][cell].columns[data_24hr['data_by_cell_line_raw'][cell].columns.str.contains('meta_')]\n",
    "\n",
    "    intersection_48hr=list(set(all_targeted_prots).intersection(set(data_48hr['data_by_cell_line_raw'][cell].columns)))\n",
    "    intersection_6hr=list(set(all_targeted_prots).intersection(set(data_6hr['data_by_cell_line_raw'][cell].columns)))\n",
    "    intersection_24hr=list(set(all_targeted_prots).intersection(set(data_24hr['data_by_cell_line_raw'][cell].columns)))\n",
    "\n",
    "\n",
    "    targeted_prots_raw_48hr[cell] = data_48hr['data_by_cell_line_raw'][cell][list(intersection_48hr) + list(meta_cols_48hr)]\n",
    "    targeted_prots_raw_6hr[cell] = data_6hr['data_by_cell_line_raw'][cell][list(intersection_6hr) + list(meta_cols_6hr)]\n",
    "    targeted_prots_raw_24hr[cell] = data_24hr['data_by_cell_line_raw'][cell][list(intersection_24hr) + list(meta_cols_24hr)]\n",
    "    \n",
    "    non_targeted_prots_raw_48hr[cell]=data_48hr['data_by_cell_line_raw'][cell].drop(columns=intersection_48hr)\n",
    "    non_targeted_prots_raw_6hr[cell]=data_6hr['data_by_cell_line_raw'][cell].drop(columns=intersection_6hr)\n",
    "    non_targeted_prots_raw_24hr[cell]=data_24hr['data_by_cell_line_raw'][cell].drop(columns=intersection_24hr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline setup for all timepoints\n",
    "saved_filter_dir=r'C:\\Users\\abdul\\OneDrive - University of Cambridge\\Desktop\\MDRA\\cellbox_torch\\Abdullah_kuziez\\preprocessing\\TNBC_notebooks\\filtering_results'\n",
    "pipeline_steps_tgt=[['filter_proteins_with_control', '_'],\n",
    "                ['log2_transform_by_control', '_'],\n",
    "                ['filter_incomplete_proteins', '.95'],\n",
    "                ['filter_incomplete_experiments', '.95'],\n",
    "                ['fill_na_with_mean', '_'],\n",
    "                # ['filter_keep_low_cv', '.8'],\n",
    "                # ['remove_outlier_proteins', '10'],\n",
    "                ['filter_by_mutual_information', '.001','meta_Inhi_5'],\n",
    "                # ['iterative_signal_filtering', '2.5'],\n",
    "                # ['spearman_corr_filtering', '0.2','meta_Inhi_5'],\n",
    "                # ['pearson_corr_filtering', '0.2','meta_Inhi_5']\n",
    "                ]\n",
    "\n",
    "pipeline_steps_nontgt=[['filter_proteins_with_control', '_'],\n",
    "                ['log2_transform_by_control', '_'],\n",
    "                ['filter_incomplete_proteins', '.8'],\n",
    "                ['filter_incomplete_experiments', '.8'],\n",
    "                ['fill_na_with_mean', '_'],\n",
    "                # ['filter_keep_low_cv', '.8'],\n",
    "                # ['remove_outlier_proteins', '10'],\n",
    "                ['filter_by_mutual_information', '.001','meta_Inhi_5'],\n",
    "                ['iterative_signal_filtering', '3'],\n",
    "                ['spearman_corr_filtering', '0.2','meta_Inhi_5'],\n",
    "                ['pearson_corr_filtering', '0.2','meta_Inhi_5'],\n",
    "                ['noise injected', '_']\n",
    "                ]\n",
    "config={'print_flag':False,'graph_flag':False,'graph_type':'hist','filter_flag':True,'verbose':True}\n",
    "#//////////LOO regression////////////////////////////\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "\n",
    "elasticnet_model= ElasticNetCV(\n",
    "    alphas=[0.001, 0.01, 0.1, 1.0, 10.0],           # Alpha values to scan\n",
    "    l1_ratio=[0.1, 0.3, 0.5, 0.7, 0.9],            # L1 ratio values to scan\n",
    "    cv=5,                                            # 5-fold CV for parameter selection\n",
    "    random_state=42,\n",
    "    max_iter=5000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6hr pipeline\n",
    "print('targeted proteins 6hr')\n",
    "six_hr_tgt_pipeline=filtering_pipeline(targeted_prots_raw_6hr,data_6hr['cell_lines'],data_6hr['control_data_by_cell_line'],'6hr',data_6hr['control_data_by_cell_line_coeffvar'],**config)\n",
    "hd_six_tgt=six_hr_tgt_pipeline.run_pipeline(pipeline_steps_tgt,save_dir=saved_filter_dir,tgt='tgt')\n",
    "sum_six_tgt=six_hr_tgt_pipeline.summary_of_pipeline()\n",
    "six_hr_tgt_pipeline.save_pipeline(saved_filter_dir,tgt='tgt')\n",
    "\n",
    "six_hr_non_tgt_pipeline=filtering_pipeline(non_targeted_prots_raw_6hr,data_6hr['cell_lines'],data_6hr['control_data_by_cell_line'],'6hr',data_6hr['control_data_by_cell_line_coeffvar'],**config)\n",
    "hd_six_non_tgt=six_hr_non_tgt_pipeline.run_pipeline(pipeline_steps_nontgt,save_dir=saved_filter_dir,tgt='nontgt')\n",
    "sum_six_non_tgt=six_hr_non_tgt_pipeline.summary_of_pipeline()\n",
    "six_hr_non_tgt_pipeline.save_pipeline(saved_filter_dir,tgt='nontgt')\n",
    "#inject noise after all the filtering happens\n",
    "\n",
    "# six_hr_non_tgt_pipeline.noise_stability_test(mean_array=[0,1,1,1,1,1,1,1,1,1],sigma_array=[0,.001,.005,.01,.05,.1,.2,.4,.8,1])\n",
    "# noisy_data=six_hr_non_tgt_pipeline.inject_noise(mean_mult=0,sigma_mult=0)\n",
    "\n",
    "# _=six_hr_non_tgt_pipeline.run_loo_regression(ycol='meta_Inhi_5',model=elasticnet_model)\n",
    "\n",
    "# plot_protein_correlation_heatmaps(six_hr_tgt_pipeline.final_filtered_data,cell_lines)\n",
    "# plot_protein_correlation_heatmaps(six_hr_non_tgt_pipeline.final_filtered_data,cell_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#24hr pipeline:\n",
    "print('targeted proteins 24hr')\n",
    "twenty_four_hr_tgt_pipeline=filtering_pipeline(targeted_prots_raw_24hr,data_24hr['cell_lines'],data_24hr['control_data_by_cell_line'],'24hr',data_24hr['control_data_by_cell_line_coeffvar'],**config)\n",
    "hd_twentyfour_tgt=twenty_four_hr_tgt_pipeline.run_pipeline(pipeline_steps_tgt,save_dir=saved_filter_dir,tgt='tgt')\n",
    "sum_twentyfour_tgt=twenty_four_hr_tgt_pipeline.summary_of_pipeline()\n",
    "twenty_four_hr_tgt_pipeline.save_pipeline(saved_filter_dir,tgt='tgt')\n",
    "\n",
    "print('non-targeted proteins 24hr')\n",
    "twenty_four_hr_non_tgt_pipeline=filtering_pipeline(non_targeted_prots_raw_24hr,data_24hr['cell_lines'],data_24hr['control_data_by_cell_line'],'24hr',data_24hr['control_data_by_cell_line_coeffvar'],**config)\n",
    "hd_twentyfour_non_tgt=twenty_four_hr_non_tgt_pipeline.run_pipeline(pipeline_steps_nontgt,save_dir=saved_filter_dir,tgt='nontgt')\n",
    "sum_twentyfour_non_tgt=twenty_four_hr_non_tgt_pipeline.summary_of_pipeline()\n",
    "twenty_four_hr_non_tgt_pipeline.save_pipeline(saved_filter_dir,tgt='nontgt')\n",
    "# _=twenty_four_hr_non_tgt_pipeline.run_loo_regression(ycol='meta_Inhi_5',model=elasticnet_model)\n",
    "\n",
    "# plot_protein_correlation_heatmaps(twenty_four_hr_tgt_pipeline.final_filtered_data,cell_lines)\n",
    "# plot_protein_correlation_heatmaps(twenty_four_hr_non_tgt_pipeline.final_filtered_data,cell_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#48hr pipeline:\n",
    "print('targeted proteins 48hr')\n",
    "forty_eight_hr_tgt_pipeline=filtering_pipeline(targeted_prots_raw_48hr,data_48hr['cell_lines'],data_48hr['control_data_by_cell_line'],'48hr',data_48hr['control_data_by_cell_line_coeffvar'],**config)\n",
    "hd_fortyeight_tgt=forty_eight_hr_tgt_pipeline.run_pipeline(pipeline_steps_tgt,save_dir=saved_filter_dir,tgt='tgt')\n",
    "sum_fortyeight_tgt=forty_eight_hr_tgt_pipeline.summary_of_pipeline()\n",
    "forty_eight_hr_tgt_pipeline.save_pipeline(saved_filter_dir,tgt='tgt')\n",
    "\n",
    "print('non-targeted proteins 48hr')\n",
    "forty_eight_hr_non_tgt_pipeline=filtering_pipeline(non_targeted_prots_raw_48hr,data_48hr['cell_lines'],data_48hr['control_data_by_cell_line'],'48hr',data_48hr['control_data_by_cell_line_coeffvar'],**config)\n",
    "hd_fortyeight_non_tgt=forty_eight_hr_non_tgt_pipeline.run_pipeline(pipeline_steps_nontgt,save_dir=saved_filter_dir,tgt='nontgt')\n",
    "sum_fortyeight_non_tgt=forty_eight_hr_non_tgt_pipeline.summary_of_pipeline()\n",
    "forty_eight_hr_non_tgt_pipeline.save_pipeline(saved_filter_dir,tgt='nontgt')\n",
    "# _=forty_eight_hr_non_tgt_pipeline.run_loo_regression(ycol='meta_Inhi_5',model=elasticnet_model)\n",
    "# plot_protein_correlation_heatmaps(forty_eight_hr_tgt_pipeline.final_filtered_data,cell_lines)\n",
    "# plot_protein_correlation_heatmaps(forty_eight_hr_non_tgt_pipeline.final_filtered_data,cell_lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making combined dicts and ablation studying to get the most important so that I can have 100 nodes:\n",
    "#ablation studying:\n",
    "#TODO reimpleent the ablation study\n",
    "\n",
    "\n",
    "#make the combined dictionaries,\n",
    "combined_6hr_dict,combined_24hr_dict,combined_48hr_dict={},{},{}\n",
    "combined_6hr_dict_log,combined_24hr_dict_log,combined_48hr_dict_log={},{},{}\n",
    "for cell in six_hr_non_tgt_pipeline.final_filtered_data.keys():\n",
    "    combined_6hr_dict[cell]=pd.concat([six_hr_tgt_pipeline.final_filtered_data[cell],six_hr_non_tgt_pipeline.final_filtered_data[cell]],axis=1)\n",
    "    combined_24hr_dict[cell]=pd.concat([twenty_four_hr_tgt_pipeline.final_filtered_data[cell],twenty_four_hr_non_tgt_pipeline.final_filtered_data[cell]],axis=1)\n",
    "    combined_48hr_dict[cell]=pd.concat([forty_eight_hr_tgt_pipeline.final_filtered_data[cell],forty_eight_hr_non_tgt_pipeline.final_filtered_data[cell]],axis=1)\n",
    "\n",
    "    combined_6hr_dict_log[cell] = pd.concat([\n",
    "        six_hr_tgt_pipeline.outputs_from_steps['step_2_log2_transform_by_control__']['filtered_data'][cell],\n",
    "        six_hr_non_tgt_pipeline.outputs_from_steps['step_2_log2_transform_by_control__']['filtered_data'][cell]\n",
    "    ], axis=1)\n",
    "    combined_6hr_dict_log[cell] = combined_6hr_dict_log[cell].loc[:, ~combined_6hr_dict_log[cell].columns.duplicated()]\n",
    "\n",
    "    combined_24hr_dict_log[cell] = pd.concat([\n",
    "        twenty_four_hr_tgt_pipeline.outputs_from_steps['step_2_log2_transform_by_control__']['filtered_data'][cell],\n",
    "        twenty_four_hr_non_tgt_pipeline.outputs_from_steps['step_2_log2_transform_by_control__']['filtered_data'][cell]\n",
    "    ], axis=1)\n",
    "    combined_24hr_dict_log[cell] = combined_24hr_dict_log[cell].loc[:, ~combined_24hr_dict_log[cell].columns.duplicated()]\n",
    "\n",
    "    combined_48hr_dict_log[cell] = pd.concat([\n",
    "        forty_eight_hr_tgt_pipeline.outputs_from_steps['step_2_log2_transform_by_control__']['filtered_data'][cell],\n",
    "        forty_eight_hr_non_tgt_pipeline.outputs_from_steps['step_2_log2_transform_by_control__']['filtered_data'][cell]\n",
    "    ], axis=1)\n",
    "    combined_48hr_dict_log[cell] = combined_48hr_dict_log[cell].loc[:, ~combined_48hr_dict_log[cell].columns.duplicated()]\n",
    "\n",
    "#now make cellbox files for each of the timepoints:\n",
    "#cellbox files look like:\n",
    "#node_Index, a list of the names of each of the nodes\n",
    "#pert.csv has all the experiments as rows, and proteins/nodes as columns; expressions zeroed out and the activities are tanh transformed;\n",
    "#expr.csv has all the experiments as rows, and proteins/nodes as columns; expressions are log2 transformed and activites are tanh transformed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_cell_line=['HS578T']\n",
    "drug_pert_id_targets_dict=data_48hr['drug_pert_id_targets_dict'] #This dict has the pert Id and drug name as keys and the targeted proteins as values\n",
    "save_dir\n",
    "make_cellbox_files(combined_6hr_dict_log[selected_cell_line], drug_pert_id_targets_dict, save_path=save_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#taking union of proteins across all the timepoints and using that to predict:\n",
    "_6hr_prots,_24hr_prots,_48hr_prots,_overall_prots={},{},{},{} #these are all lists of proteins found in each set at the final filtering step\n",
    "#includes metadata\n",
    "final_6_hr_data,final_24_hr_data,final_48_hr_data,final_overall_data={},{},{},{}\n",
    "\n",
    "for cell in cell_lines:\n",
    "    #collecting union of all proteins across all timepoints:\n",
    "    _6hr_prots[cell]=list(set(six_hr_non_tgt_pipeline.final_filtered_data[cell].columns).union(set(six_hr_tgt_pipeline.final_filtered_data[cell].columns)))\n",
    "    _24hr_prots[cell]=list(set(twenty_four_hr_non_tgt_pipeline.final_filtered_data[cell].columns).union(set(twenty_four_hr_tgt_pipeline.final_filtered_data[cell].columns)))\n",
    "    _48hr_prots[cell]=list(set(forty_eight_hr_non_tgt_pipeline.final_filtered_data[cell].columns).union(set(forty_eight_hr_tgt_pipeline.final_filtered_data[cell].columns)))\n",
    "    _overall_prots[cell]=list(set(_6hr_prots[cell]).union(set(_24hr_prots[cell])).union(set(_48hr_prots[cell])))\n",
    "    #pulling out the proteins per cell line:\n",
    "    final_6_hr_data[cell]=combined_6hr_dict_log[cell][_overall_prots[cell]]\n",
    "    final_24_hr_data[cell]=combined_24hr_dict_log[cell][_overall_prots[cell]]\n",
    "    final_48_hr_data[cell]=combined_48hr_dict_log[cell][_overall_prots[cell]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_6hr_pipeline=filtering_pipeline(final_6_hr_data,cell_lines,data_6hr['control_data_by_cell_line'],'6hr',data_6hr['control_data_by_cell_line_coeffvar'],**config)\n",
    "dummy_6hr_pipeline.final_filtered_data=final_6_hr_data\n",
    "dummy_6hr_pipeline.run_loo_regression(ycol='meta_Inhi_5',model=elasticnet_model)\n",
    "\n",
    "dummy_24hr_pipeline=filtering_pipeline(final_24_hr_data,cell_lines,data_24hr['control_data_by_cell_line'],'24hr',data_24hr['control_data_by_cell_line_coeffvar'],**config)\n",
    "dummy_24hr_pipeline.final_filtered_data=final_24_hr_data\n",
    "dummy_24hr_pipeline.run_loo_regression(ycol='meta_Inhi_5',model=elasticnet_model)\n",
    "\n",
    "dummy_48hr_pipeline=filtering_pipeline(final_48_hr_data,cell_lines,data_48hr['control_data_by_cell_line'],'48hr',data_48hr['control_data_by_cell_line_coeffvar'],**config)\n",
    "dummy_48hr_pipeline.final_filtered_data=final_48_hr_data\n",
    "dummy_48hr_pipeline.run_loo_regression(ycol='meta_Inhi_5',model=elasticnet_model)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting triple union and intersection of all proteins across each step of the pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "\n",
    "# Pull the dict of each timepoint:\n",
    "dict_6hr = six_hr_non_tgt_pipeline.outputs_from_steps\n",
    "dict_24hr = twenty_four_hr_non_tgt_pipeline.outputs_from_steps\n",
    "dict_48hr = forty_eight_hr_non_tgt_pipeline.outputs_from_steps\n",
    "\n",
    "# Collect intersection and union counts at each step for each cell line\n",
    "intersection_data = []\n",
    "union_data = []\n",
    "\n",
    "for step in dict_6hr.keys():\n",
    "    for cell_line in dict_6hr[step]['filtered_data'].keys():\n",
    "        data6hr_cell = dict_6hr[step]['filtered_data'][cell_line]\n",
    "        data24hr_cell = dict_24hr[step]['filtered_data'][cell_line]\n",
    "        data48hr_cell = dict_48hr[step]['filtered_data'][cell_line]\n",
    "\n",
    "        set6 = set(data6hr_cell.columns)\n",
    "        set24 = set(data24hr_cell.columns)\n",
    "        set48 = set(data48hr_cell.columns)\n",
    "\n",
    "        triple_intersection = set6 & set24 & set48\n",
    "        triple_union = set6 | set24 | set48\n",
    "\n",
    "        intersection_data.append({\n",
    "            'Step': step,\n",
    "            'Cell Line': cell_line,\n",
    "            'Intersection': len(triple_intersection)\n",
    "        })\n",
    "        union_data.append({\n",
    "            'Step': step,\n",
    "            'Cell Line': cell_line,\n",
    "            'Union': len(triple_union)\n",
    "        })\n",
    "\n",
    "# Convert to DataFrames for easier plotting\n",
    "intersection_df = pd.DataFrame(intersection_data)\n",
    "union_df = pd.DataFrame(union_data)\n",
    "\n",
    "# To ensure steps are in the correct order, extract step number from step name\n",
    "def extract_step_num(step_name):\n",
    "    # Assumes step_name like 'step_1_filtername_param'\n",
    "    try:\n",
    "        return int(step_name.split('_')[1])\n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "intersection_df['StepNum'] = intersection_df['Step'].apply(extract_step_num)\n",
    "union_df['StepNum'] = union_df['Step'].apply(extract_step_num)\n",
    "\n",
    "intersection_df = intersection_df.sort_values(['Cell Line', 'StepNum'])\n",
    "union_df = union_df.sort_values(['Cell Line', 'StepNum'])\n",
    "\n",
    "# Prepare x-tick labels: use full step names (no abbreviation)\n",
    "step_order = sorted(intersection_df['StepNum'].unique())\n",
    "step_labels = []\n",
    "for s in step_order:\n",
    "    # Get the first step name for this step number\n",
    "    step_name = intersection_df[intersection_df['StepNum'] == s]['Step'].iloc[0]\n",
    "    # Use the full step name as the label\n",
    "    label = step_name\n",
    "    step_labels.append(label)\n",
    "\n",
    "# Assign two tones of the same color for each cell line\n",
    "cell_lines = sorted(intersection_df['Cell Line'].unique())\n",
    "n_cell_lines = len(cell_lines)\n",
    "# Use a colormap (e.g., tab10 or Set2) and make a lighter and darker version for each cell line\n",
    "base_cmap = plt.get_cmap('tab10')\n",
    "color_dict = {}\n",
    "for idx, cell_line in enumerate(cell_lines):\n",
    "    base_color = mpl.colors.to_rgb(base_cmap(idx % 10))\n",
    "    # Darker for intersection, lighter for union\n",
    "    color_dict[cell_line] = {\n",
    "        'intersection': base_color,\n",
    "        'union': tuple([min(1, c + 0.5*(1-c)) for c in base_color])  # lighten by blending with white\n",
    "    }\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "for cell_line in cell_lines:\n",
    "    cell_df_int = intersection_df[intersection_df['Cell Line'] == cell_line]\n",
    "    cell_df_union = union_df[union_df['Cell Line'] == cell_line]\n",
    "    # Plot intersection (darker)\n",
    "    plt.plot(\n",
    "        cell_df_int['StepNum'], cell_df_int['Intersection'],\n",
    "        marker='o', label=f\"{cell_line} (∩)\", color=color_dict[cell_line]['intersection'], linewidth=2\n",
    "    )\n",
    "    # Plot union (lighter, dashed)\n",
    "    plt.plot(\n",
    "        cell_df_union['StepNum'], cell_df_union['Union'],\n",
    "        marker='s', label=f\"{cell_line} (∪)\", color=color_dict[cell_line]['union'], linestyle='--', linewidth=2\n",
    "    )\n",
    "\n",
    "plt.xlabel('Pipeline Step')\n",
    "plt.ylabel('Number of Proteins')\n",
    "plt.title('Intersection (∩) and Union (∪) of Proteins Across Timepoints at Each Filtering Step (Non-Targeted)')\n",
    "plt.legend(title='Cell Line', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.xticks(step_order, step_labels, rotation=30, ha='right')\n",
    "plt.grid(True, which='both', axis='both', linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the distribution of the 6,24, and 48hr data before and after filtering:\n",
    "\"\"\"\n",
    "Organization of filter pipeline class\n",
    ".outputs from steps {dict}\n",
    "    indexed by step name; step_#_filtername_param; step_0_original_data, step_2_log2_transform_by_control__\n",
    "    returns dict for each step with 4 elements:\n",
    "        filtered_data {dict}\n",
    "             indexed by cell lines HS578T, BT549, etc.\n",
    "             returns pd.DataFrame with proteins as columns and experiments as rows\n",
    "        graphing_dict_before {dict}\n",
    "            indexed by cell lines HS578T, BT549, etc.\n",
    "            returns pd.DataFrame with proteins as columns and experiments as rows\n",
    "        graphing_dict_after {dict}\n",
    "            indexed by cell lines HS578T, BT549, etc.\n",
    "            returns pd.DataFrame with proteins as columns and experiments as rows\n",
    "        step_dict {dict}\n",
    "            indexed by step name; step_#_filtername_param; step_0_original_data, step_2_log2_transform_by_control__\n",
    "            returns dict with 4 elements:\n",
    "\"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after log transform and before filtering:\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "cell_list = list(combined_6hr_dict_log.keys())\n",
    "n_cells = len(cell_list)\n",
    "n_cols = 3\n",
    "n_rows = int(np.ceil(n_cells / n_cols))\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(5 * n_cols, 4 * n_rows), squeeze=False)\n",
    "timepoints = ['6hr', '24hr', '48hr']\n",
    "colors = ['#1f77b4', '#d62728', '#2ca02c']\n",
    "alpha = 0.5\n",
    "\n",
    "for idx, cell in enumerate(cell_list):\n",
    "    row = idx // n_cols\n",
    "    col = idx % n_cols\n",
    "    ax = axes[row, col]\n",
    "\n",
    "    # Drop columns matching regex 'meta_' for each timepoint\n",
    "    data_6hr_cell = combined_6hr_dict[cell].drop(columns=combined_6hr_dict[cell].filter(regex='meta_').columns)\n",
    "    data_24hr_cell = combined_24hr_dict[cell].drop(columns=combined_24hr_dict[cell].filter(regex='meta_').columns)\n",
    "    data_48hr_cell = combined_48hr_dict[cell].drop(columns=combined_48hr_dict[cell].filter(regex='meta_').columns)\n",
    "\n",
    "\n",
    "    #for after filtering: keep only intersection of all proteins across all timepoints:\n",
    "    intersection_prots=list(set(data_6hr_cell.columns).intersection(set(data_24hr_cell.columns)).intersection(set(data_48hr_cell.columns)))\n",
    "    data_6hr_cell=data_6hr_cell[intersection_prots]\n",
    "    data_24hr_cell=data_24hr_cell[intersection_prots]\n",
    "    data_48hr_cell=data_48hr_cell[intersection_prots]\n",
    "\n",
    "\n",
    "    data6minus24=data_6hr_cell.reset_index()-data_24hr_cell.reset_index()\n",
    "    data24minus48=data_24hr_cell.reset_index()-data_48hr_cell.reset_index()\n",
    "\n",
    "    data6minus24.drop(columns='index',inplace=True)\n",
    "    data24minus48.drop(columns='index',inplace=True)\n",
    "  \n",
    "\n",
    "    # Flatten to 1D arrays, drop NaNs\n",
    "    vals_6hr = data_6hr_cell.values.flatten()\n",
    "    vals_6hr = vals_6hr[~np.isnan(vals_6hr)]\n",
    "    vals_24hr = data_24hr_cell.values.flatten()\n",
    "    vals_24hr = vals_24hr[~np.isnan(vals_24hr)]\n",
    "    vals_48hr = data_48hr_cell.values.flatten()\n",
    "    vals_48hr = vals_48hr[~np.isnan(vals_48hr)]\n",
    "\n",
    "\n",
    "    vals_6minus24 = data6minus24.values.flatten()\n",
    "    vals_6minus24 = vals_6minus24[~np.isnan(vals_6minus24)]\n",
    "    vals_24minus48 = data24minus48.values.flatten()\n",
    "    vals_24minus48 = vals_24minus48[~np.isnan(vals_24minus48)]\n",
    "    \n",
    "    # # Plot PDF (normalized histogram) for each timepoint\n",
    "    ax.hist(vals_6hr, bins=40, density=True, alpha=alpha, color=colors[0], label='6hr')\n",
    "    # ax.hist(vals_24hr, bins=40, density=True, alpha=alpha, color=colors[1], label='24hr')\n",
    "    # ax.hist(vals_48hr, bins=40, density=True, alpha=alpha, color=colors[2], label='48hr')\n",
    "\n",
    "\n",
    "    ax.hist(vals_6minus24, bins=40, density=True, alpha=alpha, color=colors[1], label='6hr minus 24hr')\n",
    "    ax.hist(vals_24minus48, bins=40, density=True, alpha=alpha, color=colors[2], label='24hr minus48hr')\n",
    "\n",
    "    \n",
    "    ax.set_title(f\"{cell}\")\n",
    "    ax.set_xlabel(\"Log2(Intensity)\")\n",
    "    ax.set_ylabel(\"Probability Density\")\n",
    "    ax.legend()\n",
    "    ax.grid(True, linestyle='--', alpha=0.4)\n",
    "\n",
    "# Hide any unused subplots\n",
    "for idx in range(n_cells, n_rows * n_cols):\n",
    "    fig.delaxes(axes[idx // n_cols, idx % n_cols])\n",
    "\n",
    "fig.suptitle(\"PDF of Log2 Intensities for Each Cell Line and Timepoint (After filtering, intersection only proteins)\", fontsize=16, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate overlap statistics for targeted, untargeted, and all proteins\n",
    "import pandas as pd\n",
    "\n",
    "def get_protein_columns(df):\n",
    "    \"\"\"Return set of columns that are not metadata (do not start with 'meta_').\"\"\"\n",
    "    return set([col for col in df.columns if not col.startswith('meta_')])\n",
    "\n",
    "stats_tgt = []\n",
    "stats_nontgt = []\n",
    "stats_all = []\n",
    "\n",
    "for cell in cell_lines:\n",
    "    # Targeted proteins (remove metadata columns)\n",
    "    prots_6_tgt = get_protein_columns(six_hr_tgt_pipeline.final_filtered_data[cell])\n",
    "    prots_24_tgt = get_protein_columns(twenty_four_hr_tgt_pipeline.final_filtered_data[cell])\n",
    "    prots_48_tgt = get_protein_columns(forty_eight_hr_tgt_pipeline.final_filtered_data[cell])\n",
    "    \n",
    "    # Untargeted proteins (remove metadata columns)\n",
    "    prots_6_nontgt = get_protein_columns(six_hr_non_tgt_pipeline.final_filtered_data[cell])\n",
    "    prots_24_nontgt = get_protein_columns(twenty_four_hr_non_tgt_pipeline.final_filtered_data[cell])\n",
    "    prots_48_nontgt = get_protein_columns(forty_eight_hr_non_tgt_pipeline.final_filtered_data[cell])\n",
    "    \n",
    "    # All proteins (union of targeted and untargeted, after removing metadata)\n",
    "    prots_6_all = prots_6_tgt | prots_6_nontgt\n",
    "    prots_24_all = prots_24_tgt | prots_24_nontgt\n",
    "    prots_48_all = prots_48_tgt | prots_48_nontgt\n",
    "\n",
    "    # Targeted stats\n",
    "    stats_tgt.append({\n",
    "        'Cell Line': cell,\n",
    "        '6hr': len(prots_6_tgt),\n",
    "        '24hr': len(prots_24_tgt),\n",
    "        '48hr': len(prots_48_tgt),\n",
    "        '6hr∩24hr': len(prots_6_tgt & prots_24_tgt),\n",
    "        '6hr∩48hr': len(prots_6_tgt & prots_48_tgt),\n",
    "        '24hr∩48hr': len(prots_24_tgt & prots_48_tgt),\n",
    "        '6hr∩24hr∩48hr': len(prots_6_tgt & prots_24_tgt & prots_48_tgt)\n",
    "    })\n",
    "    # Untargeted stats\n",
    "    stats_nontgt.append({\n",
    "        'Cell Line': cell,\n",
    "        '6hr': len(prots_6_nontgt),\n",
    "        '24hr': len(prots_24_nontgt),\n",
    "        '48hr': len(prots_48_nontgt),\n",
    "        '6hr∩24hr': len(prots_6_nontgt & prots_24_nontgt),\n",
    "        '6hr∩48hr': len(prots_6_nontgt & prots_48_nontgt),\n",
    "        '24hr∩48hr': len(prots_24_nontgt & prots_48_nontgt),\n",
    "        '6hr∩24hr∩48hr': len(prots_6_nontgt & prots_24_nontgt & prots_48_nontgt)\n",
    "    })\n",
    "    # All proteins stats\n",
    "    stats_all.append({\n",
    "        'Cell Line': cell,\n",
    "        '6hr': len(prots_6_all),\n",
    "        '24hr': len(prots_24_all),\n",
    "        '48hr': len(prots_48_all),\n",
    "        '6hr∩24hr': len(prots_6_all & prots_24_all),\n",
    "        '6hr∩48hr': len(prots_6_all & prots_48_all),\n",
    "        '24hr∩48hr': len(prots_24_all & prots_48_all),\n",
    "        '6hr∩24hr∩48hr': len(prots_6_all & prots_24_all & prots_48_all)\n",
    "    })\n",
    "\n",
    "# Display tables\n",
    "print(\"Targeted Protein Overlap:\")\n",
    "display(pd.DataFrame(stats_tgt))\n",
    "print(\"Untargeted Protein Overlap:\")\n",
    "display(pd.DataFrame(stats_nontgt))\n",
    "print(\"All Proteins Overlap:\")\n",
    "display(pd.DataFrame(stats_all))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Legacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtering all timepoints together p1\n",
    "overall_non_tgt_data={}\n",
    "overall_tgt_data={}\n",
    "for cell in cell_lines:\n",
    "    overall_non_tgt_data[cell]=pd.concat([non_targeted_prots_raw_6hr[cell],non_targeted_prots_raw_24hr[cell],non_targeted_prots_raw_48hr[cell]],axis=0)\n",
    "    overall_tgt_data[cell]=pd.concat([targeted_prots_raw_6hr[cell],targeted_prots_raw_24hr[cell],targeted_prots_raw_48hr[cell]],axis=0)\n",
    "\n",
    "    # Targeted stats\n",
    "    stats_tgt.append({\n",
    "        'Cell Line': cell,\n",
    "        '6hr': len(prots_6_tgt),\n",
    "        '24hr': len(prots_24_tgt),\n",
    "        '48hr': len(prots_48_tgt),\n",
    "        '6hr∩24hr': len(prots_6_tgt & prots_24_tgt),\n",
    "        '6hr∩48hr': len(prots_6_tgt & prots_48_tgt),\n",
    "        '24hr∩48hr': len(prots_24_tgt & prots_48_tgt),\n",
    "        '6hr∩24hr∩48hr': len(prots_6_tgt & prots_24_tgt & prots_48_tgt)\n",
    "    })\n",
    "    # Untargeted stats\n",
    "    stats_nontgt.append({\n",
    "        'Cell Line': cell,\n",
    "        '6hr': len(prots_6_nontgt),\n",
    "        '24hr': len(prots_24_nontgt),\n",
    "        '48hr': len(prots_48_nontgt),\n",
    "        '6hr∩24hr': len(prots_6_nontgt & prots_24_nontgt),\n",
    "        '6hr∩48hr': len(prots_6_nontgt & prots_48_nontgt),\n",
    "        '24hr∩48hr': len(prots_24_nontgt & prots_48_nontgt),\n",
    "        '6hr∩24hr∩48hr': len(prots_6_nontgt & prots_24_nontgt & prots_48_nontgt)\n",
    "    })\n",
    "    # All proteins stats\n",
    "    stats_all.append({\n",
    "        'Cell Line': cell,\n",
    "        '6hr': len(prots_6_all),\n",
    "        '24hr': len(prots_24_all),\n",
    "        '48hr': len(prots_48_all),\n",
    "        '6hr∩24hr': len(prots_6_all & prots_24_all),\n",
    "        '6hr∩48hr': len(prots_6_all & prots_48_all),\n",
    "        '24hr∩48hr': len(prots_24_all & prots_48_all),\n",
    "        '6hr∩24hr∩48hr': len(prots_6_all & prots_24_all & prots_48_all)\n",
    "    })\n",
    "\n",
    "overall_non_tgt_pipeline=filtering_pipeline(overall_non_tgt_data,cell_lines,data_6hr['control_data_by_cell_line'],'overall',data_6hr['control_data_by_cell_line_coeffvar'],**config)\n",
    "overall_tgt_pipeline=filtering_pipeline(overall_tgt_data,cell_lines,data_6hr['control_data_by_cell_line'],'overall',data_6hr['control_data_by_cell_line_coeffvar'],**config)\n",
    "\n",
    "overall_tgt_pipeline.run_pipeline(pipeline_steps_tgt,save_dir=saved_filter_dir,tgt='tgt')\n",
    "overall_tgt_pipeline.summary_of_pipeline()\n",
    "overall_tgt_pipeline.save_pipeline(saved_filter_dir,tgt='tgt')\n",
    "\n",
    "\n",
    "overall_non_tgt_pipeline.run_pipeline(pipeline_steps_nontgt,save_dir=saved_filter_dir,tgt='nontgt')\n",
    "overall_non_tgt_pipeline.summary_of_pipeline()\n",
    "overall_non_tgt_pipeline.save_pipeline(saved_filter_dir,tgt='nontgt')\n",
    "\n",
    "#splitting the overall pipeline after filtering into 6,24,and 48hrs to see if it can still elasticnet predict\n",
    "#selecting the 6hr data\n",
    "_6hr_data={}\n",
    "_24hr_data={}\n",
    "_48hr_data={}\n",
    "for cell in cell_lines:\n",
    "    _6hr_data[cell]=overall_non_tgt_pipeline.final_filtered_data[cell].loc[overall_non_tgt_pipeline.final_filtered_data[cell]['meta_pert_time_x']==6.0]\n",
    "    _24hr_data[cell]=overall_non_tgt_pipeline.final_filtered_data[cell].loc[overall_non_tgt_pipeline.final_filtered_data[cell]['meta_pert_time_x']==24.0]\n",
    "    _48hr_data[cell]=overall_non_tgt_pipeline.final_filtered_data[cell].loc[overall_non_tgt_pipeline.final_filtered_data[cell]['meta_pert_time_x']==48.0]\n",
    "\n",
    "dummy_6_hr=filtering_pipeline(_6hr_data,cell_lines,data_6hr['control_data_by_cell_line'],'6hr',data_6hr['control_data_by_cell_line_coeffvar'],**config)\n",
    "dummy_24_hr=filtering_pipeline(_24hr_data,cell_lines,data_6hr['control_data_by_cell_line'],'24hr',data_6hr['control_data_by_cell_line_coeffvar'],**config)\n",
    "dummy_48_hr=filtering_pipeline(_48hr_data,cell_lines,data_6hr['control_data_by_cell_line'],'48hr',data_6hr['control_data_by_cell_line_coeffvar'],**config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtering all timepoints together p2\n",
    "hash=overall_non_tgt_pipeline.pipeline_hash\n",
    "\n",
    "dummy_6_hr.final_filtered_data=_6hr_data\n",
    "dummy_6_hr.pipeline_hash=hash\n",
    "dummy_6_hr.run_loo_regression(ycol='meta_Inhi_5',model=elasticnet_model)\n",
    "\n",
    "\n",
    "dummy_24_hr.final_filtered_data=_24hr_data\n",
    "dummy_24_hr.pipeline_hash=hash\n",
    "dummy_24_hr.run_loo_regression(ycol='meta_Inhi_5',model=elasticnet_model)\n",
    "\n",
    "\n",
    "dummy_48_hr.final_filtered_data=_48hr_data\n",
    "dummy_48_hr.pipeline_hash=hash\n",
    "dummy_48_hr.run_loo_regression(ycol='meta_Inhi_5',model=elasticnet_model)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# legacy timepoint analysis\n",
    "# Calculate Mutual Information between timepoints for each protein in each cell line\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "# Prepare to collect Mutual Information values per cell line and timepoint pair\n",
    "mi_by_cell = {\n",
    "    '6_24': {},\n",
    "    '24_48': {},\n",
    "    '6_48': {}\n",
    "}\n",
    "\n",
    "cell_lines = data_6hr['cell_lines']\n",
    "n_cells = len(cell_lines)\n",
    "n_cols = 3\n",
    "n_rows = int(np.ceil(n_cells / n_cols))\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(5 * n_cols, 4 * n_rows), squeeze=False)\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Color and label settings for each comparison\n",
    "mi_pairs = [\n",
    "    ('6_24', log2_transform_6hr, log2_transform_24hr, '#1f77b4', '6hr vs 24hr'),\n",
    "    ('24_48', log2_transform_24hr, log2_transform_48hr, '#d62728', '24hr vs 48hr'),\n",
    "    ('6_48', log2_transform_6hr, log2_transform_48hr, '#2ca02c', '6hr vs 48hr')\n",
    "]\n",
    "\n",
    "for idx, cell in enumerate(cell_lines):\n",
    "    ax = axes[idx]\n",
    "    for pair_key, df_a_dict, df_b_dict, color, label in mi_pairs:\n",
    "        df_a = df_a_dict[cell]\n",
    "        df_b = df_b_dict[cell]\n",
    "        # Find common protein columns (excluding meta columns)\n",
    "        prot_cols_a = [col for col in df_a.columns if not col.startswith('meta_')]\n",
    "        prot_cols_b = [col for col in df_b.columns if not col.startswith('meta_')]\n",
    "        common_prots = sorted(list(set(prot_cols_a) & set(prot_cols_b)))\n",
    "\n",
    "        print(f\"Cell line: {cell} | {label}\")\n",
    "        print(f\"  Number of proteins at timepoint A: {len(prot_cols_a)}\")\n",
    "        print(f\"  Number of proteins at timepoint B: {len(prot_cols_b)}\")\n",
    "        print(f\"  Number of common proteins: {len(common_prots)}\")\n",
    "        if not common_prots:\n",
    "            mi_by_cell[pair_key][cell] = []\n",
    "            continue\n",
    "\n",
    "        # For each protein, compute mutual information between timepoints across all experiments\n",
    "        mi_scores = []\n",
    "        for prot in common_prots:\n",
    "            x = df_a[prot].values\n",
    "            y = df_b[prot].values\n",
    "            # Only keep pairs where both are finite\n",
    "            mask = np.isfinite(x) & np.isfinite(y)\n",
    "            if np.sum(mask) > 1:\n",
    "                # Reshape for sklearn\n",
    "                x_masked = x[mask].reshape(-1, 1)\n",
    "                y_masked = y[mask]\n",
    "                # mutual_info_regression expects 2D X and 1D y\n",
    "                try:\n",
    "                    mi = mutual_info_regression(x_masked, y_masked, random_state=0)\n",
    "                    if np.isfinite(mi[0]):\n",
    "                        mi_scores.append(mi[0])\n",
    "                except Exception as e:\n",
    "                    # If MI fails (e.g., constant input), skip\n",
    "                    pass\n",
    "        mi_by_cell[pair_key][cell] = mi_scores\n",
    "\n",
    "        # Plot histogram for this cell line and timepoint pair\n",
    "        ax.hist(\n",
    "            mi_scores,\n",
    "            bins=30,\n",
    "            color=color,\n",
    "            alpha=0.45,\n",
    "            edgecolor='black',\n",
    "            label=label\n",
    "        )\n",
    "\n",
    "    ax.set_xlabel('Mutual Information')\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.set_title(f'{cell}\\nHistogram of Mutual Information (per protein)')\n",
    "    ax.grid(alpha=0.2)\n",
    "    ax.legend()\n",
    "\n",
    "# Hide unused subplots\n",
    "for j in range(len(cell_lines), n_rows * n_cols):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is legacy code for the 6,24,48hr histgram data;\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.colors import to_rgba\n",
    "\n",
    "# Prepare data and cell lines\n",
    "cell_lines = data_48hr['cell_lines']\n",
    "\n",
    "# Use more distinct, custom colors and higher transparency for overlap clarity\n",
    "timepoint_colors = {\n",
    "    '6hr': to_rgba('#1f77b4', alpha=0.35),    # blue, more transparent\n",
    "    '24hr': to_rgba('#d62728', alpha=0.35),   # red, more transparent\n",
    "    '48hr': to_rgba('#2ca02c', alpha=0.35)    # green, more transparent\n",
    "}\n",
    "timepoint_labels = {\n",
    "    '6hr': '6 hr',\n",
    "    '24hr': '24 hr',\n",
    "    '48hr': '48 hr'\n",
    "}\n",
    "timepoint_data = {\n",
    "    '6hr': log2_transform_6hr,\n",
    "    '24hr': log2_transform_24hr,\n",
    "    '48hr': log2_transform_48hr\n",
    "}\n",
    "\n",
    "n_cols = 3\n",
    "n_rows = int(np.ceil(len(cell_lines) / n_cols))\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(5 * n_cols, 4 * n_rows), squeeze=False)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, cell in enumerate(cell_lines):\n",
    "    ax = axes[idx]\n",
    "    for tp, color in timepoint_colors.items():\n",
    "        # Drop meta columns and flatten values\n",
    "        meta_cols = timepoint_data[tp][cell].filter(regex='meta_').columns\n",
    "        vals = timepoint_data[tp][cell].drop(columns=meta_cols).select_dtypes(include=[float, int]).values.flatten()\n",
    "        vals = vals[~np.isnan(vals)]\n",
    "        ax.hist(\n",
    "            vals,\n",
    "            bins=100,\n",
    "            alpha=.33,  # alpha handled in color\n",
    "            color=color,\n",
    "            label=timepoint_labels[tp],\n",
    "            log=False,\n",
    "            edgecolor='none'\n",
    "        )\n",
    "    ax.set_title(f\"{cell} - Data Value Distribution\")\n",
    "    ax.set_xlabel(\"log2(fold change)\")\n",
    "    ax.set_ylabel(\"Count (log scale)\")\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Hide unused axes\n",
    "for j in range(len(cell_lines), n_rows * n_cols):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot post-filtering 6,24,48hr probability density functions (PDFs) using the combined data_dicts:\n",
    "# The dictionaries of data are the combined dicts:\n",
    "# combined_6hr_dict, combined_24hr_dict, combined_48hr_dict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.colors import to_rgba\n",
    "\n",
    "# Prepare data and cell lines\n",
    "cell_lines = list(combined_6hr_dict.keys())\n",
    "\n",
    "# Use more distinct, custom colors and higher transparency for overlap clarity\n",
    "timepoint_colors = {\n",
    "    '6hr': to_rgba('#1f77b4', alpha=0.35),    # blue, more transparent\n",
    "    '24hr': to_rgba('#d62728', alpha=0.35),   # red, more transparent\n",
    "    '48hr': to_rgba('#2ca02c', alpha=0.35)    # green, more transparent\n",
    "}\n",
    "timepoint_labels = {\n",
    "    '6hr': '6 hr',\n",
    "    '24hr': '24 hr',\n",
    "    '48hr': '48 hr'\n",
    "}\n",
    "timepoint_data = {\n",
    "    '6hr': combined_6hr_dict,\n",
    "    '24hr': combined_24hr_dict,\n",
    "    '48hr': combined_48hr_dict\n",
    "}\n",
    "\n",
    "n_cols = 3\n",
    "n_rows = int(np.ceil(len(cell_lines) / n_cols))\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(5 * n_cols, 4 * n_rows), squeeze=False)\n",
    "axes = axes.flatten()\n",
    "\n",
    "# To ensure all PDFs are on the same x-axis, find global min/max\n",
    "all_vals = []\n",
    "for tp in timepoint_data:\n",
    "    for cell in cell_lines:\n",
    "        df = timepoint_data[tp][cell]\n",
    "        meta_cols = df.filter(regex='meta_').columns\n",
    "        vals = df.drop(columns=meta_cols).select_dtypes(include=[float, int]).values.flatten()\n",
    "        vals = vals[~np.isnan(vals)]\n",
    "        all_vals.append(vals)\n",
    "all_vals = np.concatenate(all_vals)\n",
    "x_min, x_max = np.min(all_vals), np.max(all_vals)\n",
    "bins = np.linspace(x_min, x_max, 101)  # 100 bins\n",
    "\n",
    "#plotting\n",
    "for idx, cell in enumerate(cell_lines):\n",
    "    ax = axes[idx]\n",
    "    for tp, color in timepoint_colors.items():\n",
    "        # Drop meta columns and flatten values\n",
    "        df = timepoint_data[tp][cell]\n",
    "        meta_cols = df.filter(regex='meta_').columns\n",
    "        vals = df.drop(columns=meta_cols).select_dtypes(include=[float, int]).values.flatten()\n",
    "        vals = vals[~np.isnan(vals)]\n",
    "        # Plot PDF: density=True ensures area under curve sums to 1\n",
    "        ax.hist(\n",
    "            vals,\n",
    "            bins=bins,\n",
    "            alpha=.33,  # alpha handled in color\n",
    "            color=color,\n",
    "            label=timepoint_labels[tp],\n",
    "            log=False,\n",
    "            edgecolor='none',\n",
    "            density=True\n",
    "        )\n",
    "    ax.set_title(f\"{cell} - final filtered data (PDF)\")\n",
    "    ax.set_xlabel(\"log2(fold change)\")\n",
    "    ax.set_ylabel(\"Probability Density\")\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Hide unused axes\n",
    "for j in range(len(cell_lines), n_rows * n_cols):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histograms (not PDFs) of triple-overlap proteins for 6, 24, and 48 hr timepoints\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import to_rgba\n",
    "\n",
    "def get_protein_columns(df):\n",
    "    \"\"\"Return set of columns that are not metadata (do not start with 'meta_').\"\"\"\n",
    "    return set([col for col in df.columns if not col.startswith('meta_')])\n",
    "\n",
    "# Prepare data and cell lines\n",
    "cell_lines = list(combined_6hr_dict.keys())\n",
    "\n",
    "# For each cell line, find the triple-overlap proteins (present in all 3 timepoints)\n",
    "triple_overlap_proteins = {}\n",
    "for cell in cell_lines:\n",
    "    prots_6 = get_protein_columns(combined_6hr_dict[cell])\n",
    "    prots_24 = get_protein_columns(combined_24hr_dict[cell])\n",
    "    prots_48 = get_protein_columns(combined_48hr_dict[cell])\n",
    "    triple_overlap_proteins[cell] = prots_6 & prots_24 & prots_48\n",
    "\n",
    "# Set up colors and labels for timepoints\n",
    "timepoint_colors = {\n",
    "    '6hr': to_rgba('#1f77b4', alpha=0.6),    # blue\n",
    "    '24hr': to_rgba('#d62728', alpha=0.6),   # red\n",
    "    '48hr': to_rgba('#2ca02c', alpha=0.6)    # green\n",
    "}\n",
    "timepoint_labels = {\n",
    "    '6hr': '6 hr',\n",
    "    '24hr': '24 hr',\n",
    "    '48hr': '48 hr'\n",
    "}\n",
    "timepoint_data = {\n",
    "    '6hr': combined_6hr_dict,\n",
    "    '24hr': combined_24hr_dict,\n",
    "    '48hr': combined_48hr_dict\n",
    "}\n",
    "\n",
    "# For consistent binning, collect all values for triple-overlap proteins across all cell lines and timepoints\n",
    "all_vals = []\n",
    "for tp in ['6hr', '24hr', '48hr']:\n",
    "    for cell in cell_lines:\n",
    "        prots = triple_overlap_proteins[cell]\n",
    "        if len(prots) == 0:\n",
    "            continue\n",
    "        df = timepoint_data[tp][cell]\n",
    "        vals = df[list(prots)].select_dtypes(include=[float, int]).values.flatten()\n",
    "        vals = vals[~np.isnan(vals)]\n",
    "        all_vals.append(vals)\n",
    "if len(all_vals) > 0:\n",
    "    all_vals = np.concatenate(all_vals)\n",
    "    x_min, x_max = np.min(all_vals), np.max(all_vals)\n",
    "else:\n",
    "    x_min, x_max = -1, 1  # fallback\n",
    "\n",
    "bins = np.linspace(x_min, x_max, 101)  # 100 bins\n",
    "\n",
    "# Plot histograms for each cell line\n",
    "n_cols = 3\n",
    "n_rows = int(np.ceil(len(cell_lines) / n_cols))\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(5 * n_cols, 4 * n_rows), squeeze=False)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, cell in enumerate(cell_lines):\n",
    "    ax = axes[idx]\n",
    "    prots = triple_overlap_proteins[cell]\n",
    "    if len(prots) == 0:\n",
    "        ax.set_title(f\"{cell} (no triple-overlap proteins)\")\n",
    "        ax.axis('off')\n",
    "        continue\n",
    "    for tp, color in timepoint_colors.items():\n",
    "        df = timepoint_data[tp][cell]\n",
    "        vals = df[list(prots)].select_dtypes(include=[float, int]).values.flatten()\n",
    "        vals = vals[~np.isnan(vals)]\n",
    "        ax.hist(\n",
    "            vals,\n",
    "            bins=bins,\n",
    "            alpha=0.6,\n",
    "            color=color,\n",
    "            label=timepoint_labels[tp],\n",
    "            log=False,\n",
    "            edgecolor='none',\n",
    "            density=False  # Not a PDF, just counts\n",
    "        )\n",
    "    ax.set_title(f\"{cell} - triple-overlap proteins (histogram)\")\n",
    "    ax.set_xlabel(\"log2(fold change)\")\n",
    "    ax.set_ylabel(\"Count\")\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Hide unused axes\n",
    "for j in range(len(cell_lines), n_rows * n_cols):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Targeted proteins pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#//////////////////plot the distribution before and after filtering/////////////////\n",
    "config={'graph_type':'hist','bins':100,'meanandstd':True}\n",
    "before=plot_data_value_distribution(step_2_tgtd_prots, cell_lines, **config)\n",
    "after=plot_data_value_distribution(final_filtered_tgtd_prots, cell_lines, **config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#///////////////////////Exploratory analysis/////////////////////\n",
    "\n",
    "\n",
    "#graphing filters by each other, note this is broken for incomplete experiments (since it doesnt operate on proteins) \n",
    "#and for remove outlier since it calculates the threshold internally\n",
    "a_config2 = {\n",
    "    'graph_flag': False,'print_flag': True,'graph_type': 'hist','filter_flag': True, 'verbose':True }\n",
    "kwargs={'xlabel':'mutual information','ylabel':'pearson corr'}\n",
    "fxnargs1={'mi thresh':.001,'threshold':.001,'y_col':'meta_Inhi_5'}\n",
    "fxnargs2={'threshold':.001,'y_col':'meta_Inhi_5'}\n",
    "fxnargs1,fxnargs2={**a_config2,**fxnargs1},{**a_config2,**fxnargs2}\n",
    "x=filterAbyfilterB(filter_by_mutual_information,pearson_corr_filtering,step_6_tgtd_prots,cell_lines,fxnargs1,fxnargs2,**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-targeted proteins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#//////////////////plot the distribution before and after filtering/////////////////\n",
    "config={'graph_type':'hist','bins':100,'meanandstd':True}\n",
    "before=plot_data_value_distribution(step_2_non_tgt, cell_lines, **config)\n",
    "after=plot_data_value_distribution(final_filtered_non_tgt_prots, cell_lines, **config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#///////////////////////Exploratory analysis/////////////////////\n",
    "\n",
    "\n",
    "#graphing filters by each other, note this is broken for incomplete experiments (since it doesnt operate on proteins) \n",
    "#and for remove outlier since it calculates the threshold internally\n",
    "a_config2 = {\n",
    "    'graph_flag': False,'print_flag': True,'graph_type': 'hist','filter_flag': True, 'verbose':True }\n",
    "kwargs={'xlabel':'mutual information','ylabel':'pearson corr'}\n",
    "fxnargs1={'mi_thresh':.05,'threshold':.05,'y_col':'meta_Inhi_5'}\n",
    "fxnargs2={'threshold':.2,'ycol':'meta_Inhi_5'}\n",
    "fxnargs1,fxnargs2={**a_config2,**fxnargs1},{**a_config2,**fxnargs2}\n",
    "x=filterAbyfilterB(filter_by_mutual_information,pearson_corr_filtering,step_6_non_tgt,cell_lines,fxnargs1,fxnargs2,**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ablation study selection use for the last 200ish proteins;\n",
    "#cant use because we seem to lack correlation between the cell viability and the proteins.\n",
    "#have to go back and re-assess the mutual information choice\n",
    "overlaps={}\n",
    "overlaps_full_data={}\n",
    "for i, cell_line in enumerate(cell_lines):\n",
    "    print(f\"Processing cell line: {cell_line}\")\n",
    "    starting_data=final_filtered_non_tgt_prots[cell_line]\n",
    "    nfeatures=int(100-len(final_filtered_tgtd_prots[cell_line].columns))\n",
    "    topx_prots=ablation_study(starting_data,\n",
    "    model='ElasticNet',\n",
    "    n_features_to_select=nfeatures,\n",
    "    target_column='meta_Inhi_5',\n",
    "    verbosity=1\n",
    "    )\n",
    "    overlaps[cell_line],overlaps_full_data[cell_line]=overlapping_features(topx_prots,n_prots_to_keep=nfeatures)\n",
    "    print(\"\\n\\n\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making the final dictionaries of the data:\n",
    "#add in the proteins named in overlaps[cell_line] (and get data from the non-targeted data) and then add in targeted proteins\n",
    "#and then preserve viability data\n",
    "top_100_proteins={}\n",
    "for cell_line in cell_lines:\n",
    "    non_tgts=filtered_non_targeted_prots_by_spearman[cell_line].loc[:,overlaps[cell_line]]\n",
    "    tgts=filtered_targeted_prots[cell_line]\n",
    "    meta_data=meta_preserved_d3[cell_line][['pert_id','trial names']]\n",
    "    top_100_proteins[cell_line]=pd.concat([non_tgts,tgts,meta_data],axis=1)\n",
    "\n",
    "\n",
    "a_test=top_100_proteins[cell_lines[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitive_cell_lines=['HS578T','HCC70','BT549']\n",
    "for cell_line in sensitive_cell_lines:\n",
    "    df = top_100_proteins[cell_line]\n",
    "    output_path = Path(f\"{cell_line}_top_100_data.csv\")\n",
    "    df.to_csv(output_path, sep=\",\")\n",
    "    print(f\"Saved sensitive cell line data for {cell_line} to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histograms of data value distribution and column variance for each sensitive cell line\n",
    "\n",
    "for cell_line in sensitive_cell_lines:\n",
    "    df = top_100_proteins[cell_line].drop(columns=['pert_id','trial names','Inhi_5'])\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "    \n",
    "    # Histogram of all data values\n",
    "    data_values = df.values.flatten()\n",
    "    mean_data = data_values.mean()\n",
    "    axes[0].hist(data_values, bins=100, color=\"#3182bd\", alpha=0.8)\n",
    "    axes[0].axvline(mean_data, color='red', linestyle='dashed', linewidth=2, label=f\"Mean = {mean_data:.2f}\")\n",
    "    axes[0].set_title(f\"{cell_line} - Protein value Distribution\")\n",
    "    axes[0].set_xlabel(\"Protein expression (log2 normalized)\")\n",
    "    axes[0].set_ylabel(\"Count\")\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Histogram of variance of each column (experiment)\n",
    "    col_vars = df.var(axis=0)\n",
    "    mean_var = col_vars.mean()\n",
    "    axes[1].hist(col_vars, bins=50, color=\"#e6550d\", alpha=0.8)\n",
    "    axes[1].axvline(mean_var, color='blue', linestyle='dashed', linewidth=2, label=f\"Mean = {mean_var:.2f}\")\n",
    "    axes[1].set_title(f\"{cell_line} - Protein Variance \")\n",
    "    axes[1].set_xlabel(\"Variance\")\n",
    "    axes[1].set_ylabel(\"Protein count\")\n",
    "    axes[1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making cellbox files for the sensitive cell lines:\n",
    "df=pd.read_csv(\n",
    "    r'C:\\Users\\abdul\\OneDrive - University of Cambridge\\Desktop\\MDRA\\cellbox_torch\\Abdullah_kuziez\\preprocessing\\Intermediate_files\\HS578T_top_100_data.csv'\n",
    ")\n",
    "df.head()\n",
    "\n",
    "pert_ids = df['trial names'].apply(lambda x: x.split('_')[0])\n",
    "# Get the targeted proteins by pulling up all values stored under the pert Ids\n",
    "overall_tgtd_prots = list(pert_ids.map(drug_pert_id_targets_dict).explode().unique())\n",
    "overlaps=list(set(overall_tgtd_prots).intersection(set(df.columns)))\n",
    "\n",
    "acti=df.copy()\n",
    "acti[~acti.columns.isin(overlaps)]=0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Legacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "plt.clf()\n",
    "fig, axes = plt.subplots(len(cell_lines), 2, figsize=(10, 4 * len(cell_lines)))\n",
    "\n",
    "for i, cell_line in enumerate(cell_lines):\n",
    "    print(f\"Processing cell line: {cell_line}\")\n",
    "    df = top_100_proteins[cell_line].copy()\n",
    "    # Drop rows with missing values\n",
    "    df = df.dropna()\n",
    "    # Features: all protein columns (exclude 'pert_id', 'trial names', and target)\n",
    "    feature_cols = [col for col in df.columns if col not in ['pert_id', 'trial names', 'Inhi_5']]\n",
    "    if 'Inhi_5' not in df.columns:\n",
    "        print(f\"Skipping {cell_line}: 'Inhi_5' not found.\")\n",
    "        continue\n",
    "    X = df[feature_cols].values\n",
    "    y = df['Inhi_5'].values\n",
    "\n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Fit ElasticNet\n",
    "    # Scan through different alphas and l1_ratios to find the best combination using grid search\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "    param_grid = {\n",
    "        'alpha': [0.001, 0.01, 0.1, 1, 10],\n",
    "        'l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "    }\n",
    "    base_model = ElasticNet(random_state=42, max_iter=10000)\n",
    "    grid_search = GridSearchCV(base_model, param_grid, cv=3, scoring='r2')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    model = grid_search.best_estimator_\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    # Metrics\n",
    "    r2_train = r2_score(y_train, y_train_pred)\n",
    "    r2_test = r2_score(y_test, y_test_pred)\n",
    "    mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "    mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "    pearson_train = pearsonr(y_train, y_train_pred)[0]\n",
    "    pearson_test = pearsonr(y_test, y_test_pred)[0]\n",
    "\n",
    "    # Plot train\n",
    "    ax_train = axes[i, 0] if len(cell_lines) > 1 else axes[0]\n",
    "    ax_train.scatter(y_train, y_train_pred, alpha=0.7, color=\"#2b8cbe\")\n",
    "    ax_train.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'k--', lw=2)\n",
    "    ax_train.set_title(f\"{cell_line} - Train\\nR2={r2_train:.3f}, Pearson={pearson_train:.3f}, MSE={mse_train:.4f}\")\n",
    "    ax_train.set_xlabel(\"Actual Inhi_5\")\n",
    "    ax_train.set_ylabel(\"Predicted Inhi_5\")\n",
    "\n",
    "    # Plot test\n",
    "    ax_test = axes[i, 1] if len(cell_lines) > 1 else axes[1]\n",
    "    ax_test.scatter(y_test, y_test_pred, alpha=0.7, color=\"#e34a33\")\n",
    "    ax_test.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\n",
    "    ax_test.set_title(f\"{cell_line} - Test\\nR2={r2_test:.3f}, Pearson={pearson_test:.3f}, MSE={mse_test:.4f}\")\n",
    "    ax_test.set_xlabel(\"Actual Inhi_5\")\n",
    "    ax_test.set_ylabel(\"Predicted Inhi_5\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cellbox-3.6-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
