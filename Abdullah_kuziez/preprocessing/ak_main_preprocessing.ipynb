{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This file contains code to conduct the preprocessing for data\n",
    "# and has been functionalized to allow for more flexible data handling\n",
    "# Derived from Elastic_net.ipynb\n",
    "# Key parameters are included below:\n",
    "# 1.Filter by completeness; (check the number of absent entries and filter proteins below a certain threshold)\n",
    "# 5.Apply a signal to noise filter to remove proteins that have a high signal to noise ratio\n",
    "# 2.change expression values into log ratios as compared to the control test\n",
    "# 3.Fill in the missing values using various methods (1st method is to fill in with the mean of the column)\n",
    "# 3.Pull out proteins that are targeted by drugs\n",
    "# 4.Sort proteins according to how much they vary\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "re_run_flag=False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorganize_data(df_raw, meta_data_cols):\n",
    "    \"\"\"\n",
    "    Apply transformations to the raw data including extracting cell viability and reorganizing columns.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_raw : pandas.DataFrame\n",
    "        Raw input dataframe containing all data\n",
    "    meta_data_cols : list\n",
    "        List of column indices for metadata columns\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        Reorganized dataframe with protein data first, then cell viability, then metadata\n",
    "    \"\"\"\n",
    "    # Extract cell viability and organize data columns\n",
    "    cell_viability = df_raw['Cell_viability%_(cck8Drug-blk)/(control-blk)*100']\n",
    "    metadata_cols = df_raw.columns[meta_data_cols]\n",
    "    non_metadata_cols = df_raw.columns.difference(metadata_cols)\n",
    "    non_metadata_cols = non_metadata_cols.difference(['Cell_viability%_(cck8Drug-blk)/(control-blk)*100'])\n",
    "\n",
    "    # Reorganize dataframe with protein data first, then cell viability, then metadata\n",
    "    df_meta_data_at_end = pd.concat([\n",
    "        df_raw[non_metadata_cols],  # Protein expression data\n",
    "        cell_viability,            # Cell viability\n",
    "        df_raw[metadata_cols]      # Metadata columns\n",
    "    ], axis=1)\n",
    "    \n",
    "    return df_meta_data_at_end\n",
    "\n",
    "def separate_metadata(data_df, metadata_indices=None):\n",
    "    \"\"\"\n",
    "    Separates metadata columns from protein expression data based on provided indices.\n",
    "    Expression columns are all columns not in metadata_indices.\n",
    "    \n",
    "    Args:\n",
    "        data_df: DataFrame containing both metadata and protein expression data\n",
    "        metadata_indices: List of column indices for metadata (optional)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (metadata_df, expression_df) containing separated DataFrames\n",
    "    \"\"\"\n",
    "    if metadata_indices is None:\n",
    "        # Fallback to original behavior if no indices provided\n",
    "        metadata_cols = data_df.select_dtypes(include=['object', 'bool']).columns\n",
    "        expression_cols = data_df.select_dtypes(include=['float64', 'int64']).columns\n",
    "    else:\n",
    "        # Use provided indices to split columns\n",
    "        metadata_cols = data_df.columns[metadata_indices]\n",
    "        expression_cols = data_df.columns[~data_df.columns.isin(metadata_cols)]\n",
    "    \n",
    "    # Split the data\n",
    "    metadata_df = data_df[metadata_cols].copy()\n",
    "    expression_df = data_df[expression_cols].copy()\n",
    "    \n",
    "    return metadata_df, expression_df\n",
    "\n",
    "def recombine_data(metadata_df, expression_df):\n",
    "    \"\"\"\n",
    "    Recombines metadata and expression data into a single DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        metadata_df: DataFrame containing metadata\n",
    "        expression_df: DataFrame containing protein expression data\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with recombined data\n",
    "    \"\"\"\n",
    "    # Ensure indices match\n",
    "    if not metadata_df.index.equals(expression_df.index):\n",
    "        raise ValueError(\"Metadata and expression data must have matching indices\")\n",
    "    \n",
    "    # Combine the data, the order ensures that the metadata is at the end\n",
    "    combined_df = pd.concat([expression_df, metadata_df], axis=1)\n",
    "    \n",
    "    return combined_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_completeness(prot_data, prot_info, completeness_threshold=0.8, metadata_indices=None):\n",
    "    # Separate metadata and expression data\n",
    "    meta, expr = separate_metadata(prot_data, metadata_indices)\n",
    "    \n",
    "    # Calculate completeness for each protein column\n",
    "    col_completeness = expr.notna().mean()\n",
    "    complete_proteins = col_completeness[col_completeness >= completeness_threshold].index.tolist()\n",
    "    filtered_expr = expr[complete_proteins]\n",
    "    \n",
    "    # Calculate completeness for each row (sample)\n",
    "    row_completeness = filtered_expr.notna().mean(axis=1)\n",
    "    complete_rows = row_completeness[row_completeness >= completeness_threshold].index.tolist()\n",
    "    filtered_expr = filtered_expr.loc[complete_rows]\n",
    "    filtered_meta = meta.loc[complete_rows]\n",
    "    \n",
    "    # Update protein info to match filtered dataset\n",
    "    filtered_info = prot_info[prot_info['proteins'].isin(complete_proteins)]\n",
    "    \n",
    "    # Recombine metadata and filtered expression data\n",
    "    filtered_data = recombine_data(filtered_meta, filtered_expr)\n",
    "    \n",
    "    print(f\"Filtered from {len(expr.columns)} to {len(filtered_expr.columns)} proteins\")\n",
    "    print(f\"Filtered from {len(expr)} to {len(filtered_expr)} rows\")\n",
    "    print(f\"Completeness threshold: {completeness_threshold}\")\n",
    "    return filtered_data, filtered_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_control_values(prot_data, control_id='control', metadata_indices=None):\n",
    "\n",
    "    # Get control row by finding the row with control pert_id before separating metadata\n",
    "    control_row = prot_data.loc[prot_data['pert_id'] == control_id]\n",
    "    \n",
    "    # Separate metadata and expression data\n",
    "    meta, expr = separate_metadata(prot_data, metadata_indices)\n",
    "    \n",
    "    # Get protein columns (excluding pert_id)\n",
    "    protein_cols = expr.columns\n",
    "    starting_num_proteins = len(protein_cols)\n",
    "    \n",
    "    # Check which columns have values in the control row\n",
    "    valid_columns = []\n",
    "    for col in protein_cols:\n",
    "        if not pd.isna(control_row[col].iloc[0]):\n",
    "            valid_columns.append(col)\n",
    "    # Filter dataset to only include columns that had entries in control row\n",
    "    filtered_expr = expr[valid_columns]\n",
    "    remaining_num_proteins = len(filtered_expr.columns)\n",
    "    \n",
    "    print(f\"Filtered from {starting_num_proteins} to {remaining_num_proteins} proteins\")\n",
    "    \n",
    "    # Recombine with metadata and expression data\n",
    "    filtered_data = recombine_data(meta, filtered_expr)\n",
    "    return filtered_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_cell_viability(df, cell_viability_col='Cell_viability%_(cck8Drug-blk)/(control-blk)*100'):\n",
    "    initial_count = len(df)\n",
    "    filtered_df = df[df[cell_viability_col].notna()]\n",
    "    print(f\"Filtered from {initial_count} to {len(filtered_df)} rows\")\n",
    "    return filtered_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_log_ratios(prot_data, control_id='control',metadata_indices=None,log_fxn=np.log2):\n",
    "    #using logbase2\n",
    "    # Separate metadata and expression data\n",
    "    meta, expr = separate_metadata(prot_data, metadata_indices)\n",
    "    \n",
    "    # Identify numeric columns\n",
    "    numeric_cols = expr.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    # Calculate mean control values for numeric columns only\n",
    "    # Use meta to identify control samples since pert_id is in metadata\n",
    "    control_mask = meta['pert_id'] == control_id\n",
    "    control_means = expr[control_mask][numeric_cols].mean()\n",
    "    \n",
    "    # Calculate log ratios for all samples including control\n",
    "    log_ratios = expr.copy()\n",
    "    \n",
    "    # Calculate log ratios for numeric columns\n",
    "    for col in numeric_cols:\n",
    "        log_ratios[col] = log_fxn(expr[col] / control_means[col])\n",
    "    \n",
    "    print(f\"Converted {len(expr)} samples to log ratios\")\n",
    "    \n",
    "    # Recombine with metadata\n",
    "    return recombine_data(meta, log_ratios)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_signal_to_noise(\n",
    "    df, \n",
    "    signal_to_noise_threshold=0.001, \n",
    "    epsilon=1e-6, \n",
    "    plot=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Filters columns by signal-to-noise ratio (SNR).\n",
    "    Optionally plots the CDF of SNR before and after filtering.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame with metadata and expression data.\n",
    "        signal_to_noise_threshold: Threshold for filtering.\n",
    "        epsilon: Small value to avoid division by zero.\n",
    "        plot: If True, plot SNR CDF before and after filtering.\n",
    "\n",
    "    Returns:\n",
    "        Filtered DataFrame.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    meta, expr = separate_metadata(df)\n",
    "    \n",
    "    means = expr.mean()\n",
    "    stds = expr.std().replace(0, 1e-6)\n",
    "    snr = means.abs() / (stds + epsilon)\n",
    "\n",
    "    keep_cols = list(snr[snr >= signal_to_noise_threshold].index)\n",
    "    \n",
    "    cell_viab_col = 'Cell_viability%_(cck8Drug-blk)/(control-blk)*100'\n",
    "    if cell_viab_col in expr.columns and cell_viab_col not in keep_cols:\n",
    "        keep_cols.append(cell_viab_col)\n",
    "\n",
    "    # Subset expression data\n",
    "    filtered_expr = expr[keep_cols]\n",
    "    filtered_snr = snr[keep_cols] if len(keep_cols) > 0 else snr\n",
    "\n",
    "    print(f\"Filtered proteins by SNR: {expr.shape[1]} -> {filtered_expr.shape[1]}\")\n",
    "\n",
    "    if plot:\n",
    "        # Plot CDF of SNR before and after filtering\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        sorted_snr = np.sort(snr)\n",
    "        cdf = np.arange(1, len(sorted_snr)+1) / len(sorted_snr)\n",
    "        plt.plot(sorted_snr, cdf, label='Before filtering', color='blue')\n",
    "\n",
    "        sorted_filtered_snr = np.sort(filtered_snr)\n",
    "        cdf_filtered = np.arange(1, len(sorted_filtered_snr)+1) / len(sorted_filtered_snr)\n",
    "        plt.plot(sorted_filtered_snr, cdf_filtered, label='After filtering', color='green')\n",
    "\n",
    "        plt.axvline(signal_to_noise_threshold, color='red', linestyle='--', label='Threshold')\n",
    "        plt.xlabel('Signal-to-Noise Ratio (SNR)')\n",
    "        plt.ylabel('CDF')\n",
    "        plt.title('CDF of SNR Before and After Filtering')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "    # Recombine with metadata\n",
    "    filtered_data = recombine_data(meta, filtered_expr)\n",
    "    return filtered_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_signal_to_noise_gen2(df, signal_threshold=2.5,tole=0.001,filtering_to_use=4): #1 is filter 1, 2 is filter 2, 3 is filter 3, 4 is all filters\n",
    "    df_meta, df_expr = separate_metadata(df)\n",
    "\n",
    "#Method 1:\n",
    "#in each iteration of the while loop, we calculate the mean and std of the non-excluded samples\n",
    "#we then mark the samples that are larger than 2.5*std + mean as signal\n",
    "#we then exclude the signal samples from the next iteration\n",
    "#we repeat this process until the mean and std change by less than tole\n",
    "#we then return the indices of the proteins that pass the filter\n",
    "    if filtering_to_use == 1 or filtering_to_use == 4:\n",
    "        \n",
    "        std_old = 0.0\n",
    "        std_new = 0.0\n",
    "\n",
    "        inds = []\n",
    "        signal = np.zeros(df_expr.shape)\n",
    "        excluded = np.ones(df_expr.shape)\n",
    "\n",
    "        while (std_old == 0) or (abs(std_new - std_old) > tole): #this is the convergence criterion, if it changes by less than tole we stop\n",
    "            std_old = std_new\n",
    "            vec = df_expr.to_numpy().flatten()\n",
    "\n",
    "            # Get the samples that are smaller than 2.5*std\n",
    "            mean = np.mean(vec[signal.flatten() == 0])\n",
    "            std_new = np.std(vec[signal.flatten() == 0])\n",
    "            print(f\"mean: {mean:.3f}, std: {std_new:.3f}\")\n",
    "\n",
    "            # Get the samples that are larger than 2.5*std\n",
    "            # 1 in signal means the sample > 2.5*std and vice versa\n",
    "            signal = (np.abs(df_expr.to_numpy()) > 2.5*std_new + mean)*excluded\n",
    "            excluded = excluded * (1-signal)\n",
    "            signal_df = pd.DataFrame(signal, columns=df_expr.columns, index=df_expr.index)\n",
    "            inds.extend(signal_df.any(axis=0)[signal_df.any(axis=0)].index.tolist())\n",
    "\n",
    "        inds_method_1 = list(set(inds))\n",
    "\n",
    "        if filtering_to_use == 1:\n",
    "            print(f\"Filtered {len(inds_method_1)} proteins by method 1\")\n",
    "            return recombine_data(df_meta,df_expr[inds_method_1])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # # Signal-to-noise code, based on two assumptions:\n",
    "    # # - When a data sample is excluded, the whole protein column in which that protein stays in are excluded\n",
    "    # # - Mean and std are calculated using the full matrix\n",
    "    if filtering_to_use == 2 or filtering_to_use == 4:\n",
    "        std_old = 0.0\n",
    "        std_new = 0.0\n",
    "        inds = []\n",
    "        signal = np.zeros(df_expr.shape)\n",
    "        excluded = np.ones(df_expr.shape)\n",
    "\n",
    "        while (std_old == 0) or (abs(std_new - std_old) > tole):\n",
    "            std_old = std_new #update standard dev\n",
    "            vec = df_expr.to_numpy().flatten()\n",
    "\n",
    "            # Get the samples that are smaller than 2.5*std\n",
    "            mean = np.mean(vec[signal.flatten() == 0])\n",
    "            std_new = np.std(vec[signal.flatten() == 0])\n",
    "\n",
    "            # Get the samples that are larger than 2.5*std\n",
    "            # 1 in signal means the sample > 2.5*std and vice versa\n",
    "            signal = (np.abs(df_expr.to_numpy()) > 2.5*std_new + mean)*excluded\n",
    "            signal_df = pd.DataFrame(signal, columns=df_expr.columns, index=df_expr.index)\n",
    "\n",
    "            # ind is a list of indices of columns that have at least one excluded data sample\n",
    "            ind = signal_df.any(axis=0)[signal_df.any(axis=0)].index.tolist()\n",
    "            inds.extend(ind)\n",
    "            int_ind = [signal_df.columns.tolist().index(i) for i in ind]\n",
    "            excluded[:, int_ind] = np.zeros((excluded.shape[0], len(int_ind)))\n",
    "\n",
    "        inds_method_2 = list(set(inds))\n",
    "        if filtering_to_use == 2:\n",
    "            print(f\"Filtered {len(inds_method_2)} proteins by method 2\")\n",
    "            return recombine_data(df_meta,df_expr[inds_method_2])\n",
    "\n",
    "\n",
    "\n",
    "    # # Signal-to-noise code, based on two assumptions:\n",
    "    # # - Only data samples passing the criteria are excluded, not the whole protein column of which they are in\n",
    "    # # - Mean and std are calculated for each perturbation condition\n",
    "\n",
    "    if filtering_to_use == 3 or filtering_to_use == 4:\n",
    "        std_old = np.zeros(df_expr.shape)\n",
    "        std_new = np.zeros(df_expr.shape)\n",
    "        inds = []\n",
    "        tole = 0.001\n",
    "        signal = np.zeros(df_expr.shape)\n",
    "        excluded = np.ones(df_expr.shape)\n",
    "        i = -1\n",
    "\n",
    "        while np.all(std_old == 0) or np.all(np.abs(std_new - std_old) > tole):\n",
    "            i += 1\n",
    "            std_old = std_new\n",
    "            vec = df_expr.to_numpy()\n",
    "\n",
    "            # Get the samples that are smaller than 2.5*std\n",
    "            mask = vec*excluded\n",
    "            mask[mask == 0.0] = np.nan\n",
    "            if np.all(np.any(np.isnan(mask), axis=0)):\n",
    "                print(\"Break\")\n",
    "                break\n",
    "            mean = np.nanmean(mask, axis=1, keepdims=True)\n",
    "            std_new = np.nanstd(mask, axis=1, keepdims=True)\n",
    "\n",
    "            # Get the samples that are larger than 2.5*std\n",
    "            # 1 in signal means the sample > 2.5*std and vice versa\n",
    "            signal = (np.abs(df_expr.to_numpy()) > 2.5*std_new + mean)*excluded\n",
    "            excluded = excluded * (1-signal)\n",
    "            signal_df = pd.DataFrame(signal, columns=df_expr.columns, index=df_expr.index)\n",
    "            inds.extend(signal_df.any(axis=0)[signal_df.any(axis=0)].index.tolist())\n",
    "\n",
    "        inds_method_3 = list(set(inds))\n",
    "\n",
    "        if filtering_to_use == 3:\n",
    "            print(f\"Filtered {len(inds_method_3)} proteins by method 3\")\n",
    "            return recombine_data(df_meta,df_expr[inds_method_3])\n",
    "\n",
    "\n",
    "\n",
    "        # # To retain the highest fidelity proteins, we select the ones in the middle\n",
    "        inds_final = list(set(inds_method_1).intersection(set(inds_method_2)).intersection(set(inds_method_3)))\n",
    "        if filtering_to_use == 4:\n",
    "            print(f\"Filtered {len(inds_final)} proteins by all methods\")\n",
    "            return recombine_data(df_meta,df_expr[inds_final])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_mutual_information(\n",
    "    df, \n",
    "    mutual_information_threshold=0.01,\n",
    "    y_col='Cell_viability%_(cck8Drug-blk)/(control-blk)*100',\n",
    "    rand_state=42,\n",
    "    plot=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Filters columns by mutual information with respect to y_col.\n",
    "    Optionally plots the mutual information before and after filtering as CDFs.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame with metadata and expression data.\n",
    "        mutual_information_threshold: Threshold for filtering.\n",
    "        y_col: Column to compute MI against.\n",
    "        rand_state: Random state for reproducibility.\n",
    "        plot: If True, plot MI CDF before and after filtering.\n",
    "\n",
    "    Returns:\n",
    "        Filtered DataFrame.\n",
    "    \"\"\"\n",
    "    from sklearn.feature_selection import mutual_info_regression\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "\n",
    "    meta, expr = separate_metadata(df)\n",
    "    y = df[y_col]\n",
    "    mi = mutual_info_regression(\n",
    "        expr.values, \n",
    "        y.values, \n",
    "        random_state=rand_state\n",
    "    )\n",
    "    mi_series = pd.Series(mi, index=expr.columns)\n",
    "    keep_cols = list(mi_series[mi_series >= mutual_information_threshold].index)\n",
    "    cell_viab_col = 'Cell_viability%_(cck8Drug-blk)/(control-blk)*100'\n",
    "    if cell_viab_col in expr.columns and cell_viab_col not in keep_cols:\n",
    "        keep_cols.append(cell_viab_col)\n",
    "    filtered_expr = expr[keep_cols]\n",
    "    print(f\"Filtered proteins by mutual information: {expr.shape[1]} -> {filtered_expr.shape[1]}\")\n",
    "\n",
    "    if plot:\n",
    "        # Plot both CDFs on the same graph\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        # CDF before filtering\n",
    "        sorted_mi = np.sort(mi_series)\n",
    "        cdf = np.arange(1, len(sorted_mi)+1) / len(sorted_mi)\n",
    "        plt.plot(sorted_mi, cdf, color='skyblue', label='All Features')\n",
    "        # CDF after filtering\n",
    "        mi_filtered = mi_series[keep_cols]\n",
    "        sorted_mi_filtered = np.sort(mi_filtered)\n",
    "        cdf_filtered = np.arange(1, len(sorted_mi_filtered)+1) / len(sorted_mi_filtered)\n",
    "        plt.plot(sorted_mi_filtered, cdf_filtered, color='lightgreen', label='Filtered Features')\n",
    "        plt.axvline(mutual_information_threshold, color='red', linestyle='--', label='Threshold')\n",
    "        plt.title('Mutual Information CDF Before and After Filtering')\n",
    "        plt.xlabel('Mutual Information')\n",
    "        plt.ylabel('Cumulative Density')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "\n",
    "    # Recombine with metadata\n",
    "    filtered_data = recombine_data(meta, filtered_expr)\n",
    "    return filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_variance(df, variance_threshold=0.01, plot=False):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    # Calculate variance for each protein\n",
    "    meta, expr = separate_metadata(df)\n",
    "    protein_var = expr.var()\n",
    "\n",
    "    # Sort proteins by variance in descending order\n",
    "    sorted_var = protein_var.sort_values(ascending=False)\n",
    "\n",
    "    # Filter proteins by variance threshold\n",
    "    keep_cols = sorted_var[sorted_var > variance_threshold].index.tolist()\n",
    "    filtered_expr = expr[keep_cols]\n",
    "    print(\"Proteins before filtering: \", len(expr.columns))\n",
    "    print(\"Proteins after filtering (variance > \", variance_threshold, \"): \", len(keep_cols))\n",
    "    if plot:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        # CDF for all variances\n",
    "        sorted_all = np.sort(sorted_var.values)\n",
    "        cdf_all = np.arange(1, len(sorted_all)+1) / len(sorted_all)\n",
    "        plt.plot(sorted_all, cdf_all, color='skyblue', label='All Proteins')\n",
    "        # CDF for filtered variances\n",
    "        sorted_filtered = np.sort(filtered_expr.var().values)\n",
    "        cdf_filtered = np.arange(1, len(sorted_filtered)+1) / len(sorted_filtered)\n",
    "        plt.plot(sorted_filtered, cdf_filtered, color='lightgreen', label='Filtered Proteins')\n",
    "        plt.axvline(variance_threshold, color='red', linestyle='--', label='Variance Threshold')\n",
    "        plt.xlabel('Variance')\n",
    "        plt.ylabel('Cumulative Density')\n",
    "        plt.title('Protein Variance CDF Before and After Filtering')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "    # Recombine with metadata\n",
    "    filtered_data = recombine_data(meta, filtered_expr)\n",
    "    return filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of log ratios (overall): 0.002090789757845831\n",
      "Variance of log ratios (overall): 0.10170501672633162\n",
      "mean: 0.002, std: 0.319\n",
      "mean: -0.002, std: 0.032\n",
      "mean: 0.003, std: 0.323\n",
      "mean: 0.002, std: 0.319\n",
      "mean: 0.002, std: 0.319\n",
      "Filtered 1937 proteins by method 1\n"
     ]
    }
   ],
   "source": [
    "#MAIN METHOD\n",
    "#NOTE Things to change for different datasets:\n",
    "\n",
    "if(re_run_flag==False):\n",
    "    data_path=r'C:\\Users\\abdul\\OneDrive - University of Cambridge\\Desktop\\MDRA\\cellbox_torch\\preprocessing\\data.csv'\n",
    "    prot_info_path=r'C:\\Users\\abdul\\OneDrive - University of Cambridge\\Desktop\\MDRA\\cellbox_torch\\preprocessing\\prots_info.csv'\n",
    "    df_prot_info=pd.read_csv(prot_info_path)\n",
    "    df_raw=pd.read_csv(data_path)\n",
    "    df_control_row = df_raw[df_raw['pert_id']=='control'] #pulls out the control row\n",
    "    meta_data_cols = [0] + list(range(-12, -2))+[-1] #The metadata columns, must be changed for different datasets\n",
    "    #NOTE the function explicitly pulls out cell viability, that would also need to be changed for different datasets\n",
    "    df_raw=reorganize_data(df_raw, meta_data_cols)\n",
    "    #applying transforms:\n",
    "    #NOTE now that the metadata is at the end, we need to update the metadata indices\n",
    "    meta_data_cols = range(-12,0)\n",
    "    #DONE READING IN DATA AND RESHAPING\n",
    "\n",
    "\n",
    "    #FILTERING AND GENERATING STARTING DATA STRUCTURES\n",
    "    filtered_by_completeness, filtered_by_completeness_info=filter_by_completeness(df_raw, df_prot_info,completeness_threshold=0.95,metadata_indices=meta_data_cols) \n",
    "    filtered_by_control_values=filter_by_control_values(filtered_by_completeness, control_id='control',metadata_indices=meta_data_cols)\n",
    "    filtered_by_cell_viability=filter_by_cell_viability(filtered_by_control_values)\n",
    "    log_ratios=convert_to_log_ratios(filtered_by_control_values, control_id='control',metadata_indices=meta_data_cols) \n",
    "    log_ratios=log_ratios.fillna(log_ratios.mean()) #fill in with the mean of the column\n",
    "    assert not separate_metadata(log_ratios)[1].isna().any().any(), \"There are NA values in the log_ratios expression dataframe\"\n",
    "    re_run_flag=True\n",
    "\n",
    "\n",
    "# Print the mean and variance of the log ratios (expression columns only)\n",
    "meta,expr = separate_metadata(log_ratios)\n",
    "mean_log_ratios = expr.values.mean()\n",
    "var_log_ratios = expr.values.var()\n",
    "print(\"Mean of log ratios (overall):\", str(mean_log_ratios))\n",
    "print(\"Variance of log ratios (overall):\", str(var_log_ratios))\n",
    "\n",
    "\n",
    "filtered_by_signal_to_noise_gen2=filter_signal_to_noise_gen2(log_ratios,signal_threshold=2.5,tole=0.001,filtering_to_use=1)\n",
    "\n",
    "# filtered_by_mutual_information=filter_by_mutual_information(log_ratios,mutual_information_threshold=0.001,plot=True)\n",
    "# plt.xlim(0,1)\n",
    "# plt.show()\n",
    "\n",
    "# filtered_by_signal_to_noise=filter_signal_to_noise(filtered_by_mutual_information,signal_to_noise_threshold=0.5,epsilon=1e-3,plot=True)\n",
    "# plt.xlim(0,3)\n",
    "# plt.show()\n",
    "# filtered_by_variance=filter_by_variance(filtered_by_signal_to_noise,variance_threshold=0.0001,plot=True)\n",
    "# plt.xlim(0,.01)\n",
    "# plt.show()\n",
    "# # filtered_by_signal_to_noise.to_csv('Intermediate_files/completeness_log2_MI_SNR_values.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[225], line 65\u001b[0m\n\u001b[0;32m     63\u001b[0m model\u001b[38;5;241m=\u001b[39melasticnet_cv\n\u001b[0;32m     64\u001b[0m rfe\u001b[38;5;241m=\u001b[39mRFE(model,n_features_to_select\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m---> 65\u001b[0m \u001b[43mrfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_splits\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mys\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m top_100_features\u001b[38;5;241m.\u001b[39mappend(train_splits[trial]\u001b[38;5;241m.\u001b[39mcolumns[rfe\u001b[38;5;241m.\u001b[39msupport_])\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28mprint\u001b[39m(top_100_features[trial])\n",
      "File \u001b[1;32mc:\\Users\\abdul\\.conda\\envs\\cellbox\\lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\abdul\\.conda\\envs\\cellbox\\lib\\site-packages\\sklearn\\feature_selection\\_rfe.py:249\u001b[0m, in \u001b[0;36mRFE.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(\n\u001b[0;32m    226\u001b[0m     \u001b[38;5;66;03m# RFE.estimator is not validated yet\u001b[39;00m\n\u001b[0;32m    227\u001b[0m     prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    228\u001b[0m )\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params):\n\u001b[0;32m    230\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit the RFE model and then the underlying estimator on the selected features.\u001b[39;00m\n\u001b[0;32m    231\u001b[0m \n\u001b[0;32m    232\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 249\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\abdul\\.conda\\envs\\cellbox\\lib\\site-packages\\sklearn\\feature_selection\\_rfe.py:297\u001b[0m, in \u001b[0;36mRFE._fit\u001b[1;34m(self, X, y, step_score, **fit_params)\u001b[0m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    295\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting estimator with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m features.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m np\u001b[38;5;241m.\u001b[39msum(support_))\n\u001b[1;32m--> 297\u001b[0m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    299\u001b[0m \u001b[38;5;66;03m# Get importance and rank them\u001b[39;00m\n\u001b[0;32m    300\u001b[0m importances \u001b[38;5;241m=\u001b[39m _get_feature_importances(\n\u001b[0;32m    301\u001b[0m     estimator,\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimportance_getter,\n\u001b[0;32m    303\u001b[0m     transform_func\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msquare\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    304\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\abdul\\.conda\\envs\\cellbox\\lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\abdul\\.conda\\envs\\cellbox\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1663\u001b[0m, in \u001b[0;36mLinearModelCV.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1643\u001b[0m \u001b[38;5;66;03m# We do a double for loop folded in one, in order to be able to\u001b[39;00m\n\u001b[0;32m   1644\u001b[0m \u001b[38;5;66;03m# iterate in parallel on l1_ratio and folds\u001b[39;00m\n\u001b[0;32m   1645\u001b[0m jobs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1646\u001b[0m     delayed(_path_residuals)(\n\u001b[0;32m   1647\u001b[0m         X,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1661\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m folds\n\u001b[0;32m   1662\u001b[0m )\n\u001b[1;32m-> 1663\u001b[0m mse_paths \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1667\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1668\u001b[0m mse_paths \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreshape(mse_paths, (n_l1_ratio, \u001b[38;5;28mlen\u001b[39m(folds), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m   1669\u001b[0m \u001b[38;5;66;03m# The mean is computed over folds.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\abdul\\.conda\\envs\\cellbox\\lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\abdul\\.conda\\envs\\cellbox\\lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\abdul\\.conda\\envs\\cellbox\\lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\abdul\\.conda\\envs\\cellbox\\lib\\site-packages\\joblib\\parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#generating five folds of the data:\n",
    "magic_scaling_factor=37.2 #this is derived by scaling it to match the mean overall of the melanoma data\n",
    "overall_data=filtered_by_variance.iloc[:,:-12]*magic_scaling_factor\n",
    "folds=np.arange(0,4)\n",
    "train_splits=[]\n",
    "test_splits=[]\n",
    "ys=[]   \n",
    "target_column='Cell_viability%_(cck8Drug-blk)/(control-blk)*100'\n",
    "for trial in folds:\n",
    "    train,test=train_test_split(overall_data,test_size=0.2,random_state=np.random.randint(1,1000000))\n",
    "    ys.append(train[target_column])\n",
    "    train.drop(columns=[target_column],inplace=True)\n",
    "    test.drop(columns=[target_column],inplace=True)\n",
    "    train_splits.append(train)\n",
    "    test_splits.append(test)\n",
    "#running ablation studies on each of the five folds:\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "top_100_features=[]\n",
    "\n",
    "# #injecting noise temporarily:\n",
    "# for i,training_data in enumerate(train_splits):\n",
    "#     noisy_train = training_data + np.random.normal(0, 10, size=training_data.shape)\n",
    "#     train_splits[i] = noisy_train\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    max_features=100,            # try 100 or 0.1 * total features\n",
    "    min_samples_leaf=5,\n",
    "    min_samples_split=10,\n",
    "    bootstrap=True,\n",
    "    oob_score=True,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "elasticnet_cv=ElasticNetCV(\n",
    "    l1_ratio=[.1, .5, .9],\n",
    "    alphas=[0.0001, 0.001, 0.01, 0.1, 1.0],\n",
    "    cv=5,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    max_iter=10000\n",
    ")\n",
    "from sklearn.linear_model import ElasticNet\n",
    "elasticnet=ElasticNet(\n",
    "    alpha=0.01,\n",
    "    l1_ratio=0.5,\n",
    "    max_iter=1000,\n",
    "    random_state=42,\n",
    ")\n",
    "basic_elasticnet=ElasticNet()\n",
    "\n",
    "\n",
    "\n",
    "for trial in folds:\n",
    "    model=elasticnet_cv\n",
    "    rfe=RFE(model,n_features_to_select=100)\n",
    "    rfe.fit(train_splits[trial],ys[trial])\n",
    "    top_100_features.append(train_splits[trial].columns[rfe.support_])\n",
    "    print(top_100_features[trial])\n",
    "\n",
    "\n",
    "\n",
    "# Check for consistency across splits\n",
    "for i in range(len(top_100_features)):\n",
    "    for j in range(i+1, len(top_100_features)):\n",
    "        overlap = set(top_100_features[i]).intersection(set(top_100_features[j]))\n",
    "        print(f\"Overlap between split {i} and {j}: {len(overlap)} features\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_targeted_proteins(prot_data, prot_info, id_key='Uniprot.ID'):\n",
    "    \"\"\"\n",
    "    Identifies proteins that are targeted by drugs in the dataset.\n",
    "    \n",
    "    Args:\n",
    "        prot_data: DataFrame containing protein expression data\n",
    "        prot_info: DataFrame containing protein metadata\n",
    "        id_key: Column name containing target IDs (default: 'Uniprot.ID')\n",
    "    \n",
    "    Returns:\n",
    "        List of protein IDs that are targeted by drugs\n",
    "    \"\"\"\n",
    "    # Get all unique uniprot ID's for each trial from data.csv\n",
    "    #This is from data.csv file and the uniprot ID says which proteins are targeted by drugs\n",
    "    all_targets = prot_data[id_key].dropna().unique()\n",
    "    \n",
    "    # Split targets that may be comma or semicolon-separated and flatten the list\n",
    "    target_list = []\n",
    "    #if there are multiple targets, split them by comma or semicolon, then convert to uppercase and get rid of whitespace and then add to target_list\n",
    "    for targets in all_targets:\n",
    "        if isinstance(targets, str):\n",
    "            target_list.extend([t.strip().upper() for t in re.split(r'[;,]', targets)])\n",
    "    \n",
    "    # Get unique targets, (just looking at the set which gets rid of duplicates)\n",
    "    unique_targets = list(set(target_list))\n",
    "    \n",
    "    # Filter to only include targets that exist in our protein measurements\n",
    "    valid_targets = [t for t in unique_targets if t in prot_data.columns]\n",
    "    \n",
    "    print(f\"Found {len(valid_targets)} unique targeted proteins out of {len(unique_targets)} total targets\")\n",
    "    return valid_targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pert_id_to_targets_dict(prot_log):\n",
    "    #make blank dictionary\n",
    "    pert_id_to_targets_dict={}\n",
    "    #get the pert id and uniprot id from the prot_log dataframe\n",
    "    for index, row in prot_log.iterrows():\n",
    "        pert_id=row['pert_id']\n",
    "        uniprot_id=row['Uniprot.ID']\n",
    "        id_list=[]\n",
    "        #split the uniprot ID by comma or semicolon, doing some basic checks to make sure it's not empty:\n",
    "        if uniprot_id is not np.nan:\n",
    "            if 'not' not in str(uniprot_id).lower():\n",
    "                id_list.extend([t.strip().upper() for t in re.split(r'[;,]', uniprot_id)])\n",
    "        #associate all the uniprot ID's with the pert id in the dict.\n",
    "        pert_id_to_targets_dict[pert_id]=id_list\n",
    "    return pert_id_to_targets_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_activity_nodes(targeted_proteins_with_metadata,pert_id_to_targets_dict):\n",
    "    activity_nodes=targeted_proteins_with_metadata.copy()\n",
    "    #scan through each protein column\n",
    "    for protein in targeted_proteins_with_metadata.columns:\n",
    "\n",
    "        #scan through each row and get the pert_id, then look it up in the dictionary to see if the protein is targeted;\n",
    "        for index, row in targeted_proteins_with_metadata.iterrows():\n",
    "\n",
    "            pert_id=row['pert_id']\n",
    "\n",
    "            #check if the pert_id is in the pert_ID_list\n",
    "            if pert_id in pert_id_to_targets_dict.keys():\n",
    "                #check if the protein is in the dictionary\n",
    "                if protein in pert_id_to_targets_dict[pert_id]:\n",
    "                    pass\n",
    "                else:\n",
    "                    if protein in activity_nodes.columns:\n",
    "                        activity_nodes.loc[index,protein]=0\n",
    "\n",
    "    #cleave off last 12 columns for metadata:\n",
    "    activity_nodes=activity_nodes.iloc[:,:-12]\n",
    "    return activity_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_targeted_indices(targeted_proteins_with_metadata):\n",
    "    protein_list=targeted_proteins_with_metadata.columns\n",
    "    targeted_indices = []\n",
    "    #first check that it is proteins:\n",
    "    for index, Uniprots in enumerate(targeted_proteins_with_metadata['Uniprot.ID']):\n",
    "        if Uniprots is not np.nan:\n",
    "            if 'not' not in str(Uniprots).lower():\n",
    "                id_list=[]\n",
    "                id_list.extend([t.strip().upper() for t in re.split(r'[;,]', Uniprots)])\n",
    "                #then check that the protein is in the list of targeted proteins:\n",
    "                for i in id_list:\n",
    "                    if i in protein_list:\n",
    "                        targeted_indices.append(index)\n",
    "                        break\n",
    "    return targeted_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cellbox takes in three files: expr.csv, pert.csv, node_Index.csv\n",
    "#expr.csv is the expression data, of size drug trials x(proteins+phenotypes+activity nodes)\n",
    "#pert.csv is the perturbation data, of size drug trials x(proteins+phenotypes+activity nodes); the proteins and phenotypes are zeroed out\n",
    "#the activity nodes indicate the activity of each protein in each drug trial, and so therefore should be of size #of targeted proteins, \n",
    "#these are activated.\n",
    "\n",
    "def make_cellbox_files(prot_log, acti_df, file_prefix, file_path):\n",
    "    \"\"\"\n",
    "    Creates CellBox input files from processed data.\n",
    "    \n",
    "    Args:\n",
    "        prot_log: DataFrame containing log ratios\n",
    "        acti_df: DataFrame containing activity nodes\n",
    "        file_prefix: Prefix for output files\n",
    "        file_path: Path to save output files\n",
    "    \n",
    "    Returns: cellbox_files\n",
    "    \"\"\"\n",
    "\n",
    "    expr_csv = prot_log.merge(acti_df, left_index=True, right_index=True)\n",
    "\n",
    "    # Create perturbation data\n",
    "    zeros_pert = pd.DataFrame(np.zeros_like(prot_log), columns=prot_log.columns, index=prot_log.index)\n",
    "    acti_df_arctanh = pd.DataFrame(\n",
    "        np.arctanh(acti_df.to_numpy().astype(float)),\n",
    "        columns=acti_df.columns, index=acti_df.index\n",
    "    )\n",
    "    pert_csv = pd.merge(zeros_pert, acti_df_arctanh, left_index=True, right_index=True)\n",
    "\n",
    "    # Create node index\n",
    "    columns = pert_csv.columns.tolist()\n",
    "    node_index_csv = pd.DataFrame({\"A\": columns})\n",
    "\n",
    "    # Save files\n",
    "    expr_csv.to_csv(\n",
    "        (file_path + file_prefix + \"expr.csv\"),\n",
    "        header=False,\n",
    "        index=False\n",
    "    )\n",
    "    pert_csv.to_csv(\n",
    "        (file_path + file_prefix + \"pert.csv\"),\n",
    "        header=False,\n",
    "        index=False\n",
    "    )\n",
    "    node_index_csv.to_csv(\n",
    "        (file_path + file_prefix + \"node_Index.csv\"),\n",
    "        sep=\" \",\n",
    "        header=False,\n",
    "        index=False\n",
    "    )\n",
    "    return expr_csv, pert_csv, node_index_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered from 8544 to 5733 proteins\n",
      "Completeness threshold: 0.95\n",
      "Converted 94 samples to log ratios\n",
      "Found 61 unique targeted proteins out of 277 total targets\n",
      "targeted_proteins written to Intermediate_files/targeted_proteins.txt\n",
      "Shape of targeted proteins dataframe: (94, 73)\n",
      "tgt_indices written to Intermediate_files/tgt_indices.txt\n"
     ]
    }
   ],
   "source": [
    "# Get the targeted proteins from the log_ratios dataframe\n",
    "targeted_proteins_df = log_ratios[targeted_proteins]\n",
    "targeted_proteins_df.to_csv('Intermediate_files/targeted_proteins_df.csv', index=False)\n",
    "# Combine with metadata columns\n",
    "targeted_proteins_with_metadata = pd.concat([\n",
    "    targeted_proteins_df,\n",
    "    log_ratios.iloc[:, meta_data_cols]\n",
    "], axis=1)\n",
    "\n",
    "# Display the shape of the resulting dataframe\n",
    "print(f\"Shape of targeted proteins dataframe: {targeted_proteins_with_metadata.shape}\")\n",
    "# targeted_proteins_with_metadata.to_csv('targeted_proteins_with_metadata.csv', index=False)\n",
    "\n",
    "pert_id_to_targets_dict=make_pert_id_to_targets_dict(targeted_proteins_with_metadata)\n",
    "\n",
    "tgt_indices=get_targeted_indices(targeted_proteins_with_metadata)\n",
    "# Write tgt_indices to a file\n",
    "tgt_indices_file = \"Intermediate_files/tgt_indices.txt\"\n",
    "with open(tgt_indices_file, \"w\") as f:\n",
    "    for idx in tgt_indices:\n",
    "        f.write(str(idx) + \"\\n\")\n",
    "print(f\"tgt_indices written to {tgt_indices_file}\")\n",
    "\n",
    "#\n",
    "targeted_proteins_with_metadata=targeted_proteins_with_metadata.loc[tgt_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select targeted rows\n",
    "targeted_proteins=get_targeted_proteins(log_ratios, df_prot_info) #seems like it works\n",
    "# Write targeted_proteins to a file\n",
    "with open('Intermediate_files/targeted_proteins.txt', 'w') as f:\n",
    "    for prot in targeted_proteins:\n",
    "        f.write(str(prot) + '\\n')\n",
    "print(\"targeted_proteins written to Intermediate_files/targeted_proteins.txt\")\n",
    "tgtd_log_ratios=log_ratios.loc[tgt_indices]\n",
    "\n",
    "#select targeted proteins:\n",
    "tgtd_log_ratios=tgtd_log_ratios[targeted_proteins +['Cell_viability%_(cck8Drug-blk)/(control-blk)*100']]\n",
    "tgtd_log_ratios.fillna(0,inplace=True)\n",
    "tgtd_log_ratios.to_csv('Intermediate_files/tgtd_log_ratios.csv', index=False)\n",
    "activity_nodes=make_activity_nodes(targeted_proteins_with_metadata,pert_id_to_targets_dict)\n",
    "activity_nodes=activity_nodes.loc[tgt_indices]\n",
    "activity_nodes.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: 'C:\\Users\\abdul\\OneDrive - University of Cambridge\\Desktop\\MDRA\\cellbox_torch\\Abdullah_kuziez\\Data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tgtd_prots_cellbox\u001b[38;5;241m=\u001b[39m\u001b[43mmake_cellbox_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtgtd_log_ratios\u001b[49m\u001b[43m,\u001b[49m\u001b[43mactivity_nodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mfile_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdirectly_targeted_proteins\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mfile_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mUsers\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mabdul\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mOneDrive - University of Cambridge\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mDesktop\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mMDRA\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mcellbox_torch\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mAbdullah_kuziez\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mData\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mrun1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[0;32m      4\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[11], line 35\u001b[0m, in \u001b[0;36mmake_cellbox_files\u001b[1;34m(prot_log, acti_df, file_prefix, file_path)\u001b[0m\n\u001b[0;32m     32\u001b[0m node_index_csv \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns})\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Save files\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m \u001b[43mexpr_csv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfile_prefix\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mexpr.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m     39\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m pert_csv\u001b[38;5;241m.\u001b[39mto_csv(\n\u001b[0;32m     41\u001b[0m     (file_path \u001b[38;5;241m+\u001b[39m file_prefix \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpert.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     42\u001b[0m     header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     43\u001b[0m     index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     44\u001b[0m )\n\u001b[0;32m     45\u001b[0m node_index_csv\u001b[38;5;241m.\u001b[39mto_csv(\n\u001b[0;32m     46\u001b[0m     (file_path \u001b[38;5;241m+\u001b[39m file_prefix \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnode_Index.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     47\u001b[0m     sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     48\u001b[0m     header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     49\u001b[0m     index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     50\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\abdul\\.conda\\envs\\cellbox\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\abdul\\.conda\\envs\\cellbox\\lib\\site-packages\\pandas\\core\\generic.py:3720\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3709\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[0;32m   3711\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[0;32m   3712\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[0;32m   3713\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3717\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[0;32m   3718\u001b[0m )\n\u001b[1;32m-> 3720\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3723\u001b[0m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3724\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3725\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3726\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3727\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3728\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3729\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3730\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3731\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3732\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3733\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3734\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3735\u001b[0m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3737\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\abdul\\.conda\\envs\\cellbox\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\abdul\\.conda\\envs\\cellbox\\lib\\site-packages\\pandas\\io\\formats\\format.py:1189\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m   1168\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1170\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[0;32m   1171\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[0;32m   1172\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1187\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[0;32m   1188\u001b[0m )\n\u001b[1;32m-> 1189\u001b[0m \u001b[43mcsv_formatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[0;32m   1192\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[1;32mc:\\Users\\abdul\\.conda\\envs\\cellbox\\lib\\site-packages\\pandas\\io\\formats\\csvs.py:241\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;124;03mCreate the writer & save.\u001b[39;00m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[1;32m--> 241\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    245\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    247\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[0;32m    249\u001b[0m \n\u001b[0;32m    250\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[0;32m    252\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[0;32m    253\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    258\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[0;32m    259\u001b[0m     )\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save()\n",
      "File \u001b[1;32mc:\\Users\\abdul\\.conda\\envs\\cellbox\\lib\\site-packages\\pandas\\io\\common.py:734\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    732\u001b[0m \u001b[38;5;66;03m# Only for write methods\u001b[39;00m\n\u001b[0;32m    733\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m is_path:\n\u001b[1;32m--> 734\u001b[0m     \u001b[43mcheck_parent_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    736\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compression:\n\u001b[0;32m    737\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzstd\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    738\u001b[0m         \u001b[38;5;66;03m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\abdul\\.conda\\envs\\cellbox\\lib\\site-packages\\pandas\\io\\common.py:597\u001b[0m, in \u001b[0;36mcheck_parent_directory\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    595\u001b[0m parent \u001b[38;5;241m=\u001b[39m Path(path)\u001b[38;5;241m.\u001b[39mparent\n\u001b[0;32m    596\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parent\u001b[38;5;241m.\u001b[39mis_dir():\n\u001b[1;32m--> 597\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124mrf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot save file into a non-existent directory: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mOSError\u001b[0m: Cannot save file into a non-existent directory: 'C:\\Users\\abdul\\OneDrive - University of Cambridge\\Desktop\\MDRA\\cellbox_torch\\Abdullah_kuziez\\Data'"
     ]
    }
   ],
   "source": [
    "tgtd_prots_cellbox=make_cellbox_files(tgtd_log_ratios,activity_nodes,\n",
    "                   file_prefix='directly_targeted_proteins',\n",
    "                   file_path=r'C:\\Users\\abdul\\OneDrive - University of Cambridge\\Desktop\\MDRA\\cellbox_torch\\Abdullah_kuziez\\Data\\run1'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CUTTING DOWN TO ONLY ROWS WITH TARGETED PROTEINS:\n",
    "df_of_neighbors=pd.read_csv(r'C:\\Users\\abdul\\OneDrive - University of Cambridge\\Desktop\\MDRA\\cellbox_torch\\Abdullah_kuziez\\Testing_encodings\\String_adjacency\\protein_neighbors_and_degrees.csv')\n",
    "filtered_log_ratios=log_ratios.loc[tgt_indices]\n",
    "second_deg_neighbors=df_of_neighbors['second_order'].tolist()\n",
    "second_deg_neighbors_in_prots = [neighbor for neighbor in second_deg_neighbors if neighbor in filtered_log_ratios.columns]\n",
    "filtered_log_ratios=filtered_log_ratios[second_deg_neighbors_in_prots +['Cell_viability%_(cck8Drug-blk)/(control-blk)*100']]\n",
    "\n",
    "#shitty patching of holes but for now sufficient:\n",
    "filtered_log_ratios.fillna(0,inplace=True)\n",
    "activity_nodes.fillna(0,inplace=True)\n",
    "#should be good now to run the cellbox_file_maker\n",
    "x=make_cellbox_files(filtered_log_ratios,activity_nodes,\n",
    "                   file_prefix='_test_',\n",
    "                   file_path=r'C:\\Users\\abdul\\OneDrive - University of Cambridge\\Desktop\\MDRA\\cellbox_torch\\Abdullah_kuziez\\Data\\STRINGDB_encodings'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cellbox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
