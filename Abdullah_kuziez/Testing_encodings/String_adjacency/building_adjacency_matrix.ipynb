{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First two cells are the ways that I generated the lookup table and got the list of all the relevant neighbors so that I can use that for the adjacency matrix populating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#not_relevant: this is written as a function, but not really intended to be used multiple times, just functionalized to put it to the side when I run\n",
    "def producing_uniprot_ensembl_mapping():\n",
    "\n",
    "    #new version using the downloaded mapping file:\n",
    "    # Read the tab-delimited file with 22 columns\n",
    "    import pandas as pd\n",
    "    df = pd.read_csv(r\"C:\\Users\\abdul\\OneDrive - University of Cambridge\\Desktop\\MDRA\\cellbox_torch\\Abdullah_kuziez\\Testing_encodings\\HUMAN_9606_idmapping_selected.tab\", \n",
    "                    sep='\\t',\n",
    "                    header=None,\n",
    "                    names=[f'col_{i}' for i in range(22)],\n",
    "                    low_memory=False)\n",
    "    df_uniprots_and_ensembles=df[['col_0','col_18','col_19','col_20']]\n",
    "    del df\n",
    "    # Rename columns to match the provided names\n",
    "    df_uniprots_and_ensembles.rename(columns={\n",
    "        'col_0': 'Uniprot.ID',\n",
    "        'col_18': 'ENSG',\n",
    "        'col_19': 'ENST', \n",
    "        'col_20': 'ENSP',\n",
    "    }, inplace=True)\n",
    "\n",
    "\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    for index in tqdm(range(len(df_uniprots_and_ensembles['ENSP'])), desc=\"Processing ENSP entries\"):\n",
    "        ID_list=df_uniprots_and_ensembles['ENSP'][index]\n",
    "        # Split the string of IDs by semicolon and strip whitespace\n",
    "        if isinstance(ID_list, str):\n",
    "            split_ids = [id_.strip() for id_ in ID_list.split(';')]\n",
    "        else:\n",
    "            split_ids = []\n",
    "        # For each element in split_ids, remove the '.' and everything after the '.':\n",
    "        split_ids = [id_.split('.', 1)[0] for id_ in split_ids]\n",
    "        df_uniprots_and_ensembles['ENSP'][index]=split_ids\n",
    "\n",
    "    df_uniprots_and_ensembles.to_csv('uniprots_to_ensembls_lookup_table.csv')\n",
    "    return df_uniprots_and_ensembles\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#NOTE  #general testing that I did to make sure it was working:\n",
    "# import pandas as pd\n",
    "\n",
    "# import ast\n",
    "# df_uniprots_and_ensembles2 = pd.read_csv(\n",
    "#     'uniprots_to_ensembls_lookup_table.csv',\n",
    "#     converters={'ENSP': lambda x: ast.literal_eval(x) if pd.notnull(x) else []}\n",
    "# )\n",
    "# df_uniprots_and_ensembles2=df_uniprots_and_ensembles2[['Uniprot.ID','ENSG','ENST','ENSP']]\n",
    "# # more testing:\n",
    "# ENSMBL_list=['ENSP00000438677','ENSP00000298316','ENSP00000005257']\n",
    "# # For each ensembl ID in ENSMBL_list, search the 'ENSP' column (which contains lists of strings) for a match.\n",
    "# matching_uniprot_ids = []\n",
    "# for ens_id in ENSMBL_list:\n",
    "#     # Find rows where the ENSP list contains the ens_id\n",
    "#     matches = df_uniprots_and_ensembles2[df_uniprots_and_ensembles2['ENSP'].apply(lambda x: ens_id in x if isinstance(x, list) else False)]\n",
    "#     # For each match, get the Uniprot.ID (could be multiple per ENSP)\n",
    "#     for _, row in matches.iterrows():\n",
    "#         matching_uniprot_ids.append(row['Uniprot.ID'])\n",
    "\n",
    "# print(\"Matching Uniprot IDs for given ENSMBL_list:\", matching_uniprot_ids)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#not relevant, but this is how I generated the original list of neighbors and wrote to file, functionalized so it doesnt run always\n",
    "def get_and_write_neighbor_list():\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    #get starting data\n",
    "    data_path=r'C:\\Users\\abdul\\OneDrive - University of Cambridge\\Desktop\\MDRA\\cellbox_torch\\preprocessing\\data.csv'\n",
    "    large_dataset=pd.read_csv(data_path)\n",
    "    targeted_proteins_with_metadata=pd.read_csv('targeted_proteins_with_metadata.csv')\n",
    "\n",
    "    #get the zeroth order proteins\n",
    "    listed_zeroth_order_prots=get_clean_unique_uniprots(targeted_proteins_with_metadata)\n",
    "    zeroth_order_proteins=get_uniprots_in_dataset(listed_zeroth_order_prots,large_dataset)\n",
    "    print(\"number of zeroth order proteins: \",len(zeroth_order_proteins))\n",
    "\n",
    "    # #get the first order proteins by finding the partners of the zeroth order proteins\n",
    "    first_order_proteins_ENSMBL=get_protein_partners(zeroth_order_proteins)\n",
    "    first_order_proteins_uniprot=ensmbl_to_uniprot_local(first_order_proteins_ENSMBL)\n",
    "    print(\"total number of first order proteins: \",len(first_order_proteins_uniprot))\n",
    "    final_first_order_proteins=get_uniprots_in_dataset(list(set(first_order_proteins_uniprot)),large_dataset)\n",
    "    print(\"number of first order proteins in the dataset: \",len(final_first_order_proteins))\n",
    "\n",
    "    #get the second order proteins by finding the partners of the first order proteins\n",
    "    second_order_proteins_ENSMBL=get_protein_partners(final_first_order_proteins)\n",
    "    second_order_proteins_uniprot=ensmbl_to_uniprot_local(second_order_proteins_ENSMBL)\n",
    "    print(\"total number of second order proteins: \",len(second_order_proteins_uniprot))\n",
    "    final_second_order_proteins=get_uniprots_in_dataset(list(set(second_order_proteins_uniprot)),large_dataset)\n",
    "    print(\"number of second order proteins in the dataset: \",len(final_second_order_proteins))\n",
    "\n",
    "\n",
    "    # Create a DataFrame with the three columns\n",
    "    # First ensure all arrays are the same length by padding with None\n",
    "    max_length = max(len(zeroth_order_proteins), len(final_first_order_proteins), len(final_second_order_proteins))\n",
    "    zeroth_order_padded = zeroth_order_proteins + [None] * (max_length - len(zeroth_order_proteins))\n",
    "    first_order_padded = final_first_order_proteins + [None] * (max_length - len(final_first_order_proteins))\n",
    "    second_order_padded = final_second_order_proteins + [None] * (max_length - len(final_second_order_proteins))\n",
    "    protein_orders_df = pd.DataFrame({\n",
    "        'zeroth_order': zeroth_order_padded,\n",
    "        'first_order': first_order_padded,\n",
    "        'second_order': second_order_padded\n",
    "    })\n",
    "\n",
    "    # Save to CSV\n",
    "    protein_orders_df.to_csv('protein_neighbors_and_degrees.csv', index=False)\n",
    "    return\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The older functions are preserved for posterity because I might want to mine them and get useful code at some point from them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#older functions that were writtien to directly mark neighbors\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def binary_marking_first__order(targeted_proteins_with_metadata):\n",
    "    #first make a dataframe that is all zeros of the same dimension as the targeted proteins with metadata\n",
    "    targeted_proteins_matrix = pd.DataFrame(0, index=targeted_proteins_with_metadata.index, columns=targeted_proteins_with_metadata.columns)\n",
    "    #using targeted proteins with metadata, identify for each row which proteins are targeted;\n",
    "    for index, row in targeted_proteins_with_metadata.iterrows():\n",
    "        targets = row['Uniprot.ID']\n",
    "        if isinstance(targets, str):\n",
    "            # Use regex to split on either semicolon, pipe, or comma\n",
    "            target_list = [t.strip().upper() for t in re.split('[;|,]', targets)]\n",
    "            # Mark these targets as 1 in the matrix\n",
    "            for target in target_list:\n",
    "                if target in targeted_proteins_matrix.columns:\n",
    "                    targeted_proteins_matrix.loc[index, target] = 1\n",
    "    return targeted_proteins_matrix\n",
    "\n",
    "def continuous_marking_first__order(targeted_proteins_with_metadata):\n",
    "    #first make a dataframe that is all zeros of the same dimension as the targeted proteins with metadata\n",
    "    targeted_proteins_matrix = pd.DataFrame(0, index=targeted_proteins_with_metadata.index, columns=targeted_proteins_with_metadata.columns)\n",
    "    #using targeted proteins with metadata, identify for each row which proteins are targeted;\n",
    "    for index, row in targeted_proteins_with_metadata.iterrows():\n",
    "        targets = row['Uniprot.ID']\n",
    "        if isinstance(targets, str):\n",
    "            # Use regex to split on either semicolon, pipe, or comma\n",
    "            target_list = [t.strip().upper() for t in re.split('[;|,]', targets)]\n",
    "            # Mark these targets as 1 in the matrix\n",
    "            for target in target_list:\n",
    "                if target in targeted_proteins_matrix.columns:\n",
    "                    targeted_proteins_matrix.loc[index, target] = targeted_proteins_with_metadata.loc[index, target]*1 #the one is for first order interactions\\\n",
    "    return targeted_proteins_matrix\n",
    "#unclear if this is working correctly;\n",
    "\n",
    "def binary_marking_second_order_neighbors(first_order_marked): #TODO write this method, but first decide on a database to use\n",
    "    # we should do this for each of the rows in first order marked independently\n",
    "    #first make a copy of the first order marked\n",
    "    second_order_marked = first_order_marked.copy()\n",
    "    #then iterate through each row in the second order marked\n",
    "    for index, row in second_order_marked.iterrows():\n",
    "        #get the names of the columns that have a 1 in them\n",
    "        first_order_proteins = row[row == 1].index.tolist()\n",
    "        # then use the get protein partners to get the second order neighbors of the proteins that have a one in them\n",
    "        print('Index: ', index, 'Proteins: ', first_order_proteins)\n",
    "        second_order_neighbors = get_protein_partners(first_order_proteins)\n",
    "        #then use the convert to uniprot to get the uniprot ID's of the second order neighbors\n",
    "        second_order_uniprot_neighbors = ensmbl_to_uniprot_local(second_order_neighbors)\n",
    "        #then go through and check the columns to see if any of the second order neighbors are in the columns, and if so, add a .33 for the second order neighbors\n",
    "        for neighbor in second_order_uniprot_neighbors:\n",
    "            if neighbor in second_order_marked.columns:\n",
    "                if second_order_marked.loc[index, neighbor] == 0:\n",
    "                    second_order_marked.loc[index, neighbor] = 0.33\n",
    "    return second_order_marked\n",
    "\n",
    "\n",
    "# def continuous_marking_second_order_neighbors(first_order_marked,prot_info): #TODO write this method, but first decide on a database to use\n",
    "\n",
    "def filter_out_all_zero_rows(dataset):\n",
    "    # Filter out rows where all values are 0\n",
    "    filtered_dataset = dataset.loc[~(dataset == 0).all(axis=1)]\n",
    "    return filtered_dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deprecated, using ENSMBL to Uniprot using online service\n",
    "def convert_ensembl_to_uniprot_online(ensembl_ids):\n",
    "    \"\"\"\n",
    "takes in a list of ensemble ID's and returns a list of uniprot ID's using\n",
    " the uniprot mapping service\n",
    "    \"\"\"\n",
    "    import unipressed\n",
    "    if len(ensembl_ids) == 0:\n",
    "        return []\n",
    "    \n",
    "    job = unipressed.IdMappingClient.submit(\n",
    "        'Ensembl_Protein',   # source database\n",
    "        'UniProtKB',         # target database\n",
    "        ids=ensembl_ids      # list of Ensembl protein IDs\n",
    "    )\n",
    "    \n",
    "    uniprot_ids = []\n",
    "    time.sleep(.5)\n",
    "    results = unipressed.id_mapping.core.IdMappingJob.each_result(job)\n",
    "    print(results)\n",
    "    for result in results:\n",
    "        uniprot_ids.append(result['to'])\n",
    "        \n",
    "    return uniprot_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions are used in the first two cells to generate the lookup table and the list of partners. They are also used to get the adjacency matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions to do ENSMBL to uniprot\n",
    "def get_ENSMBL_Uniprot_lookup_table(file_path):\n",
    "    \"\"\"imports lookup table and returns it\n",
    "    lookup table was made using producing_uniprot_ensmbl_mapping \"\"\"\n",
    "    import ast, pandas as pd\n",
    "    df_uniprots_and_ensembles = pd.read_csv(\n",
    "    file_path,\n",
    "    converters={'ENSP': lambda x: ast.literal_eval(x) if pd.notnull(x) else []}\n",
    ")\n",
    "    df_uniprots_and_ensembles=df_uniprots_and_ensembles[['Uniprot.ID','ENSG','ENST','ENSP']]\n",
    "    return df_uniprots_and_ensembles\n",
    "\n",
    "def ensmbl_to_uniprot_local(ENSMBLs,df_uniprots_and_ensembles):\n",
    "    \"\"\"intakes a list of ENSMBLs or a single string of ENSMBLS and the lookup table,\n",
    "    returns the Uniprot IDs for each of the ENSMBLS\"\"\"\n",
    "    matching_uniprot_ids=[]\n",
    "    if isinstance(ENSMBLs,list):\n",
    "        for ens_id in ENSMBLs:\n",
    "    # Find rows where the ENSP list contains the ens_id\n",
    "            matches = df_uniprots_and_ensembles[df_uniprots_and_ensembles['ENSP'].apply(lambda x: ens_id in x if isinstance(x, list) else False)]\n",
    "    # For each match, get the Uniprot.ID (could be multiple per ENSP)\n",
    "            for _, row in matches.iterrows():\n",
    "\n",
    "                matching_uniprot_ids.append(row['Uniprot.ID'])\n",
    "            \n",
    "    if isinstance(ENSMBLs, str):\n",
    "        # Check each row's ENSP list for a string that matches ENSMBLs\n",
    "        matches = df_uniprots_and_ensembles[df_uniprots_and_ensembles['ENSP'].apply(lambda x: ENSMBLs in x if isinstance(x, list) else False)]\n",
    "        for _, row in matches.iterrows():\n",
    "            matching_uniprot_ids.append(row['Uniprot.ID'])\n",
    "\n",
    "    return matching_uniprot_ids\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fxn to get the protein partners from STRING\n",
    "def get_protein_partners(protein_id, min_score=900):\n",
    "    \"\"\"\n",
    "    Get protein interaction partners from STRING database\n",
    "    \n",
    "    Args:\n",
    "        protein_id (str): UniProt accession number\n",
    "        min_score (int): Minimum interaction score (0-1000)\n",
    "        \n",
    "    Returns:\n",
    "        list: List of STRING IDs for interaction partners (without species prefix)\n",
    "    \"\"\"\n",
    "    #if null exit    \n",
    "    if len(protein_id) == 0:\n",
    "            return []\n",
    "    #for each protein in the list get the partners\n",
    "    overall_partners=[]\n",
    "    for protein in protein_id:\n",
    "        url = (\"https://string-db.org/api/tsv/interaction_partners\"\n",
    "            f\"?identifiers={protein}&species=9606&required_score={min_score}\")\n",
    "        \n",
    "        data = requests.get(url).text\n",
    "        df = pd.read_csv(io.StringIO(data), sep='\\t')\n",
    "        # Remove the '9606.' prefix from each STRING ID\n",
    "        partners = [partner.replace('9606.', '') for partner in df['stringId_B'].tolist()]\n",
    "        overall_partners.append(partners)\n",
    "    flat_partners=[]\n",
    "    for i in overall_partners:\n",
    "         for j in i:\n",
    "              flat_partners.append(j)\n",
    "\n",
    "    return flat_partners\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batched protein partners, all of them\n",
    "def get_protein_partners_batched(protein_ids, min_score=900, batch_size=100):\n",
    "    if len(protein_ids) == 0:\n",
    "        return []\n",
    "    overall_partners = []\n",
    "    string_a=[]\n",
    "    string_b=[]\n",
    "    prots=[]\n",
    "    # Process proteins in batches\n",
    "    for i in range(0, len(protein_ids), batch_size):\n",
    "        batch = protein_ids[i:i + batch_size]\n",
    "        # Join proteins with %0d separator\n",
    "        batch_str = '%0d'.join(batch)\n",
    "        \n",
    "        url = (\"https://string-db.org/api/tsv/interaction_partners\"\n",
    "               f\"?identifiers={batch_str}&species=9606&required_score={min_score}\")\n",
    "        \n",
    "        data = requests.get(url).text\n",
    "        df = pd.read_csv(io.StringIO(data), sep='\\t')\n",
    "        string_a.extend(df['stringId_A'].tolist())\n",
    "        string_b.extend(df['stringId_B'].tolist())\n",
    "    \n",
    "    # Group by stringA and aggregate stringB into lists\n",
    "    fixed_string_b=[]\n",
    "    fixed_string_a=[]\n",
    "    for string_b_item in string_b:\n",
    "         fixed_string_b.append(string_b_item.replace('9606.',''))\n",
    "    for string_a_item in string_a:\n",
    "        fixed_string_a.append(string_a_item.replace('9606.',''))\n",
    "    \n",
    "\n",
    "    overall_df_2=pd.DataFrame({'stringId_A':fixed_string_a,'stringId_B':fixed_string_b})\n",
    "    \n",
    "    restructured_df = overall_df_2.groupby('stringId_A')['stringId_B'].agg(list).reset_index()\n",
    "    # Remove the '9606.' prefix from each STRING ID\n",
    "    return restructured_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fxns to get the original uinque uniprots in dataset and then used in neighbor finding\n",
    "def get_clean_unique_uniprots(targeted_proteins_with_metadata):\n",
    "    \"\"\"takes in a dataframe outputs a list of unique uniprots found in the dataframe Uniprot.ID column\n",
    "     splits things using regX, flattens, then gets unique elements \"\"\"\n",
    "    uniprots=targeted_proteins_with_metadata['Uniprot.ID']\n",
    "    cleaned_uniprots=[]\n",
    "    for IDs in uniprots:\n",
    "        if isinstance(IDs, str) and IDs !=' not protein' and IDs !='not human':\n",
    "            indiv_ID=[t.strip().upper() for t in re.split('[;|,]', IDs)]\n",
    "            cleaned_uniprots.append(indiv_ID)\n",
    "\n",
    "    flat_uniprots=get_flattened_list(cleaned_uniprots)\n",
    "    unique_uniprots=list(set(flat_uniprots)) #gives the unique elements of the set\n",
    "    return unique_uniprots\n",
    "\n",
    "def get_flattened_list(list_of_lists):\n",
    "    \"\"\"takes in a list of lists and returns a list of all the elements in the lists\"\"\"\n",
    "    flat_list=[]\n",
    "    for i in list_of_lists:\n",
    "        for j in i:\n",
    "            flat_list.append(j)\n",
    "    return flat_list\n",
    "\n",
    "#gets the uniprots in the dataset\n",
    "def get_uniprots_in_dataset(protein_list,dataset):\n",
    "    \"\"\"takes in a list of uniprots and a dataset and returns a list of uniprots that are in the dataset\"\"\"\n",
    "    proteins_in_dataset=[]\n",
    "    columns=dataset.columns\n",
    "    for i in protein_list:\n",
    "        if i in columns:\n",
    "            proteins_in_dataset.append(i)\n",
    "    return list(set(proteins_in_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensmbl_to_uniprot_local(ENSMBLs, df_uniprots_and_ensembles):\n",
    "    \"\"\"\n",
    "    Optimized version of ensmbl_to_uniprot_local that uses vectorized operations\n",
    "    \"\"\"\n",
    "    if not ENSMBLs:  # Handle empty input\n",
    "        return []\n",
    "    \n",
    "    # Convert ENSMBLs to a set for faster lookups\n",
    "    if isinstance(ENSMBLs, str):\n",
    "        ensembl_set = {ENSMBLs}\n",
    "    else:\n",
    "        ensembl_set = set(ENSMBLs)\n",
    "    \n",
    "    # Create a mapping of ENSP to Uniprot.ID\n",
    "    # First, explode the ENSP lists into separate rows\n",
    "    exploded_df = df_uniprots_and_ensembles.explode('ENSP')\n",
    "    \n",
    "    # Filter only the rows where ENSP is in our target set\n",
    "    matches = exploded_df[exploded_df['ENSP'].isin(ensembl_set)]\n",
    "    \n",
    "    # Get unique Uniprot IDs\n",
    "    matching_uniprot_ids = matches['Uniprot.ID'].unique().tolist()\n",
    "    \n",
    "    return matching_uniprot_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rewriting the making of the adjacency matrix:\n",
    "#intaking relevant files:\n",
    "import pandas as pd\n",
    "import requests\n",
    "import csv\n",
    "import io\n",
    "import unipressed\n",
    "import unipressed.id_mapping\n",
    "import time\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "df_uniprots_and_ensembles=get_ENSMBL_Uniprot_lookup_table('uniprots_to_ensembls_lookup_table.csv')\n",
    "df_neighbors=pd.read_csv('protein_orders.csv')\n",
    "protein_list=df_neighbors['second_order']\n",
    "\n",
    "#now for each row, go through and mark 1st and second degree neighbors\n",
    "\n",
    "pts=get_protein_partners_batched(protein_list,batch_size=500)\n",
    "#affixing on the uniprot ID's\n",
    "uniprot_ids=[]\n",
    "for id in pts['stringId_A']:\n",
    "    #pull them out of individual lists and turn them into strings\n",
    "    uniprot_ids.append(ensmbl_to_uniprot_local(id,df_uniprots_and_ensembles)[0])\n",
    "pts['uniprots']=uniprot_ids\n",
    "partner_list=pts\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#local implementation of get protein partner\n",
    "def get_protein_partner_local(protein,parnter_list=partner_list):\n",
    "    # Find the row where the uniprots column matches the input protein\n",
    "    matching_row = partner_list[partner_list['uniprots'] == protein]\n",
    "    \n",
    "    # If a match is found, return the stringId_B value\n",
    "    if not matching_row.empty:\n",
    "        return matching_row['stringId_B'].tolist()[0]\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "test=list(set(protein_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get unique ENSEMBL IDs from partner_list\n",
    "# unique_ensembl_ids = set()\n",
    "# for partners in partner_list['stringId_B']:\n",
    "#     if isinstance(partners, list):\n",
    "#         unique_ensembl_ids.update(partners)\n",
    "#     else:\n",
    "#         unique_ensembl_ids.add(partners)\n",
    "\n",
    "# # Filter df_uniprots_and_ensembles to only include rows where ensp is in unique_ensembl_ids\n",
    "# filtered_df = df_uniprots_and_ensembles[df_uniprots_and_ensembles['ensp'].isin(unique_ensembl_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing proteins:   0%|          | 0/928 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing proteins: 100%|██████████| 928/928 [00:50<00:00, 18.22it/s]\n"
     ]
    }
   ],
   "source": [
    "# building adjacency matrix\n",
    "# Restrict the lookup table to only the Uniprot IDs present in df_neighbors['second_order']\n",
    "# First, get the set of Uniprot IDs in df_neighbors\n",
    "uniprots_in_neighbors = set(df_neighbors['second_order'].unique())\n",
    "# Filter the lookup table to only those Uniprot IDs\n",
    "restricted_df_uniprots_and_ensembles = df_uniprots_and_ensembles[df_uniprots_and_ensembles['Uniprot.ID'].isin(uniprots_in_neighbors)].reset_index(drop=True)\n",
    "\n",
    "\n",
    "#start building the adjacency matrix:\n",
    "adjacency_matrix = pd.DataFrame(0, \n",
    "                            index=protein_list,\n",
    "                            columns=protein_list)\n",
    "\n",
    "                        #first degree neighbors\n",
    "for protein in tqdm(adjacency_matrix.index, desc=\"Processing proteins\"):\n",
    "                   \n",
    "                    #STEP 1: mark first degree neighbors\n",
    "    partners_ensembl = get_protein_partner_local(protein) #working\n",
    "    partners_uniprot = ensmbl_to_uniprot_local(partners_ensembl, restricted_df_uniprots_and_ensembles) # not working\n",
    "  \n",
    "    # now make sure that the partners are in the matrix\n",
    "    partners_uniprot_in_matrix = get_uniprots_in_dataset(partners_uniprot, adjacency_matrix)\n",
    "\n",
    "    # then update the matrix\n",
    "    adjacency_matrix.loc[protein, partners_uniprot_in_matrix] = 1\n",
    "\n",
    "    #                 #second degree neighbors\n",
    "    for j in partners_uniprot_in_matrix: #change it to partners_uniprot if you want to include links that skip outside the observed protein dataset\n",
    "\n",
    "                    #STEP 2: mark second_degree_neighbors\n",
    "        partners_of_partners_ensembl=list(set(get_protein_partner_local(j)))\n",
    "        partners_of_partners_uniprot=list(set(ensmbl_to_uniprot_local(partners_of_partners_ensembl,restricted_df_uniprots_and_ensembles)))\n",
    "\n",
    "        partners_of_partners_uniprot_in_matrix=list(set(get_uniprots_in_dataset(partners_of_partners_uniprot,adjacency_matrix)))\n",
    "        #we need to be careful to not overwrite the ones\n",
    "        for k in partners_of_partners_uniprot_in_matrix:\n",
    "            if adjacency_matrix.loc[protein,k]==0:\n",
    "                adjacency_matrix.loc[protein,k]=.33 \n",
    "\n",
    "\n",
    "#NOTE!! THere is a meaningful question to ask here as follows: Assume: Protein A and C in dataset, B is not\n",
    "#if protein A is directly targeted, and B is the first degree neighbor of A and C is the first degree neighbor of B, should I mark C as the second degree neighbor of A?\n",
    "#if I do, I preserve the logical link, but B is not in the dataset. This is a meaningful question!\n",
    "#at present, implemented such that we generate second degree neigbhors by only looking at first degree neighbors in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjacency_matrix.to_csv('adjacency_matrix.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdul\\.conda\\envs\\cellbox\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:2182: DataConversionWarning: Data was converted to boolean for metric jaccard\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "#constructing an alternae way of protein protein interaction by evaluating similarity of the vectors for each protein.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "adjacency_matrix=pd.read_csv('adjacency_matrix.csv')\n",
    "pre_matrix=adjacency_matrix.iloc[:,1:].to_numpy()\n",
    "labels=adjacency_matrix.iloc[:,0].to_numpy()\n",
    "#using cosine similarity:\n",
    "cosine_similarity=1-pairwise_distances(pre_matrix,metric='cosine')\n",
    "\n",
    "#using pairwise distances:\n",
    "jaccard_similarity=1-pairwise_distances(pre_matrix,metric='jaccard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdul\\OneDrive - University of Cambridge\\Desktop\\MDRA\\cellbox_torch\\Abdullah_kuziez\\Testing_encodings\\String_adjacency\n",
      "================================================================================\n",
      "   _____     _ _ ____              \n",
      "  / ____|   | | |  _ \\             \n",
      " | |     ___| | | |_) | _____  __  \n",
      " | |    / _ \\ | |  _ < / _ \\ \\/ /  \n",
      " | |___|  __/ | | |_) | (_) >  <   \n",
      "  \\_____\\___|_|_|____/ \\___/_/\\_\\  \n",
      "Running CellBox scripts developed in Sander lab\n",
      "Maintained by Bo Yuan, Judy Shen, and Augustin Luna; contributions by Phuc Nguyen\n",
      "\n",
      "        version 0.1.0\n",
      "        -- Aug 24, 2023 --\n",
      "        * First stable release of CellBox pytorch\n",
      "        * Add pytest with a range of test cases\n",
      "        \n",
      "Tutorials and documentations are available at https://github.com/Mustardburger/CellBox\n",
      "If you want to discuss the usage or to report a bug, please use the 'Issues' function at GitHub.\n",
      "If you find CellBox useful for your research, please consider citing the corresponding publication.\n",
      "For more information, please email us at boyuan@g.harvard.edu and c_shen@g.harvard.edu, augustin_luna@hms.harvard.edu\n",
      " --------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#now I have to draw the rest of the owl :( plug in the adjacency matrix flat into cellbox and see if it works.\n",
    "import sys\n",
    "import os\n",
    "#what this code is doing is adding the cellbox directory to the path and then I can import the cellbox package which now meansd I can use the code there\n",
    "\n",
    "sys.path.append('../../../cellbox')\n",
    "print(os.getcwd())\n",
    "import cellbox\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Config' object has no attribute 'iter_train'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcellbox\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_torch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m factory\n\u001b[0;32m      4\u001b[0m args \u001b[38;5;241m=\u001b[39m Config(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrunning_adacency_config.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m model, args \u001b[38;5;241m=\u001b[39m \u001b[43mfactory\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Prepare your input data as needed, then:\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# output = model(y0, mu)  # y0 and mu are your input tensors\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\abdul\\OneDrive - University of Cambridge\\Desktop\\MDRA\\cellbox_torch\\Abdullah_kuziez\\Testing_encodings\\String_adjacency\\../../../cellbox\\cellbox\\model_torch.py:19\u001b[0m, in \u001b[0;36mfactory\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03mDefine model type based on configuration input. Currently supporting only 'CellBox'\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;124;03m    - args: the updated Config object\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCellBox\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 19\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mCellBox\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m     args \u001b[38;5;241m=\u001b[39m get_ops(args, model)\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model, args\n",
      "File \u001b[1;32mc:\\Users\\abdul\\OneDrive - University of Cambridge\\Desktop\\MDRA\\cellbox_torch\\Abdullah_kuziez\\Testing_encodings\\String_adjacency\\../../../cellbox\\cellbox\\model_torch.py:36\u001b[0m, in \u001b[0;36mPertBio.__init__\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m=\u001b[39m args\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_x \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mn_x\n\u001b[1;32m---> 36\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_train, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_monitor, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_eval \u001b[38;5;241m=\u001b[39m \u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_train\u001b[49m, args\u001b[38;5;241m.\u001b[39miter_monitor, args\u001b[38;5;241m.\u001b[39miter_eval\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ml1_lambda, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ml2_lambda \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mlr\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Config' object has no attribute 'iter_train'"
     ]
    }
   ],
   "source": [
    "from cellbox.config import Config\n",
    "from cellbox.model_torch import factory\n",
    "\n",
    "args = Config(\"running_adacency_config.json\")\n",
    "model, args = factory(args)\n",
    "\n",
    "# Prepare your input data as needed, then:\n",
    "# output = model(y0, mu)  # y0 and mu are your input tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing model training now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: torch.Size([21, 61])\n",
      "Test data shape: torch.Size([10, 61])\n"
     ]
    }
   ],
   "source": [
    "# providing the learning vectors now:\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Convert ground truth to PyTorch tensor\n",
    "ground_truth_tensor = torch.tensor(ground_truth.values, dtype=torch.float32)\n",
    "\n",
    "# Split the data into training and test sets (70:30 split)\n",
    "train_data, test_data = train_test_split(ground_truth_tensor, test_size=0.3, random_state=42)\n",
    "\n",
    "print(f\"Training data shape: {train_data.shape}\")\n",
    "print(f\"Test data shape: {test_data.shape}\")\n",
    "#now that we have the training and test data split we can start learning\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GENERAL NOTES\n",
    "\n",
    "#figuring out how to actually make the model;\n",
    "#principle component analysis to reduce dimensionality\n",
    "#collaborative filtering algorithm to interpolate missing values\n",
    "#netflix style problem;\n",
    "#\n",
    "\n",
    "#now do the machine learning...\n",
    "#the ground truths are targeted_proteins_with_metadata; but we will cleave off the metadata\n",
    "#the input is the first order marked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ENSP00000255416', 'ENSP00000263321', 'ENSP00000273588', 'ENSP00000296589', 'ENSP00000373570', 'ENSP00000341550', 'ENSP00000319531', 'ENSP00000451605', 'ENSP00000370737', 'ENSP00000363046', 'ENSP00000287996', 'ENSP00000361064', 'ENSP00000333262', 'ENSP00000260386', 'ENSP00000376142', 'ENSP00000411152', 'ENSP00000263370', 'ENSP00000413937', 'ENSP00000074304', 'ENSP00000363046', 'ENSP00000287996', 'ENSP00000361064', 'ENSP00000333262', 'ENSP00000260386', 'ENSP00000376142', 'ENSP00000411152', 'ENSP00000263370', 'ENSP00000413937', 'ENSP00000074304', 'ENSP00000440689', 'ENSP00000342007', 'ENSP00000301141', 'ENSP00000286479', 'ENSP00000353720', 'ENSP00000368727', 'ENSP00000225275']\n",
      "<generator object IdMappingJob.each_result at 0x000001F0EDD464A0>\n",
      "['Q13203', 'P14679', 'P48728', 'Q9UMX9', 'P17643', 'Q71RS6', 'P23434', 'Q01726', 'P23378', 'Q8NFU5', 'Q9H8X2', 'Q9UNW1', 'Q15735', 'P23677', 'P49441', 'P27987', 'Q96DU7', 'Q9BT40', 'P05181', 'P05177', 'P11509', 'P11245', 'P23141', 'P47989', 'P05164']\n",
      "Index:  0 Proteins:  []\n",
      "Index:  1 Proteins:  []\n",
      "Index:  2 Proteins:  []\n",
      "Index:  3 Proteins:  []\n",
      "Index:  4 Proteins:  []\n",
      "Index:  5 Proteins:  ['P00846', 'Q14457']\n",
      "<generator object IdMappingJob.each_result at 0x000001F0EE1D8890>\n",
      "Index:  6 Proteins:  []\n",
      "Index:  7 Proteins:  ['P26358']\n",
      "<generator object IdMappingJob.each_result at 0x000001F0EE1D8970>\n",
      "Index:  8 Proteins:  []\n",
      "Index:  9 Proteins:  []\n",
      "Index:  10 Proteins:  ['O00764']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(partners)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(convert_ensembl_to_uniprot(partners))\n\u001b[1;32m---> 16\u001b[0m second_order_marked\u001b[38;5;241m=\u001b[39m\u001b[43mbinary_marking_second_order_neighbors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfirst_order_marked\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m filtered_network_vectors\u001b[38;5;241m=\u001b[39mfilter_out_all_zero_rows(second_order_marked)\n\u001b[0;32m     19\u001b[0m filtered_network_vectors\u001b[38;5;241m=\u001b[39mfiltered_network_vectors\u001b[38;5;241m.\u001b[39miloc[:,:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m12\u001b[39m]\n",
      "Cell \u001b[1;32mIn[9], line 15\u001b[0m, in \u001b[0;36mbinary_marking_second_order_neighbors\u001b[1;34m(first_order_marked)\u001b[0m\n\u001b[0;32m     13\u001b[0m second_order_neighbors \u001b[38;5;241m=\u001b[39m get_protein_partners(first_order_proteins)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m#then use the convert to uniprot to get the uniprot ID's of the second order neighbors\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m second_order_uniprot_neighbors \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_ensembl_to_uniprot\u001b[49m\u001b[43m(\u001b[49m\u001b[43msecond_order_neighbors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m#then go through and check the columns to see if any of the second order neighbors are in the columns, and if so, add a .33 for the second order neighbors\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m neighbor \u001b[38;5;129;01min\u001b[39;00m second_order_uniprot_neighbors:\n",
      "Cell \u001b[1;32mIn[4], line 15\u001b[0m, in \u001b[0;36mconvert_ensembl_to_uniprot\u001b[1;34m(ensembl_ids)\u001b[0m\n\u001b[0;32m      8\u001b[0m job \u001b[38;5;241m=\u001b[39m unipressed\u001b[38;5;241m.\u001b[39mIdMappingClient\u001b[38;5;241m.\u001b[39msubmit(\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEnsembl_Protein\u001b[39m\u001b[38;5;124m'\u001b[39m,   \u001b[38;5;66;03m# source database\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUniProtKB\u001b[39m\u001b[38;5;124m'\u001b[39m,         \u001b[38;5;66;03m# target database\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     ids\u001b[38;5;241m=\u001b[39mensembl_ids      \u001b[38;5;66;03m# list of Ensembl protein IDs\u001b[39;00m\n\u001b[0;32m     12\u001b[0m )\n\u001b[0;32m     14\u001b[0m uniprot_ids \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 15\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m results \u001b[38;5;241m=\u001b[39m unipressed\u001b[38;5;241m.\u001b[39mid_mapping\u001b[38;5;241m.\u001b[39mcore\u001b[38;5;241m.\u001b[39mIdMappingJob\u001b[38;5;241m.\u001b[39meach_result(job)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(results)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Legacy does not work\n",
    "\n",
    "# import numpy as np\n",
    "# targeted_proteins_with_metadata=pd.read_csv('targeted_proteins_with_metadata.csv')\n",
    "# # get the\n",
    "\n",
    "\n",
    "\n",
    "# first_order_marked=binary_marking_first__order(targeted_proteins_with_metadata)\n",
    "# protein = \"P38398\"          # replace with your UniProt accession # need to actually update this:\n",
    "# partners=(get_protein_partners(protein))\n",
    "# print(partners)\n",
    "# print(convert_ensembl_to_uniprot(partners))\n",
    "# second_order_marked=binary_marking_second_order_neighbors(first_order_marked)\n",
    "\n",
    "# filtered_network_vectors=filter_out_all_zero_rows(second_order_marked)\n",
    "# filtered_network_vectors=filtered_network_vectors.iloc[:,:-12]\n",
    "# rows_to_keep=filtered_network_vectors.index.tolist()\n",
    "\n",
    "# #now make the ground truth matrix\n",
    "# ground_truth=targeted_proteins_with_metadata.iloc[:,:-12]\n",
    "# ground_truth=ground_truth.iloc[rows_to_keep,:]\n",
    "# ground_truth = ground_truth.fillna(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cellbox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
